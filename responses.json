[
    {
        "Observable": "An inventory system or database containing records of all enterprise assets with attributes such as network address, hardware address, machine name, owner, department, and approval status, along with logs or timestamps indicating bi-annual or more frequent reviews and updates.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": {
            "Checklist": "The safeguard can be assessed using a checklist to verify the existence of the inventory and the presence of required attributes for each asset.",
            "Verifiable": "The accuracy and completeness of the inventory can be verified by cross-referencing with actual asset data and system configurations.",
            "Measurable": "Quantitative metrics such as coverage, accuracy, and update frequency can be derived from the inventory data to measure enforcement quality."
        },
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": {
            "Data-driven": "Statistics and metrics can be generated from the inventory database, event logs, and network scan data to assess compliance.",
            "Model-based": "The configuration and structure of the inventory system can be evaluated against the safeguard requirements to determine if it is properly set up.",
            "Active testing": "Network probing or scanning tools can be used to actively discover assets not listed in the inventory, providing data for metrics."
        },
        "Metric": [
            {
                "defination": "Inventory Coverage - The proportion of enterprise assets that are correctly listed in the inventory.",
                "measure.description": "Count of assets listed in the inventory and count of assets not listed but detected.",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Information Accuracy - The proportion of inventoried assets with all required attributes present and accurate.",
                "measure.description": "Count of assets in inventory with complete and accurate attributes and total assets in inventory.",
                "measure.id": "M3, M1",
                "equation": "M3 / M1"
            },
            {
                "defination": "Update Frequency - The time between inventory reviews to ensure it is at least bi-annual.",
                "measure.description": "Timestamps of the last and previous inventory reviews.",
                "measure.id": "M5, M6",
                "equation": "M5 - M6 (in days, should be <= 180 for bi-annual compliance)"
            },
            {
                "defination": "Approval Compliance - The proportion of connected assets that are approved to connect to the network.",
                "measure.description": "Count of assets approved to connect (from inventory) and count of assets connected but not approved (from scans).",
                "measure.id": "M7, M8",
                "equation": "M7 / (M7 + M8)"
            }
        ]
    },
    {
        "Observable": "Logs of unauthorized asset detection, records of actions taken (e.g., removal, denial, quarantine), and weekly review reports.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Checklist because the existence of a process can be assessed through documentation or scripts; Verifiable because the actions taken can be confirmed by checking system logs and configurations; Measurable because the frequency and effectiveness of the process can be quantified using data from logs and reviews.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven because it involves analyzing event logs, asset detection data, and action records to evaluate enforcement; Model-based because it requires checking the configuration and setup of the process for addressing unauthorized assets; Active testing because probing the network with unauthorized assets can test if the process responds appropriately.",
        "Metric": [
            {
                "defination": "Whether a process for addressing unauthorized assets is documented and in place.",
                "measure.description": [
                    "Boolean indicator if a process for addressing unauthorized assets exists"
                ],
                "measure.id": [
                    "M1"
                ],
                "equation": "M1"
            },
            {
                "defination": "Compliance with the weekly review schedule based on the time between consecutive reviews.",
                "measure.description": [
                    "Timestamp of the last weekly review",
                    "Timestamp of the previous weekly review"
                ],
                "measure.id": [
                    "M2",
                    "M3"
                ],
                "equation": "1 if |M2 - M3| <= 7 days, else 0"
            },
            {
                "defination": "Percentage of detected unauthorized assets that are addressed (removed, denied, or quarantined) in the monitoring period.",
                "measure.description": [
                    "Number of unauthorized assets detected in the last complete week",
                    "Number of unauthorized assets addressed in the last complete week"
                ],
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "M5 / M4 if M4 > 0, else 1"
            }
        ]
    },
    {
        "Observable": "Scan reports from the active discovery tool, configuration settings indicating scan schedule (e.g., cron jobs or tool configurations), and logs of scan executions.",
        "Class": "Checklist, Verifiable",
        "Class.explanation": "Checklist because the presence and configuration of the tool can be assessed through scripting (e.g., checking if the tool is installed and configured correctly). Verifiable because the system configuration (e.g., scan frequency settings) can be manually or automatically verified against policies.",
        "Evaluation_Method": "Data-driven, Active testing",
        "Evaluation_Method.explanation": "Data-driven because statistics can be generated from scan logs, configuration data, and network coverage reports to evaluate enforcement. Active testing because probing the system (e.g., running the discovery tool or checking its output) may be necessary to verify functionality and scan execution.",
        "Metric": [
            {
                "defination": "Measures the proportion of active discovery tools that are properly configured.",
                "measure.description": "Count of properly configured active discovery tools (M1), Count of improperly configured active discovery tools (M2)",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Measures the percentage of enterprise networks covered by the active discovery tool scans.",
                "measure.description": "Count of enterprise networks covered by active discovery scans (M3), Count of enterprise networks not covered by active discovery scans (M4)",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Measures the time between consecutive scans to ensure daily or more frequent execution.",
                "measure.description": "Time of the most recent scan executed by the active discovery tool (M5), Time of the previous scan executed by the active discovery tool (M6)",
                "measure.id": "M5, M6",
                "equation": "M5 - M6"
            }
        ]
    },
    {
        "Observable": "DHCP server configuration logs, IP address management tool configurations, asset inventory update logs, and timestamps of inventory updates.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the configuration of DHCP servers and IP management tools can be inspected to check if logging is enabled or tools are configured. Measurable because quantitative data such as the count of servers with logging enabled and the frequency of inventory updates can be analyzed from logs and records.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based because the configuration settings of DHCP servers and IP tools can be compared against standards to verify compliance. Data-driven because log data and update timestamps can be statistically analyzed to measure coverage and update frequencies.",
        "Metric": [
            {
                "defination": "Proportion of DHCP servers that have logging enabled.",
                "measure.description": [
                    "Count of DHCP servers with logging enabled",
                    "Count of DHCP servers without logging enabled"
                ],
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of IP address management tools that are properly configured to update asset inventory.",
                "measure.description": [
                    "Count of IP address management tools configured to update asset inventory",
                    "Count of IP address management tools not configured properly"
                ],
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Time between consecutive asset inventory updates.",
                "measure.description": [
                    "Timestamp of the last asset inventory update",
                    "Timestamp of the previous asset inventory update"
                ],
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 - M6"
            }
        ]
    },
    {
        "Observable": "Scan results from passive discovery tools, configuration settings of the tools, logs of scan activities, and records of asset inventory updates.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration and presence of the passive discovery tool can be checked against established standards or scripts; Measurable because the frequency, coverage, and timeliness of scans and updates require quantitative analysis of data from logs and inventory records.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Evaluation involves analyzing data from event logs, scan results, and inventory updates to generate statistics on tool configuration, network coverage, scan intervals, and update timing, rather than relying solely on configuration checks or active probing.",
        "Metric": [
            {
                "defination": "Proportion of networks covered by properly configured passive discovery tools.",
                "measure.description": "M3: Count of networks covered by properly configured tools, M4: Count of networks not covered by properly configured tools",
                "measure.id": "M3, M4",
                "equation": "Coverage = M3 / (M3 + M4)"
            },
            {
                "defination": "Proportion of passive discovery tools that are properly configured.",
                "measure.description": "M1: Count of properly configured passive discovery tools, M2: Count of improperly configured passive discovery tools",
                "measure.id": "M1, M2",
                "equation": "Configuration Compliance = M1 / (M1 + M2)"
            },
            {
                "defination": "Time interval between consecutive scans performed by the passive discovery tool.",
                "measure.description": "M6: Timestamp of the last scan, M7: Timestamp of the previous scan",
                "measure.id": "M6, M7",
                "equation": "Scan Interval = M6 - M7"
            },
            {
                "defination": "Freshness of the asset inventory update relative to the last scan, indicating how quickly updates are made.",
                "measure.description": "M6: Timestamp of the last scan, M8: Timestamp of the last asset inventory update",
                "measure.id": "M6, M8",
                "equation": "Update Freshness = 1 / (M8 - M6) if M8 > M6, else undefined"
            }
        ]
    },
    {
        "Observable": "A detailed software inventory database or list with entries containing title, publisher, initial install/use date, business purpose, and optionally URL, app store, version, deployment mechanism, decommission date, and number of licenses; logs or records of bi-annual reviews and updates; integration with software discovery tools.",
        "Class": "Checklist, Verifiable, Measurable",
        "Class.explanation": "Checklist because the presence and format of the inventory can be automatically checked via scripting; Verifiable because the configuration of the inventory management system can be inspected for compliance; Measurable because data on inventory coverage, metadata completeness, and update frequency can be analyzed quantitatively.",
        "Evaluation_Method": "Data-driven, Model-based, Active testing",
        "Evaluation_Method.explanation": "Data-driven for statistical analysis of inventory data and review logs to assess coverage and timeliness; Model-based for evaluating the configured policies and settings in the inventory system; Active testing for probing the system to verify inventory accuracy against actual software installations.",
        "Metric": [
            {
                "defination": "Proportion of software installations that are documented in the inventory",
                "measure.description": "Count of software installations documented in inventory and count of software installations not documented in inventory",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of inventory entries that have all required details (title, publisher, initial install date, business purpose)",
                "measure.description": "Count of inventory entries with complete required details and count of inventory entries missing some required details",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Time interval between consecutive reviews of the inventory",
                "measure.description": "Time of last inventory review and time of previous inventory review",
                "measure.id": "M5, M6",
                "equation": "M5 - M6"
            }
        ]
    },
    {
        "Observable": "The software inventory list with authorization status (authorized/unauthorized) and support status for each software, exception documentation files for unsupported software, and timestamps or logs of review activities.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard is verifiable because it involves checking the configuration of the software inventory (e.g., support status and authorization designations) and the existence of exception documentation, which can be assessed through manual inspection or automated verification tools without requiring complex data analytics.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation is used to assess the configuration of authorized software and exception documentation by examining system settings and documents. Data-driven evaluation is used for review frequency by analyzing timestamps from review logs to generate statistics on compliance with the monthly review requirement.",
        "Metric": [
            {
                "defination": "The proportion of authorized software that is either currently supported or has valid exception documentation.",
                "measure.description": "Count of compliant authorized software (supported or with exception) and total authorized software.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / M2"
            },
            {
                "defination": "The time interval in days since the last review of software support status.",
                "measure.description": "Date of the last review and the current date.",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M4 - M3"
            },
            {
                "defination": "The percentage of unsupported software that has exception documentation.",
                "measure.description": "Count of unsupported software with exception documentation and total unsupported software.",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 / M6"
            }
        ]
    },
    {
        "Observable": "Software inventory lists, exception documentation records, review logs, and evidence of software removal actions on enterprise assets.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the presence of unauthorized software and exceptions can be checked against system configurations and documents (e.g., software inventories and exception logs). Measurable because the number of unauthorized software instances and their handling can be quantified and analyzed over time using data from logs and inventories.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because statistics on software counts, exception rates, and removal actions can be generated from event logs, inventory data, and review records. Model-based because the system's software configuration and exception policies can be modeled and verified against established standards (e.g., checking if software lists align with authorized baselines).",
        "Metric": [
            {
                "definition": "The percentage of unauthorized software instances that are either removed or have a documented exception, indicating compliance with the safeguard.",
                "measure.description": "M1: Total count of unauthorized software instances on enterprise assets; M2: Count of unauthorized software instances with documented exceptions; M3: Count of unauthorized software instances that have been removed",
                "measure.id": [
                    "M1",
                    "M2",
                    "M3"
                ],
                "equation": "(M2 + M3) / M1"
            },
            {
                "definition": "The frequency of reviews conducted, measured as the number of reviews in the past 30 days, to ensure monthly or more frequent assessments.",
                "measure.description": "M4: Number of reviews conducted in the past 30 days",
                "measure.id": [
                    "M4"
                ],
                "equation": "M4"
            }
        ]
    },
    {
        "Observable": "Presence of software inventory tools on enterprise assets, discovery logs generated by these tools, and documentation reports of installed software.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Checklist because the installation of software inventory tools can be verified through automated scripts; Verifiable because the configuration settings of these tools can be inspected for compliance; Measurable because the effectiveness of discovery and documentation requires data analysis from logs and reports.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven for analyzing event logs, discovery data, and documentation rates; Model-based for checking the configuration models of the inventory tools; Active testing for probing systems to ensure the tools are functioning correctly and discovering software.",
        "Metric": [
            {
                "defination": "The proportion of enterprise assets covered by properly configured software inventory tools.",
                "measure.description": "Count of assets with tools installed and configured, and total number of assets.",
                "measure.id": "M1, M3",
                "equation": "M1 / M3"
            },
            {
                "defination": "The average time between consecutive discovery scans performed by the software inventory tools.",
                "measure.description": "Time of the last discovery scan and time of the previous discovery scan.",
                "measure.id": "M6, M7",
                "equation": "M6 - M7"
            },
            {
                "defination": "The ratio of documented software installations to the total discovered software installations.",
                "measure.description": "Count of software installations documented and count of software installations discovered.",
                "measure.id": "M4, M5",
                "equation": "M5 / M4"
            }
        ]
    },
    {
        "Observable": "Configuration settings for application allowlisting on systems, logs of software execution attempts (both allowed and blocked), and records of allowlist reassessment activities including timestamps and details of reviews.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of application allowlisting can be directly inspected on systems to confirm enablement and settings. Measurable because the effectiveness of blocking unauthorized software and the timeliness of reassessments can be quantified using data from logs and configuration checks over time.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves verifying configuration models and settings for allowlisting on systems. Data-driven evaluation uses statistical analysis of execution logs and reassessment records to measure block rates, coverage, and compliance with reassessment schedules.",
        "Metric": [
            {
                "defination": "Proportion of systems with application allowlisting enabled",
                "measure.description": "Count of systems with allowlisting enabled and properly configured, and count of systems without allowlisting or improperly configured",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Block rate of unauthorized software execution attempts",
                "measure.description": "Count of unauthorized software execution attempts that were blocked, and count of unauthorized attempts that were not blocked (if detectable)",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Proportion of systems where the allowlist was reassessed within the last 6 months",
                "measure.description": "Count of systems reassessed within the last 6 months, and count of systems not reassessed within the last 6 months",
                "measure.id": "M5, M6",
                "equation": "M5 / (M5 + M6)"
            }
        ]
    },
    {
        "Observable": "System logs showing library load events (e.g., attempts to load .dll, .ocx, .so files), configuration files defining authorized libraries, records of reassessment activities, and evidence of blocked unauthorized loads.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of allowed libraries can be inspected against system settings to confirm compliance. Measurable because the effectiveness of blocking unauthorized loads can be quantified through analysis of event logs and reassessment frequencies.",
        "Evaluation_Method": "Model-based, Data-driven, Active testing",
        "Evaluation_Method.explanation": "Model-based evaluation involves checking system configurations to verify the list of authorized libraries. Data-driven evaluation uses statistics from event logs to measure block rates and load attempts. Active testing involves probing the system by attempting to load unauthorized libraries to test if controls are effective.",
        "Metric": [
            {
                "defination": "Proportion of system processes that have technical controls enabled to restrict library loading.",
                "measure.description": "Count of processes with controls enabled and count of processes without controls.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "Coverage = M1 / (M1 + M2)"
            },
            {
                "defination": "Effectiveness of blocking unauthorized library loads, measured as the ratio of blocked unauthorized load attempts to total unauthorized load attempts.",
                "measure.description": "Count of unauthorized library load attempts and count of blocked unauthorized loads.",
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "Block Rate = M5 / M4"
            },
            {
                "defination": "Frequency of reassessment, measured as the time difference between consecutive reassessments to ensure it is done at least bi-annually.",
                "measure.description": "Time of last reassessment and time of previous reassessment.",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "Reassessment Frequency = M7 - M8"
            },
            {
                "defination": "Compliance with reassessment schedule, indicating if reassessment is performed within the required bi-annual period (e.g., 180 days).",
                "measure.description": "Time of last reassessment and time of previous reassessment.",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "Compliance = 1 if (M7 - M8) <= 180 days, else 0"
            }
        ]
    },
    {
        "Observable": "Script execution logs, configuration settings for digital signatures and version control, reassessment records, blocked script attempts logs",
        "Class": "Checklist, Verifiable, Measurable",
        "Class.explanation": "Checklist: We can check off items like digital signature requirement and version control integration through a predefined list. Verifiable: The configurations of technical controls can be verified against security policies. Measurable: Execution statistics, such as counts of authorized and blocked scripts, can be measured from logs and data.",
        "Evaluation_Method": "Data-driven, Model-based, Active testing",
        "Evaluation_Method.explanation": "Data-driven: Execution logs and event data provide statistics on script attempts and blocks. Model-based: Configuration settings define the expected model for script execution controls. Active testing: Probing the system with unauthorized scripts tests if they are blocked as expected.",
        "Metric": [
            {
                "defination": "Proportion of script executions that are authorized, indicating compliance with the safeguard.",
                "measure.description": "M1: Total number of script execution attempts, M2: Number of authorized script executions (e.g., with valid digital signature or in version control)",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M2 / M1"
            },
            {
                "defination": "Proportion of unauthorized script execution attempts that are blocked, measuring the effectiveness of blocking controls.",
                "measure.description": "M1: Total number of script execution attempts, M2: Number of authorized script executions, M3: Number of unauthorized script execution attempts blocked",
                "measure.id": [
                    "M1",
                    "M2",
                    "M3"
                ],
                "equation": "M3 / (M1 - M2)"
            },
            {
                "defination": "Average compliance score for the configuration of technical controls, such as digital signatures and version control.",
                "measure.description": "M5: Configuration status for digital signature enforcement (1 if enabled, 0 otherwise), M6: Configuration status for version control integration (1 if enabled, 0 otherwise)",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "(M5 + M6) / 2"
            },
            {
                "defination": "Time interval between reassessments, assessing adherence to the bi-annual requirement.",
                "measure.description": "M7: Time of last reassessment, M8: Time of previous reassessment",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "M7 - M8"
            }
        ]
    },
    {
        "Observable": "The documented data management process, including its content addressing data sensitivity, data owner, handling of data, data retention limits, and disposal requirements; logs or records of reviews and updates, such as timestamps and version history.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the documentation can be inspected to confirm it contains all required elements. Measurable because aspects like completeness and review frequency can be quantified using data.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because timestamps and logs from document management systems can be analyzed to measure review frequency and update history. Model-based because the documented process serves as a configuration model that can be validated against enterprise standards and requirements.",
        "Metric": [
            {
                "defination": "Measures the proportion of required elements addressed in the data management process document.",
                "measure.description": "M1: Count of the number of required elements present in the document. Required elements are: data sensitivity, data owner, handling of data, data retention limits, disposal requirements.",
                "measure.id": "M1",
                "equation": "M1 / 5"
            },
            {
                "defination": "Indicates whether the document was reviewed within the last 365 days, as required annually.",
                "measure.description": "M2: Date of the last review of the document, M3: Current date.",
                "measure.id": "M2, M3",
                "equation": "IF( (M3 - M2) <= 365, 1, 0 )"
            }
        ]
    },
    {
        "Observable": "Presence of a data inventory system, records of sensitive data inclusion in the inventory, and logs or timestamps of annual reviews and updates to the inventory.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the existence and configuration of the data inventory can be checked through system audits or inspections. Measurable because the coverage of sensitive data and the frequency of reviews can be quantified numerically using data from the inventory.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven because assessment requires analyzing data from the inventory records and review logs to compute metrics like coverage and compliance. Model-based because it involves inspecting the configuration and setup of the data management process to verify its existence and rules.",
        "Metric": [
            {
                "definition": "Proportion of sensitive data assets that are listed in the data inventory, indicating how well sensitive data is covered.",
                "measure.description": "Count of sensitive data assets in the inventory and estimated total number of sensitive data assets",
                "measure.id": "M1, M2",
                "equation": "M1 / M2"
            },
            {
                "definition": "Indicator of whether the data inventory was reviewed within the last 365 days, as required annually.",
                "measure.description": "Date of the last inventory review",
                "measure.id": "M3",
                "equation": "1 if (current_date - M3) <= 365 else 0"
            },
            {
                "definition": "Indicator of whether the inventory was updated after the last review, ensuring it is maintained current.",
                "measure.description": "Date of the last inventory update and date of the last review",
                "measure.id": "M5, M3",
                "equation": "1 if M5 >= M3 else 0"
            }
        ]
    },
    {
        "Observable": "Access control list configurations on file systems, databases, and applications; access logs showing user permissions and access attempts.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because ACL configurations can be directly inspected in system settings to check for proper setup based on need-to-know. Measurable because access patterns and permission alignments can be analyzed from logs and data to assess compliance with need-to-know principles.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves examining the configuration models of systems, such as file permissions, database roles, and application settings, to verify that ACLs are correctly applied. Data-driven evaluation involves generating statistics from access event logs, user activities, and permission records to analyze if access patterns and permissions align with need-to-know requirements.",
        "Metric": [
            {
                "definition": "The proportion of resources (file systems, databases, applications) that have access control lists configured based on need-to-know.",
                "measure": {
                    "description": "Count of resources with ACLs configured and count without ACLs configured or not based on need-to-know.",
                    "id": "M1, M2"
                },
                "equation": "M1 / (M1 + M2)"
            },
            {
                "definition": "The percentage of access attempts that are authorized based on need-to-know principles.",
                "measure": {
                    "description": "Count of authorized access attempts and count of unauthorized access attempts.",
                    "id": "M3, M4"
                },
                "equation": "M3 / (M3 + M4)"
            },
            {
                "definition": "The proportion of users whose permissions are correctly aligned with their need-to-know requirements.",
                "measure": {
                    "description": "Count of users with permissions aligned to need-to-know and count of users with permissions not aligned.",
                    "id": "M5, M6"
                },
                "equation": "M5 / (M5 + M6)"
            }
        ]
    },
    {
        "Observable": "The documented data management process, data retention policies (including minimum and maximum timelines), system logs of data retention and deletion actions, and timestamps of data creation and access.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the existence and content of the documented data management process can be checked through configuration reviews and policy audits. Measurable because data retention times can be quantified and compared against the policy timelines to assess compliance.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because assessment requires analyzing data from logs, timestamps, and retention actions to compute compliance metrics. Model-based because the policy specifications (minimum and maximum retention times) must be modeled and verified against system configurations and data states.",
        "Metric": [
            {
                "Metric.definition": "Proportion of data sets that have a defined retention policy with both minimum and maximum timelines.",
                "Metric.measure.description": "M1: Count of data sets with a defined retention policy (min and max), M2: Count of data sets without a defined retention policy",
                "Metric.measure.id": [
                    "M1",
                    "M2"
                ],
                "Metric.equation": "M1 / (M1 + M2)"
            },
            {
                "Metric.definition": "Proportion of data sets with defined policies that are compliant with retention timelines (not deleted before minimum time or retained beyond maximum time).",
                "Metric.measure.description": "M1: Count of data sets with a defined retention policy, M3: Count of data sets with policy that are deleted before the minimum retention time, M4: Count of data sets with policy that are not deleted and current time exceeds the maximum retention time",
                "Metric.measure.id": [
                    "M1",
                    "M3",
                    "M4"
                ],
                "Metric.equation": "1 - (M3 + M4) / M1"
            }
        ]
    },
    {
        "Observable": "Records of data disposal events, audit logs, documented data management process, sensitivity classifications of data, and evidence of disposal methods matching sensitivity levels.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the disposal process and method can be checked against documented standards and configurations. Measurable because compliance rates and sensitivity matching can be quantified from data logs and events.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because it involves analyzing disposal logs, sensitivity data, and event records to generate statistics. Model-based because it requires validating against the enterprise's documented data management process model for configuration and compliance.",
        "Metric": [
            {
                "defination": "Proportion of data disposal events that comply with the documented data management process.",
                "measure": [
                    {
                        "description": "Count of data disposal events that comply with the documented process",
                        "id": "M1"
                    },
                    {
                        "description": "Count of data disposal events that do not comply with the documented process",
                        "id": "M2"
                    }
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of data disposal events where the disposal method is commensurate with the data sensitivity.",
                "measure": [
                    {
                        "description": "Count of data disposal events with method commensurate to data sensitivity",
                        "id": "M3"
                    },
                    {
                        "description": "Count of data disposal events with method not commensurate to data sensitivity",
                        "id": "M4"
                    }
                ],
                "equation": "M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Encryption status indicators on end-user devices, such as BitLocker enabled in Windows, FileVault enabled in macOS, or dm-crypt active in Linux, along with configuration settings and logs indicating encryption state.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard is verifiable because the encryption status can be directly checked through system configuration settings, command-line tools, or management consoles provided by the operating systems, allowing for confirmation without extensive data analysis or active probing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is model-based as it involves examining the configuration models of devices, such as registry entries, configuration files, or system settings, to determine if encryption is enabled, without requiring statistical analysis of logs or active testing methods.",
        "Metric": {
            "defination": "Encryption Compliance Rate - The proportion of end-user devices that contain sensitive data and have encryption enabled, indicating how well the safeguard is enforced.",
            "measure": {
                "description": [
                    "Count of end-user devices with encryption enabled",
                    "Count of end-user devices identified as containing sensitive data"
                ],
                "id": [
                    "M1",
                    "M2"
                ]
            },
            "equation": "M1 / M2"
        }
    },
    {
        "Observable": "Existence of a data classification policy documentation, application of classification labels (e.g., 'Sensitive', 'Confidential', 'Public') to data assets, audit logs or records of classification activities, and documentation of annual reviews or reviews triggered by significant enterprise changes.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": [
            "Verifiable: The safeguard can be verified by inspecting the data classification policy, review records, and label applications to ensure they exist and are compliant with the scheme.",
            "Measurable: The enforcement quality can be quantified by measuring the coverage of data classification and the timeliness of scheme reviews using numerical data."
        ],
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": [
            "Model-based: Evaluation involves examining the configuration and documentation of the data classification scheme, such as policy files and review logs, to verify its existence and completeness.",
            "Data-driven: Evaluation requires analyzing data from assets, such as counts of classified data and timestamps of reviews, to compute metrics like coverage and review frequency."
        ],
        "Metric": [
            {
                "defination": "Percentage of data assets that have been classified according to the enterprise classification scheme.",
                "measure.description": "Count of data assets with classification labels applied and total number of data assets.",
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "(M1 / M3) * 100"
            },
            {
                "defination": "Time elapsed between consecutive reviews of the classification scheme, measured in days.",
                "measure.description": "Timestamps of the last and previous classification scheme reviews.",
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "M4 - M5"
            },
            {
                "defination": "Compliance with reviewing the scheme after significant enterprise changes, expressed as the proportion of changes that triggered a review.",
                "measure.description": "Number of significant enterprise changes since the last review and number of reviews performed in response to those changes.",
                "measure.id": [
                    "M6",
                    "M7"
                ],
                "equation": "IF(M6 > 0, M7 / M6, 1)"
            }
        ]
    },
    {
        "Observable": "Data flow documentation files, review logs, update timestamps, records of significant changes, and metadata such as last review date and update history.",
        "Class": [
            "Verifiable"
        ],
        "Class.explanation": "The safeguard can be verified by inspecting the documentation and its metadata to ensure it exists, is complete, and is reviewed/updated as required, which involves checking system configurations or files without needing complex data analysis.",
        "Evaluation_Method": [
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Evaluation is based on examining the documented data flows, review records, and update logs, which are part of the system's configuration or process model, without requiring active probing or extensive data-driven analytics.",
        "Metric": [
            {
                "defination": "Proportion of data flows that are documented, indicating completeness of documentation.",
                "measure.description": "Count of data flows with documentation and total count of data flows.",
                "measure.id": "M_doc_count, M_total_count",
                "equation": "M_doc_count / M_total_count"
            },
            {
                "defination": "Time elapsed since the last review of data flow documentation, measured in days, to assess timeliness against the annual requirement.",
                "measure.description": "Date of the last review.",
                "measure.id": "M_last_review_date",
                "equation": "current_date - M_last_review_date"
            },
            {
                "defination": "Binary indicator of whether an update occurred after a significant change, based on the presence of update records following change events.",
                "measure.description": "Number of significant changes that occurred and number of updates performed after those changes.",
                "measure.id": "M_sig_changes, M_updates_after_changes",
                "equation": "1 if M_updates_after_changes >= M_sig_changes else 0"
            }
        ]
    },
    {
        "Observable": "Encryption settings on removable media devices, configuration files, or logs indicating encryption status, such as BitLocker or similar encryption tools being enabled.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be verified by checking the configuration of operating systems or device management tools to ensure that encryption is enabled for all removable media, as it involves inspecting settings rather than requiring data analytics or active probing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "The evaluation relies on examining the system configuration models, such as group policies, device settings, or management console configurations, to determine if encryption is required and enabled for removable media.",
        "Metric": [
            {
                "defination": "The proportion of removable media devices that have encryption enabled, indicating compliance with the safeguard.",
                "measure.description": "M1: Count of removable media devices with encryption enabled. M2: Count of removable media devices without encryption enabled.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            }
        ]
    },
    {
        "Observable": "Encrypted network connections for sensitive data in transit, such as TLS or SSH sessions, and configuration settings on systems enabling encryption.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because encryption can be confirmed by inspecting system configurations (e.g., TLS settings on servers). Measurable because the extent of encryption usage can be quantified through analysis of network traffic logs.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation involves analyzing network traffic logs to measure encryption usage. Model-based evaluation checks system configurations for encryption settings. Active testing includes performing network probes or scans to verify encryption functionality.",
        "Metric": [
            {
                "defination": "The proportion of sensitive data transmissions that are encrypted.",
                "measure.description": "Count of sensitive data transmissions that are encrypted and count of sensitive data transmissions that are unencrypted.",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "The proportion of endpoints handling sensitive data that are configured to encrypt data in transit.",
                "measure.description": "Count of endpoints with encryption enabled for data in transit and count of endpoints without encryption enabled.",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Encryption configurations on servers, applications, and databases; encrypted data at rest; audit logs of encryption processes; settings indicating storage-layer or application-layer encryption.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because encryption settings can be inspected through system configurations, such as checking for enabled encryption in database management systems or server settings. Measurable because the amount and proportion of encrypted sensitive data can be quantified using data-driven analytics, such as comparing encrypted vs. unencrypted data volumes.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based by examining configuration files and system settings to verify if encryption is enabled on servers, applications, and databases. Active testing by probing the system, such as attempting to access data without proper decryption keys, to confirm that data is indeed encrypted and inaccessible in plain text.",
        "Metric": [
            {
                "defination": "Percentage of sensitive data encrypted at the storage layer.",
                "measure.description": "M_enc_storage: Amount of sensitive data encrypted at the storage layer (e.g., using server-side encryption), M_total: Total amount of sensitive data.",
                "measure.id": "M_enc_storage, M_total",
                "equation": "M_enc_storage / M_total"
            },
            {
                "defination": "Percentage of sensitive data encrypted at the application layer.",
                "measure.description": "M_enc_app: Amount of sensitive data encrypted at the application layer (e.g., using client-side encryption), M_total: Total amount of sensitive data.",
                "measure.id": "M_enc_app, M_total",
                "equation": "M_enc_app / M_total"
            },
            {
                "defination": "Overall percentage of sensitive data that is encrypted at any layer (storage or application).",
                "measure.description": "M_not_enc: Amount of sensitive data not encrypted at any layer, M_total: Total amount of sensitive data.",
                "measure.id": "M_not_enc, M_total",
                "equation": "1 - (M_not_enc / M_total)"
            },
            {
                "defination": "Compliance rate with the minimum requirement of storage-layer encryption.",
                "measure.description": "M_enc_storage_min: Amount of sensitive data encrypted at least at the storage layer (meeting minimum requirement), M_total: Total amount of sensitive data.",
                "measure.id": "M_enc_storage_min, M_total",
                "equation": "M_enc_storage_min / M_total"
            }
        ]
    },
    {
        "Observable": "Data classification labels, network segmentation configurations, access logs showing data flows restricted by sensitivity levels, and records of data processing events.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of data classification and segmentation rules can be checked through system inspections; Measurable because actual data processing and violations need to be analyzed using data from logs and flows to assess enforcement quality.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based for evaluating the configuration models of data segmentation and classification; Data-driven for generating statistics from data access logs and processing events to monitor compliance.",
        "Metric": [
            {
                "definition": "Percentage of data assets that have been classified with a sensitivity level.",
                "measure.description": "Count of data assets with sensitivity classification and total data assets.",
                "measure.id": "M1, M6",
                "equation": "M1 / M6"
            },
            {
                "definition": "Percentage of configured segmentation rules that are correct and enforce sensitivity-based restrictions.",
                "measure.description": "Count of correct segmentation rules and total configured segmentation rules.",
                "measure.id": "M5, M4",
                "equation": "M5 / M4"
            },
            {
                "definition": "Number of instances where sensitive data is processed on assets intended for lower sensitivity, indicating violations.",
                "measure.description": "Count of violation events where sensitive data is accessed inappropriately.",
                "measure.id": "M3",
                "equation": "M3"
            },
            {
                "definition": "Percentage of processing assets that have a defined intended sensitivity level.",
                "measure.description": "Count of processing assets with defined sensitivity level and total processing assets.",
                "measure.id": "M2, M7",
                "equation": "M2 / M7"
            }
        ]
    },
    {
        "Observable": "Configuration of the DLP tool, logs from DLP scans showing identified sensitive data, and records of updated data inventory",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the installation and configuration of the DLP tool can be checked through system audits and configuration reviews. Measurable because assessing the identification of sensitive data and the frequency of inventory updates requires data analysis from scan logs and inventory records.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven because evaluation involves generating statistics from DLP scan logs and inventory update logs to measure effectiveness. Model-based because it involves verifying the configuration and setup of the DLP tool against expected models.",
        "Metric": [
            {
                "defination": "Proportion of enterprise assets covered by the DLP tool",
                "measure": [
                    {
                        "id": "M1",
                        "description": "Count of enterprise assets with the DLP tool properly configured and running"
                    },
                    {
                        "id": "M2",
                        "description": "Count of enterprise assets without the DLP tool or improperly configured"
                    }
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Time between consecutive DLP scans, indicating how frequently scans are performed",
                "measure": [
                    {
                        "id": "M3",
                        "description": "Timestamp of the last DLP scan"
                    },
                    {
                        "id": "M4",
                        "description": "Timestamp of the previous DLP scan"
                    }
                ],
                "equation": "M3 - M4"
            },
            {
                "defination": "Proportion of sensitive data instances identified by the DLP tool compared to the total in the data inventory, assessing identification completeness",
                "measure": [
                    {
                        "id": "M5",
                        "description": "Count of sensitive data instances identified in the last DLP scan"
                    },
                    {
                        "id": "M6",
                        "description": "Total number of sensitive data instances in the data inventory"
                    }
                ],
                "equation": "M5 / M6"
            },
            {
                "defination": "Freshness of the data inventory update relative to the last DLP scan, with higher values indicating better timeliness",
                "measure": [
                    {
                        "id": "M7",
                        "description": "Timestamp of the last data inventory update"
                    },
                    {
                        "id": "M8",
                        "description": "Timestamp of the last DLP scan"
                    }
                ],
                "equation": "1 / (M7 - M8) if M7 > M8, else 0"
            }
        ]
    },
    {
        "Observable": "Logs recording access, modification, and disposal of sensitive data, including timestamps, user identities, and data identifiers.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the configuration of logging systems (e.g., enabling logs for sensitive data) can be checked against system settings and policies. Measurable because the volume, coverage, and completeness of logged events can be analyzed using data-driven methods to assess enforcement.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based because it involves inspecting system configurations and models to verify if logging is enabled for sensitive data. Data-driven because it requires collecting and analyzing log data, such as event counts and timestamps, to evaluate the actual logging activity.",
        "Metric": [
            {
                "defination": "The proportion of systems that have logging enabled for sensitive data access, modification, and disposal.",
                "measure.description": "Count of systems with logging enabled for sensitive data and count of systems without such logging enabled.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            }
        ]
    },
    {
        "Observable": "The documented secure configuration process file, its metadata (e.g., creation date, last modified date), and records of reviews and updates (e.g., audit logs or change management entries indicating when reviews occurred).",
        "Class": [
            "Verifiable",
            "Checklist"
        ],
        "Class.explanation": "The safeguard belongs to the Verifiable class because the documentation can be directly inspected for existence, content, and compliance with requirements. It belongs to the Checklist class because automated scripts or tools can check for the presence of the document, validate metadata such as dates, and ensure it covers specified asset types without deep analysis.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation method is used because it involves examining the configuration and state of the documentation system, such as file properties and content. Data-driven method is used because it requires analyzing temporal data from metadata and logs, such as review dates and update frequencies, to assess compliance.",
        "Metric": [
            {
                "defination": "Indicates whether the documented secure configuration process exists.",
                "measure.description": "Existence of the documented process (1 if present, 0 if not)",
                "measure.id": "M1",
                "equation": "M1"
            },
            {
                "defination": "Indicates whether the last review of the documentation was conducted within the last 365 days, as required annually.",
                "measure.description": "Date of the last review (as a timestamp or date value)",
                "measure.id": "M2",
                "equation": "1 if (current_date - M2) <= 365 else 0"
            },
            {
                "defination": "Indicates whether the documentation was updated within the last 365 days or promptly after significant changes, though the 'significant changes' aspect may require manual verification.",
                "measure.description": "Date of the last update (as a timestamp or date value)",
                "measure.id": "M3",
                "equation": "1 if (current_date - M3) <= 365 else 0"
            }
        ]
    },
    {
        "Observable": "The documented secure configuration process for network devices, records of annual reviews, and logs of updates made in response to significant enterprise changes.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be assessed by verifying the existence and content of the documentation through manual inspection or automated checks if the document is in a standard format, as it involves checking for the presence of documents and their adherence to requirements.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "The evaluation relies on using the documented process as a model to compare against actual network device configurations or change management logs, assessing compliance and enforcement through configuration checks.",
        "Metric": [
            {
                "defination": "Score indicating whether the documented secure configuration process exists.",
                "measure.description": "Binary indicator of the presence of the documented process.",
                "measure.id": "M1",
                "equation": "M1"
            },
            {
                "defination": "Score indicating if the documentation was reviewed within the last year (365 days).",
                "measure.description": "Date of the last review of the documentation.",
                "measure.id": "M2",
                "equation": "1 if (current_date - M2) <= 365, else 0"
            },
            {
                "defination": "Score indicating if updates to the documentation were made promptly after significant enterprise changes.",
                "measure.description": "Date of the last update to the documentation and date of the last significant enterprise change.",
                "measure.id": "M3, M4",
                "equation": "1 if M4 is null or M3 > M4, else 0"
            }
        ]
    },
    {
        "Observable": "Configuration settings for automatic session locking on enterprise assets, including inactivity timeout values and enablement status, as well as logs or records of session locking events.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be assessed by directly checking the system configuration on each device to verify that automatic session locking is configured with the correct inactivity periods (e.g., through registry settings, plist files, or system preferences), without requiring scripting or data analytics.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is based on inspecting the configuration model of the devices, such as examining settings in operating systems or management tools, to determine compliance with the timeout requirements, without needing active probing or analysis of event logs.",
        "Metric": [
            {
                "defination": "The percentage of general purpose operating system devices that have automatic session locking configured with an inactivity timeout not exceeding 15 minutes.",
                "measure.description": "M1: Count of general purpose OS devices with session locking configured and timeout <= 15 minutes; M2: Count of general purpose OS devices with session locking not configured or timeout > 15 minutes.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "The percentage of mobile end-user devices that have automatic session locking configured with an inactivity timeout not exceeding 2 minutes.",
                "measure.description": "M3: Count of mobile devices with session locking configured and timeout <= 2 minutes; M4: Count of mobile devices with session locking not configured or timeout > 2 minutes.",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "The overall percentage of all enterprise assets (both general purpose OS and mobile devices) that comply with the session locking configuration requirements.",
                "measure.description": "M1: Count of compliant general purpose OS devices; M3: Count of compliant mobile devices; M5: Total number of general purpose OS devices; M6: Total number of mobile devices.",
                "measure.id": [
                    "M1",
                    "M3",
                    "M5",
                    "M6"
                ],
                "equation": "(M1 + M3) / (M5 + M6)"
            }
        ]
    },
    {
        "Observable": "Firewall configuration files, system logs indicating firewall status, and presence of firewall software on servers.",
        "Class": [
            "Checklist",
            "Verifiable"
        ],
        "Class.explanation": "Checklist: Can be automated with scripts to check for firewall installation and basic settings. Verifiable: Configurations can be verified against security policies through manual or automated checks.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based: Use configuration data to assess if firewalls are implemented and configured correctly. Active testing: Perform tests to verify that the firewall is effectively blocking unauthorized access.",
        "Metric": [
            {
                "definition": "Proportion of servers that have a firewall implemented out of those that support it.",
                "measure.description": [
                    "Number of servers that support firewall implementation",
                    "Number of servers with firewall implemented"
                ],
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M2 / M1"
            },
            {
                "definition": "Proportion of firewall rules that comply with security policy.",
                "measure.description": [
                    "Number of compliant firewall rules",
                    "Total number of firewall rules"
                ],
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / M4"
            }
        ]
    },
    {
        "Observable": "Configuration settings of host-based firewalls on end-user devices (e.g., rule sets, status), logs of firewall activities (e.g., traffic allowed or denied), presence of firewall software, and management tool configurations.",
        "Class": "Verifiable, Checklist",
        "Class.explanation": "This safeguard is verifiable because the firewall configuration can be inspected on each device to confirm the default-deny rule and explicitly allowed ports. It is checklist because a script or manual process can check for the presence and basic configuration of the firewall on all devices.",
        "Evaluation_Method": "Model-based, Active testing",
        "Evaluation_Method.explanation": "Model-based evaluation involves examining the firewall configuration files or settings to verify rules. Active testing involves probing the firewall with test traffic to ensure unauthorized traffic is dropped and allowed traffic passes.",
        "Metric": [
            {
                "definition": "Proportion of end-user devices with a host-based firewall installed and enabled.",
                "measure": [
                    {
                        "description": "Count of end-user devices with host-based firewall installed and enabled",
                        "id": "M1"
                    },
                    {
                        "description": "Total number of end-user devices",
                        "id": "M6"
                    }
                ],
                "equation": "M1 / M6"
            },
            {
                "definition": "Proportion of devices with firewall that have a default-deny rule configured (drop all traffic except explicitly allowed).",
                "measure": [
                    {
                        "description": "Count of devices with host-based firewall installed and enabled",
                        "id": "M1"
                    },
                    {
                        "description": "Count of devices with default-deny rule configured",
                        "id": "M3"
                    }
                ],
                "equation": "M3 / M1"
            },
            {
                "definition": "Effectiveness of traffic blocking based on firewall rules, measured by the ratio of blocked unauthorized traffic attempts to total unauthorized attempts (if logs are available).",
                "measure": [
                    {
                        "description": "Count of unauthorized traffic attempts blocked by the firewall (from logs)",
                        "id": "M7"
                    },
                    {
                        "description": "Count of unauthorized traffic attempts that were not blocked (if detectable, e.g., from penetration testing logs)",
                        "id": "M8"
                    }
                ],
                "equation": "M7 / (M7 + M8) if M8 data is available; otherwise, not applicable without active testing"
            }
        ]
    },
    {
        "Observable": "Presence of Infrastructure-as-Code (IaC) management tools and repositories, configuration files, access logs showing use of secure protocols (SSH, HTTPS) for administrative interfaces, and absence of insecure protocol (Telnet, HTTP) usage where not operationally essential.",
        "Class": [
            "Verifiable",
            "Checklist",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because system configurations for IaC and protocol settings can be checked; Checklist because automated scripts can verify protocol configurations and IaC usage; Measurable because proportions of secure management and protocol usage can be quantified from data.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based by examining system configurations and IaC definitions; Active testing by probing interfaces to detect protocol usage; Data-driven by analyzing access logs and management data to measure compliance.",
        "Metric": [
            {
                "defination": "Proportion of enterprise assets managed with version-controlled Infrastructure-as-Code",
                "measure.description": "M1: Count of assets with IaC management, M2: Count of assets without IaC management",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of administrative interfaces using secure protocols (SSH, HTTPS)",
                "measure.description": "M3: Count of interfaces using secure protocols, M5: Total number of administrative interfaces",
                "measure.id": "M3, M5",
                "equation": "M3 / M5"
            },
            {
                "defination": "Ratio of insecure protocol usage (to monitor avoidance, with lower values indicating better compliance)",
                "measure.description": "M4: Count of interfaces using insecure protocols (Telnet, HTTP), M5: Total number of administrative interfaces",
                "measure.id": "M4, M5",
                "equation": "M4 / M5"
            }
        ]
    },
    {
        "Observable": "The enabled/disabled status of default accounts (e.g., root, administrator), configuration settings indicating account management (e.g., account disable flags), and audit logs of account disablement or modification actions.",
        "Class": [
            "Checklist",
            "Verifiable"
        ],
        "Class.explanation": "Checklist because automated scripts can be used to inventory and check the status of default accounts; Verifiable because system configurations can be inspected to confirm if accounts are disabled or managed as required.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based because we can use configuration data and models to evaluate compliance with account management policies; Active testing because we can probe accounts by attempting authentication to test if they are disabled or unusable.",
        "Metric": [
            {
                "defination": "The proportion of default accounts that are disabled or made unusable, indicating compliance with the safeguard.",
                "measure.description": "Count of disabled default accounts and total number of default accounts.",
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "M1 / M3"
            },
            {
                "defination": "The proportion of systems where default accounts are managed (e.g., disabled), showing coverage of enforcement across assets.",
                "measure.description": "Count of systems with managed default accounts and total number of systems with default accounts.",
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "M4 / M5"
            }
        ]
    },
    {
        "Observable": "If the safeguard is enforced, unnecessary services are uninstalled or disabled on enterprise assets, observable through system configuration checks, service status logs, or asset management tools showing disabled or absent services.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be verified by inspecting the configuration of services on devices, such as checking service status in operating systems or configuration management systems, to ensure unnecessary services are not enabled, which aligns with verifiable class as it involves configuration checks without extensive data analytics.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "The evaluation involves examining the model of the system configuration, i.e., the current state of services on devices, to determine if unnecessary services are disabled or uninstalled, which is model-based as it relies on configuration data rather than active probing or data-driven analytics.",
        "Metric": [
            {
                "defination": "The proportion of unnecessary services that are either disabled or uninstalled, indicating compliance with the safeguard.",
                "measure.description": "Count of unnecessary services that are disabled (M_disable), count of unnecessary services that are uninstalled (M_uninstall), and total number of unnecessary services identified (M_total).",
                "measure.id": "M_disable, M_uninstall, M_total",
                "equation": "(M_disable + M_uninstall) / M_total"
            }
        ]
    },
    {
        "Observable": "DNS server configuration settings on network infrastructure devices, including the IP addresses of configured DNS servers and whether they match a list of trusted DNS servers (e.g., enterprise-controlled or reputable external ones).",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be verified by inspecting the configuration settings of network devices, such as routers or switches, to confirm if DNS servers are set to trusted IP addresses. It does not require data-driven analytics or active probing; instead, it relies on checking static configurations, which can be done through manual audits or automated scripts.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is based on the configuration model of the network devices. By examining the configured DNS server IPs against a predefined whitelist of trusted DNS servers, we can assess compliance without generating statistics from logs or performing active tests. This approach uses the system's configuration data to determine enforcement.",
        "Metric": {
            "defination": "Percentage of network devices with trusted DNS servers configured",
            "measure": {
                "id": "M1",
                "description": "Count of network devices where the DNS server configuration points to trusted IP addresses (enterprise-controlled or reputable external DNS servers)"
            },
            "equation": "M1 / M3, where M3 is the total number of network devices"
        }
    },
    {
        "Observable": "Configuration settings for device lockout policies (e.g., enablement status and threshold values in management tools like Microsoft InTune or Apple Configuration Profiles), logs of failed authentication attempts, and lockout events on portable end-user devices.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be verified by inspecting the device configuration settings to ensure that automatic lockout is enabled with the correct thresholds for failed authentication attempts, which can be done through scripting or manual checks of system configurations.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "The evaluation is based on examining the configuration models of devices, such as those managed by tools like InTune or Apple profiles, to confirm that lockout policies are set according to the specified thresholds, without needing active probing or extensive data analysis.",
        "Metric": [
            {
                "defination": "Proportion of portable end-user devices that have automatic device lockout enabled.",
                "measure.description": "Count of devices with lockout enabled and total number of portable devices",
                "measure.id": "M_enabled, M_total",
                "equation": "M_enabled / M_total"
            },
            {
                "defination": "Proportion of laptops that have automatic lockout enabled with a threshold of 20 or fewer failed attempts.",
                "measure.description": "Count of compliant laptops (lockout enabled and threshold \u226420) and total number of laptops",
                "measure.id": "M_laptop_comp, M_laptop_total",
                "equation": "M_laptop_comp / M_laptop_total"
            },
            {
                "defination": "Proportion of tablets and smartphones that have automatic lockout enabled with a threshold of 10 or fewer failed attempts.",
                "measure.description": "Count of compliant tablets/smartphones (lockout enabled and threshold \u226410) and total number of tablets/smartphones",
                "measure.id": "M_tablet_comp, M_tablet_total",
                "equation": "M_tablet_comp / M_tablet_total"
            },
            {
                "defination": "Overall proportion of all portable end-user devices that are compliant with the lockout policy (enabled and correct threshold based on device type).",
                "measure.description": "Sum of compliant devices (laptops and tablets/smartphones) and total number of portable devices",
                "measure.id": "M_laptop_comp, M_tablet_comp, M_total",
                "equation": "(M_laptop_comp + M_tablet_comp) / M_total"
            }
        ]
    },
    {
        "Observable": "Logs and records of remote wipe commands, confirmation of data wipes, MDM (Mobile Device Management) configuration settings, and event reports for devices being lost, stolen, or users departing.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the presence and configuration of remote wipe systems can be checked through administrative interfaces and system settings. Measurable because the execution, frequency, and success of wipe operations can be quantified and analyzed from event logs and data records.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven evaluation utilizes statistics from wipe event logs, success rates, and timeliness data. Model-based evaluation involves assessing the configuration, policies, and setup of the remote wipe system in MDM or similar tools.",
        "Metric": [
            {
                "defination": "Proportion of devices equipped with remote wipe capability.",
                "measure": [
                    {
                        "id": "M1",
                        "description": "Count of enterprise-owned portable devices with remote wipe capability enabled."
                    },
                    {
                        "id": "M2",
                        "description": "Count of enterprise-owned portable devices without remote wipe capability enabled."
                    }
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of wipe-required events where a command was sent.",
                "measure": [
                    {
                        "id": "M3",
                        "description": "Count of devices that experienced events requiring wipe (e.g., lost, stolen, user departure)."
                    },
                    {
                        "id": "M4",
                        "description": "Count of wipe commands sent for these events."
                    }
                ],
                "equation": "M4 / M3"
            },
            {
                "defination": "Success rate of wipe commands.",
                "measure": [
                    {
                        "id": "M4",
                        "description": "Count of wipe commands sent."
                    },
                    {
                        "id": "M5",
                        "description": "Count of successful wipe completions."
                    }
                ],
                "equation": "M5 / M4"
            },
            {
                "defination": "Average time from event report to wipe command sent.",
                "measure": [
                    {
                        "id": "M7",
                        "description": "Total time delay from event report to wipe command sent (in hours) for all events."
                    },
                    {
                        "id": "M4",
                        "description": "Count of wipe commands sent."
                    }
                ],
                "equation": "M7 / M4"
            }
        ]
    },
    {
        "Observable": "The configuration status of mobile devices indicating if a separate enterprise workspace is enabled, such as work profile settings, MDM configuration logs, or the presence of enterprise applications in isolated containers.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be verified by checking the configuration of mobile devices through Mobile Device Management (MDM) systems or device settings to confirm if a work profile or similar separation is implemented, as it involves inspecting system configurations rather than scripting or data analytics.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation can be done by examining the configuration models of mobile devices, such as reviewing MDM policies, device configuration profiles (e.g., Apple Configuration Profile or Android Work Profile), to ensure that separate workspaces are set up correctly, without requiring data-driven statistics or active probing.",
        "Metric": [
            {
                "definition": "The proportion of supported mobile devices that have a separate enterprise workspace configured and enabled.",
                "measure.description": "M2: Count of supported devices with separate enterprise workspace enabled, M3: Count of supported devices without separate enterprise workspace enabled",
                "measure.id": [
                    "M2",
                    "M3"
                ],
                "equation": "M2 / (M2 + M3)"
            }
        ]
    },
    {
        "Observable": "An account inventory containing details such as person's name, username, start/stop dates, department for all user, administrator, and service accounts, along with records of validation checks for authorization performed at least quarterly.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the inventory's existence and configuration can be checked against system settings and databases. Measurable because data analytics can be used to assess completeness, metadata quality, and validation frequency and effectiveness.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because evaluation requires analyzing statistics from inventory data and validation logs. Model-based because it involves verifying the configuration and setup of the inventory system to ensure it meets requirements.",
        "Metric": [
            {
                "defination": "Proportion of accounts that are listed in the inventory",
                "measure.description": [
                    "Count of accounts listed in the inventory",
                    "Count of active accounts not listed in the inventory"
                ],
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of accounts with complete metadata (name, username, start/stop dates, department)",
                "measure.description": [
                    "Count of accounts with complete metadata",
                    "Count of accounts with incomplete metadata"
                ],
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Time between validation checks, indicating frequency (should be \u2264 90 days for quarterly compliance)",
                "measure.description": [
                    "Time of the last validation check",
                    "Time of the previous validation check"
                ],
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 - M6"
            },
            {
                "defination": "Proportion of validated accounts that are authorized, measuring effectiveness of validation",
                "measure.description": [
                    "Count of unauthorized accounts identified during validation",
                    "Count of accounts validated in the last check"
                ],
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "1 - (M7 / M8)"
            },
            {
                "defination": "Proportion of active accounts that have been validated, measuring coverage of validation",
                "measure.description": [
                    "Count of accounts validated in the last check",
                    "Total number of active accounts"
                ],
                "measure.id": [
                    "M8",
                    "M9"
                ],
                "equation": "M8 / M9"
            }
        ]
    },
    {
        "Observable": "Password policy configurations in identity management systems, MFA settings, authentication logs, password compliance records, and account statuses.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because password policies and MFA settings can be checked directly in system configurations (e.g., Active Directory or IAM systems). Measurable because assessing password uniqueness and compliance with length requirements requires data-driven analysis from logs, audits, and user account data.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves verifying configuration settings and policies in identity management systems. Data-driven evaluation requires collecting and analyzing data from authentication logs, password audits, and account databases to measure compliance and uniqueness.",
        "Metric": [
            {
                "defination": "Password Uniqueness Rate",
                "measure.description": "Count of accounts with unique passwords, Count of accounts with reused or shared passwords",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "MFA Account Compliance Rate",
                "measure.description": "Count of accounts with MFA enabled and password length \u2265 8 characters, Count of accounts with MFA enabled",
                "measure.id": "M4, M3",
                "equation": "M4 / M3"
            },
            {
                "defination": "Non-MFA Account Compliance Rate",
                "measure.description": "Count of accounts without MFA and password length \u2265 14 characters, Count of accounts without MFA",
                "measure.id": "M7, M6",
                "equation": "M7 / M6"
            },
            {
                "defination": "Overall Password Policy Compliance Rate",
                "measure.description": "Count of accounts with MFA enabled and password length \u2265 8 characters, Count of accounts without MFA and password length \u2265 14 characters, Total number of user accounts",
                "measure.id": "M4, M7, M9",
                "equation": "(M4 + M7) / M9"
            }
        ]
    },
    {
        "Observable": "Account lists with last login timestamps, logs of account disablement or deletion actions, and identity management system configurations showing account status and policies.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the enforcement can be checked by verifying system configurations and logs for account management policies. Measurable because it requires quantitative analysis of inactivity periods and disablement actions over time.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because assessment involves generating statistics from event logs and user activity data to determine inactivity and actions. Model-based because it can involve checking configuration settings in identity management systems for account policies.",
        "Metric": [
            {
                "defination": "The proportion of dormant accounts (inactive for more than 45 days) that have been properly disabled or deleted.",
                "measure.description": [
                    "Count of dormant accounts disabled or deleted",
                    "Count of dormant accounts still active"
                ],
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "The proportion of all user accounts that are compliant with the policy, meaning they are either active within the last 45 days or dormant and disabled.",
                "measure.description": [
                    "Count of accounts active within the past 45 days",
                    "Count of dormant accounts disabled or deleted",
                    "Total number of user accounts"
                ],
                "measure.id": [
                    "M1",
                    "M3",
                    "M5"
                ],
                "equation": "(M1 + M3) / M5"
            },
            {
                "defination": "The proportion of user accounts that are dormant (inactive for more than 45 days), used for monitoring overall dormancy.",
                "measure.description": [
                    "Count of dormant accounts",
                    "Total number of user accounts"
                ],
                "measure.id": [
                    "M2",
                    "M5"
                ],
                "equation": "M2 / M5"
            }
        ]
    },
    {
        "Observable": "Configuration of user accounts showing privilege levels (e.g., admin vs. non-privileged), activity logs indicating which account is used for specific actions like web browsing, email, or administrative tasks, and evidence of policy enforcement such as account separation.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because account configurations and privilege settings can be directly inspected through system audits or configuration checks. Measurable because compliance can be quantified by analyzing activity logs to determine the frequency of correct account usage for general versus administrative activities.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves examining the configuration of accounts and policies to ensure dedicated admin accounts are set up. Data-driven evaluation uses logs of user activities to analyze patterns and measure adherence to the safeguard, such as counting activities from appropriate accounts.",
        "Metric": [
            {
                "defination": "The proportion of administrator accounts that are dedicated and not used for general computing activities.",
                "measure.description": "Count of dedicated admin accounts (those with no recorded general activities) and total number of admin accounts.",
                "measure.id": "M_dedicated_admin, M_total_admin",
                "equation": "M_dedicated_admin / M_total_admin"
            },
            {
                "defination": "The percentage of general computing activities (e.g., web browsing, email) performed from non-privileged accounts.",
                "measure.description": "Count of general activities from non-privileged accounts and total general activities logged.",
                "measure.id": "M_gen_non_priv, M_total_gen",
                "equation": "M_gen_non_priv / M_total_gen"
            }
        ]
    },
    {
        "Observable": "The maintained inventory of service accounts with metadata including department owner, review date, and purpose, along with audit logs of review activities and records of account status changes.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the inventory's existence, configuration, and metadata completeness can be checked against system settings and policies; Measurable because quantitative metrics such as inventory completeness and review frequency can be derived from data analysis.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based for assessing the configuration of the inventory system and policy adherence; Data-driven for analyzing event logs, review records, and account data to compute enforcement metrics.",
        "Metric": [
            {
                "defination": "Proportion of active service accounts that are documented in the inventory",
                "measure.description": "Count of service accounts listed in the inventory (M1), Count of active service accounts not listed in the inventory (M2)",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of service accounts in the inventory with complete metadata (department owner, review date, purpose)",
                "measure.description": "Count of service accounts with complete metadata (M3), Count of service accounts with incomplete metadata (M4)",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Time interval between consecutive reviews of service accounts, in days",
                "measure.description": "Time of the last review (M5), Time of the previous review (M6)",
                "measure.id": "M5, M6",
                "equation": "M5 - M6"
            },
            {
                "defination": "Proportion of reviewed service accounts that are authorized",
                "measure.description": "Count of unauthorized service accounts identified during review (M7), Count of service accounts reviewed (M8)",
                "measure.id": "M7, M8",
                "equation": "1 - (M7 / M8)"
            },
            {
                "defination": "Proportion of active service accounts that were reviewed in the last cycle",
                "measure.description": "Count of service accounts reviewed (M8), Total number of active service accounts (M9)",
                "measure.id": "M8, M9",
                "equation": "M8 / M9"
            }
        ]
    },
    {
        "Observable": "Logs of account management activities, configuration settings of directory or identity services (e.g., Active Directory, LDAP), integration status of systems with the centralized service, and user provisioning records.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of directory services can be checked through system settings and scripts. Measurable because data on account management (e.g., counts and logs) can be analyzed to assess the degree of centralization.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves verifying system configurations and integration settings. Data-driven evaluation requires analyzing logs, account counts, and audit events to measure centralization effectiveness.",
        "Metric": [
            {
                "defination": "Centralization Ratio - Proportion of user accounts managed through the centralized identity service.",
                "measure.description": "Count of user accounts managed centrally and total user accounts.",
                "measure.id": "M1, M3",
                "equation": "M1 / M3"
            },
            {
                "defination": "Decentralized Account Ratio - Proportion of user accounts not managed through the centralized identity service.",
                "measure.description": "Count of user accounts managed outside the centralized service and total user accounts.",
                "measure.id": "M2, M3",
                "equation": "M2 / M3"
            },
            {
                "defination": "Integration Coverage - Proportion of systems integrated with the centralized identity service.",
                "measure.description": "Count of systems integrated with the centralized service and total systems requiring user account access.",
                "measure.id": "M4, M5",
                "equation": "M4 / M5"
            },
            {
                "defination": "Governance Drift Indicator - Frequency of events indicating manual or decentralized account management, measured over a time period.",
                "measure.description": "Count of identity-related audit events (e.g., manual account creation) in a given time period.",
                "measure.id": "M6",
                "equation": "M6 / T (where T is the time period in days, for example)"
            }
        ]
    },
    {
        "Observable": "Documented access grant process, automation configuration in identity management systems, access logs, user change records, and policy documents",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the existence of documentation and system configuration for automation can be checked through audits or reviews. Measurable because the rate of process adherence and automation can be calculated from data such as access logs and user activity records.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven evaluation uses statistics from access logs and user activity data to measure adherence and automation rates. Model-based evaluation involves checking the configuration of identity management systems to verify automation settings and documentation existence.",
        "Metric": [
            {
                "defination": "Measures the proportion of access grants that adhere to the documented process",
                "measure.description": "Count of access grants that followed the documented process (M1), Count of access grants that did not follow the documented process (M2)",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Measures the proportion of access grants that are automated",
                "measure.description": "Count of automated access grants (M3), Count of manual access grants (M4)",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Checks if a documented process exists",
                "measure.description": "Binary indicator if a documented process exists (1 for yes, 0 for no) (M5)",
                "measure.id": "M5",
                "equation": "M5"
            }
        ]
    },
    {
        "Observable": "Documented process for access revocation, automated disablement scripts or tools, logs of account disablement actions, timestamps indicating when accounts were disabled relative to termination or role change events, audit trails showing disabled accounts.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the existence and configuration of the process can be checked through system audits and documentation reviews. Measurable because the effectiveness, timeliness, and compliance of access revocation can be quantified using data from logs and events.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven because analysis of event logs, timestamps, and account status changes provides statistical insights into disablement patterns. Model-based because the process configuration, automation settings, and documentation can be verified against established models or policies.",
        "Metric": [
            {
                "defination": "Access Revocation Compliance Rate - The proportion of access revocation events where accounts are disabled promptly.",
                "measure.description": "Count of user terminations or role changes that should trigger access revocation (M1), Count of accounts disabled in response to such events (M2)",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "Average Disablement Time - The mean time taken to disable accounts after revocation events.",
                "measure.description": "Time delay between event and disablement for each account (M3)",
                "measure.id": "M3",
                "equation": "Mean(M3)"
            },
            {
                "defination": "Process Automation Indicator - Whether an automated process for access revocation is established and used.",
                "measure.description": "Indicator if an automated process is in place (1 if yes, 0 if no) (M4)",
                "measure.id": "M4",
                "equation": "M4"
            },
            {
                "defination": "Documentation Compliance Rate - The existence and completeness of the documented process for access revocation.",
                "measure.description": "Existence of documented process (1 if yes, 0 if no) (M5), Count of process elements covered (e.g., owner, steps) (M6)",
                "measure.id": "M5, M6",
                "equation": "M5 * (M6 / total expected elements)"
            }
        ]
    },
    {
        "Observable": "MFA configuration settings on externally-exposed applications, integration with directory services or SSO for MFA enforcement, and authentication logs indicating MFA usage.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be verified by inspecting the configuration of applications and identity providers to ensure MFA is enabled where supported, as it involves checking system settings and policies without requiring extensive data analysis or active probing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Model-based evaluation is used because it involves examining the configuration models and policies of applications and identity providers to determine if MFA is enforced as required, relying on system configuration checks rather than data logs or active tests.",
        "Metric": {
            "defination": "The proportion of externally-exposed applications that support MFA and have it enforced.",
            "measure": {
                "description": "Count of applications with MFA enforced (M1), count of applications where MFA is not supported (M3), total count of externally-exposed applications (M4)",
                "id": "M1, M3, M4"
            },
            "equation": "M1 / (M4 - M3)"
        }
    },
    {
        "Observable": "Configuration settings in remote access systems (e.g., VPN, RDP gateways) requiring MFA, authentication logs showing MFA challenges and successes for remote access, user account settings indicating MFA enrollment for remote access.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be verified by inspecting the configuration of remote access systems and identity providers to confirm that MFA is required for network access, as it involves checking system settings rather than requiring complex data analysis or active probing alone.",
        "Evaluation_Method": "Model-based, Data-driven, Active testing",
        "Evaluation_Method.explanation": "Model-based: Evaluation involves analyzing configuration models of systems to ensure MFA is enforced. Data-driven: Statistics from authentication logs and event data are used to compute enforcement rates. Active testing: Probing the remote access endpoints by attempting access without MFA to verify blocking behavior.",
        "Metric": [
            {
                "defination": "Coverage of MFA enforcement across remote access methods",
                "measure.description": "M1: number of remote access methods (e.g., VPN endpoints) with MFA required in configuration, M2: number of remote access methods without MFA required",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "User compliance with MFA enrollment for remote access",
                "measure.description": "M3: number of user accounts with MFA enabled for remote access, M4: number of user accounts without MFA enabled for remote access",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Rate of MFA usage in remote access authentications",
                "measure.description": "M5: number of successful remote access authentications with MFA, M6: number of successful remote access authentications without MFA",
                "measure.id": "M5, M6",
                "equation": "M5 / (M5 + M6)"
            },
            {
                "defination": "Block rate for non-MFA access attempts",
                "measure.description": "M7: number of blocked remote access attempts due to lack of MFA, M6: number of successful remote access authentications without MFA",
                "measure.id": "M7, M6",
                "equation": "M7 / (M7 + M6)"
            }
        ]
    },
    {
        "Observable": "MFA configuration settings for administrative accounts in identity management systems, authentication event logs showing MFA usage, and list of enterprise assets with MFA support capabilities",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because MFA settings can be inspected and confirmed through system configurations such as IAM policies. Measurable because enforcement and usage can be quantified through data analysis of authentication logs and account settings.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves examining configuration models of identity and access management systems to verify MFA requirements. Data-driven evaluation uses authentication log data and event statistics to assess actual MFA usage and compliance.",
        "Metric": [
            {
                "defination": "MFA Configuration Compliance Rate for Administrative Accounts",
                "measure": [
                    {
                        "description": "Count of administrative accounts with MFA enabled",
                        "id": "M1"
                    },
                    {
                        "description": "Count of administrative accounts on assets that support MFA",
                        "id": "M2"
                    }
                ],
                "equation": "M1 / M2"
            },
            {
                "defination": "MFA Support Coverage Rate for Enterprise Assets",
                "measure": [
                    {
                        "description": "Count of enterprise assets that support MFA for administrative accounts",
                        "id": "M3"
                    },
                    {
                        "description": "Count of all enterprise assets",
                        "id": "M4"
                    }
                ],
                "equation": "M3 / M4"
            }
        ]
    },
    {
        "Observable": "Inventory document or database listing authentication and authorization systems, logs of review and update activities, and records from discovery tools.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the inventory's existence and content can be checked against system configurations and logs; Measurable because the coverage ratio and review frequency can be quantified using data analytics.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven because evaluation requires analyzing data from inventory logs, discovery tools, and timestamps; Model-based because it involves comparing the inventory against a conceptual model of expected authentication and authorization systems.",
        "Metric": [
            {
                "defination": "Inventory Coverage - The proportion of authentication and authorization systems that are documented in the inventory.",
                "measure": {
                    "description": "Count of systems listed in the inventory (M1) and count of systems detected but not listed (M2)",
                    "id": "M1, M2"
                },
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Review Frequency Compliance - Indicates whether the inventory is reviewed at least annually, as per the safeguard requirement.",
                "measure": {
                    "description": "Date of the last review (M3) and the current date (M5) for calculating the time elapsed",
                    "id": "M3, M5"
                },
                "equation": "1 if (M5 - M3) <= 365 else 0"
            }
        ]
    },
    {
        "Observable": "Configuration settings of a central directory service or SSO provider, integration status of enterprise assets with the central service, and logs of access control events indicating centralized authentication and authorization.",
        "Class": "Verifiable, Checklist",
        "Class.explanation": "Verifiable because the enforcement can be assessed by inspecting system configurations to confirm integration with the central service; Checklist because a list of assets can be used to systematically verify each one's integration status.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Model-based because the evaluation relies on examining the configuration models of enterprise assets to determine if they are set up to use the central directory service or SSO provider for access control.",
        "Metric": [
            {
                "defination": "The proportion of supported enterprise assets that have access control centralized through a directory service or SSO provider.",
                "measure.description": "M_supported: count of enterprise assets that support central access control, M_integrated: count of supported assets that are integrated with the central service",
                "measure.id": "M_supported, M_integrated",
                "equation": "M_integrated / M_supported"
            }
        ]
    },
    {
        "Observable": "Role definitions in identity management systems, documentation of access rights for each role (including department owner, review date, and purpose), logs of access control reviews, records of unauthorized privileges identified during reviews.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the existence and correctness of role definitions and documentation can be checked by inspecting system configurations and policy documents. Measurable because the frequency and outcomes of access control reviews require data analysis to assess compliance and effectiveness.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation is used to verify the configuration of role-based access control systems by examining role definitions and access rights. Data-driven evaluation is used to analyze review logs, timestamps, and findings to measure review frequency and compliance rates.",
        "Metric": [
            {
                "defination": "The proportion of roles that have their access rights documented, indicating completeness of role documentation.",
                "measure.description": "M1: Count of all roles defined in the enterprise. M2: Count of roles with access rights documented (including necessary details like owner, review date, and purpose).",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "The time interval between consecutive access control reviews, to ensure they are performed at least annually as required.",
                "measure.description": "M3: Timestamp of the most recent access control review. M4: Timestamp of the previous access control review.",
                "measure.id": "M3, M4",
                "equation": "M3 - M4"
            },
            {
                "defination": "The proportion of privileges found to be unauthorized during the last review, indicating the effectiveness of access control validation.",
                "measure.description": "M5: Count of unauthorized privileges identified in the most recent review. M6: Count of privileges reviewed in the most recent review.",
                "measure.id": "M5, M6",
                "equation": "M5 / M6"
            }
        ]
    },
    {
        "Observable": "The documented vulnerability management process, including its existence, content, review dates, update records, and evidence of updates after significant enterprise changes.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be assessed by verifying the existence and details of the documentation through manual inspection or automated checks of document metadata, such as review and update timestamps.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation involves examining the configuration and properties of the documentation, which are part of the system's model, including document existence, review dates, update history, and integration with change management systems, without requiring active probing or extensive data analytics.",
        "Metric": [
            {
                "defination": "Whether the vulnerability management process document is established",
                "measure.description": "Presence of the document (1 if exists, 0 otherwise)",
                "measure.id": "M1",
                "equation": "M1"
            },
            {
                "defination": "Compliance with the requirement to review the documentation at least annually",
                "measure.description": "Date of last review",
                "measure.id": "M2",
                "equation": "1 if (current_date - M2) <= 365 else 0"
            },
            {
                "defination": "Compliance with updating the documentation after significant enterprise changes",
                "measure.description": "Date of last update and date of last significant change",
                "measure.id": "M3, M4",
                "equation": "1 if (M4 is not set or M3 > M4) else 0"
            },
            {
                "defination": "Overall compliance with the safeguard, considering document existence and either annual review or update after changes",
                "measure.description": "Combination of document presence, review date, update date, and change date",
                "measure.id": "M1, M2, M3, M4",
                "equation": "1 if M1 and ( (current_date - M2) <= 365 or (M4 is not set or M3 > M4) ) else 0"
            }
        ]
    },
    {
        "Observable": "The documented remediation process, logs of review activities, and records of monthly reviews.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the existence and content of the documentation can be confirmed through inspection; Measurable because the frequency and timeliness of reviews can be quantified and analyzed over time.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based as it involves evaluating the documented remediation strategy as a configuration item; Data-driven as it requires collecting and analyzing data on review timestamps and frequencies from logs.",
        "Metric": [
            {
                "defination": "Indicates whether the remediation process document is established and maintained.",
                "measure.description": "Count of remediation process documents",
                "measure.id": "M1",
                "equation": "1 if M1 >= 1 else 0"
            },
            {
                "defination": "Percentage of months with at least one review, ensuring monthly or more frequent compliance.",
                "measure.description": "Number of months with at least one review and total number of months in the assessment period",
                "measure.id": "M2, M3",
                "equation": "M2 / M3"
            },
            {
                "defination": "Timeliness of the most recent review, checking if it was within the last 30 days.",
                "measure.description": "Time since last review in days",
                "measure.id": "M4",
                "equation": "1 if M4 <= 30 else 0"
            }
        ]
    },
    {
        "Observable": "Presence of automated patch management systems, logs of patch deployments, system update statuses, and timestamps of last updates for enterprise assets.",
        "Class": "Measurable",
        "Class.explanation": "This safeguard requires measuring the frequency and completeness of operating system updates, which involves data analysis from logs, reports, and system inventories to assess compliance, rather than just verifying configurations or using a checklist.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Evaluation involves collecting and analyzing data from patch management tools, event logs, and system inventories to generate statistics on update frequencies, automation coverage, and compliance rates, which is characteristic of a data-driven approach.",
        "Metric": [
            {
                "Metric.defination": "Proportion of enterprise assets that have automated patch management enabled.",
                "Metric.measure.description": [
                    "Count of assets with automated patch management enabled",
                    "Total number of enterprise assets"
                ],
                "Metric.measure.id": [
                    "M1",
                    "M2"
                ],
                "Metric.equation": "M1 / M2"
            },
            {
                "Metric.defination": "Proportion of assets that have been updated within the last 30 days, ensuring compliance with the monthly or more frequent update requirement.",
                "Metric.measure.description": [
                    "Count of assets with last operating system update within the last 30 days",
                    "Total number of enterprise assets"
                ],
                "Metric.measure.id": [
                    "M3",
                    "M2"
                ],
                "Metric.equation": "M3 / M2"
            }
        ]
    },
    {
        "Observable": "Automated patch management system configurations, patch deployment logs, application inventory with update status, and update schedules.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "This safeguard can be assessed through scripting to check if automated patch management is configured (Checklist), verifying system settings and configurations (Verifiable), and measuring update frequencies and compliance using data analytics (Measurable).",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation uses logs and data from patch management systems to analyze update frequencies and compliance; Model-based evaluation relies on configuration models to assess if automated patch management is properly set up; Active testing involves probing systems to test if updates are being applied as required.",
        "Metric": [
            {
                "definition": "Percentage of enterprise assets covered by automated patch management",
                "measure.description": "Count of assets with automated patch management enabled and count without it",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "definition": "Percentage of applications updated within the last 30 days",
                "measure.description": "Count of applications updated within the last 30 days and count not updated within that period",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Vulnerability scan reports, logs of scan activities, and configuration settings of scanning tools indicating scan schedules and types.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of scanning tools can be checked for scheduled scans and scan types (e.g., cron jobs or tool settings). Measurable because actual scan data from logs can be analyzed to compute frequency, coverage, and types of scans.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven evaluation uses historical scan log data to assess scan frequency and coverage. Model-based evaluation checks the configured settings of scanning tools to verify intended scan schedules and types.",
        "Metric": [
            {
                "defination": "Percentage of internal enterprise assets scanned within the last 90 days.",
                "measure.description": "M1: Total number of internal enterprise software assets. M2: Number of assets scanned at least once in the last 90 days.",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "Indicates whether both authenticated and unauthenticated scans have been conducted in the evaluation period.",
                "measure.description": "M3: Number of authenticated vulnerability scans conducted in the last 90 days. M4: Number of unauthenticated vulnerability scans conducted in the last 90 days.",
                "measure.id": "M3, M4",
                "equation": "1 if M3 > 0 and M4 > 0, else 0"
            }
        ]
    },
    {
        "Observable": "Scan reports or logs showing automated vulnerability scans performed on externally-exposed assets, including timestamps of scans and lists of assets covered.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": {
            "Checklist": "Automated scripts can check for the existence and execution of scan jobs by verifying logs or scheduling systems.",
            "Verifiable": "The configuration of vulnerability scanning tools can be inspected to ensure they are set up correctly for external assets.",
            "Measurable": "Data from scan logs and results can be quantified to assess frequency, coverage, and compliance with scanning policies."
        },
        "Evaluation_Method": [
            "Data-driven",
            "Active testing",
            "Model-based"
        ],
        "Evaluation_Method.explanation": {
            "Data-driven": "Evaluation involves analyzing statistics from scan logs, such as timestamps and asset coverage, to compute metrics like frequency and coverage.",
            "Active testing": "Vulnerability scans actively probe the externally-exposed assets to identify vulnerabilities, making this a form of active testing.",
            "Model-based": "The configuration models of scanning tools can be verified against policies to ensure proper setup and scheduling."
        },
        "Metric": [
            {
                "defination": "The proportion of externally-exposed assets that have been scanned within the last 30 days, indicating coverage compliance.",
                "measure.description": "M1: Total number of externally-exposed enterprise assets. M2: Number of assets scanned within the last 30 days.",
                "measure.id": "M1, M2",
                "equation": "Coverage_Ratio = M2 / M1"
            },
            {
                "defination": "The time elapsed between consecutive scan events, measured in days, to ensure scans are performed at least monthly.",
                "measure.description": "M3: Timestamp of the most recent scan event. M4: Timestamp of the previous scan event.",
                "measure.id": "M3, M4",
                "equation": "Scan_Interval = M3 - M4"
            }
        ]
    },
    {
        "Observable": "Logs of vulnerability detections and remediations, including timestamps, status changes, and records from vulnerability management tools and processes.",
        "Class": "Measurable",
        "Class.explanation": "The safeguard requires quantifying the time between vulnerability detection and remediation to ensure it is within a monthly period, which involves measurement of numerical values such as counts and time intervals, rather than simple checklist verification or configuration checks.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Evaluation involves collecting and analyzing data from vulnerability management systems, such as event logs, scan reports, and remediation records, to compute statistics on remediation times and rates, which is characteristic of a data-driven approach.",
        "Metric": [
            {
                "defination": "Percentage of vulnerabilities remediated within 30 days of detection",
                "measure.description": "Count of vulnerabilities detected in a defined period (e.g., per month) and count of those remediated within 30 days",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "Frequency of remediation process execution",
                "measure.description": "Count of remediation process executions in the last 30 days",
                "measure.id": "M3",
                "equation": "M3"
            }
        ]
    },
    {
        "Observable": "The documented audit log management process, including its content addressing collection, review, and retention of audit logs, as well as records of annual reviews and updates triggered by significant changes.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be verified by inspecting the documentation to ensure it exists, covers the required elements (collection, review, retention), and has evidence of reviews and updates, which can be done through manual checks or automated validation of document metadata.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is performed by reviewing the documented process model and associated records (e.g., document versions, review logs) to assess compliance with the requirements, without requiring active probing or statistical analysis of event data.",
        "Metric": [
            {
                "defination": "Measures the completeness of the documented process in covering the required elements: collection, review, and retention of audit logs.",
                "measure.description": "Binary scores indicating the presence of the document and coverage of each requirement: collection, review, retention.",
                "measure.id": [
                    "M1",
                    "M2",
                    "M3",
                    "M4"
                ],
                "equation": "(M1 * (M2 + M3 + M4)) / 3"
            },
            {
                "defination": "Measures whether the documentation is reviewed at least annually, as specified.",
                "measure.description": "Date of the last review and the current date, used to calculate the time since the last review.",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "1 if (M6 - M5) <= 365 else 0"
            }
        ]
    },
    {
        "Observable": "Audit logs, configuration settings indicating logging enabled, and records from the audit log management process.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "The safeguard is verifiable through inspection of system configurations to check if logging is enabled, and measurable through analysis of log data volumes and coverage.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves checking the configuration models of assets to verify logging enablement. Data-driven evaluation involves analyzing audit log data to ensure collection and coverage.",
        "Metric": [
            {
                "Metric.defination": "The proportion of enterprise assets that have audit logging enabled.",
                "Metric.measure.description": "M1: Count of assets with logging enabled, M3: Total number of enterprise assets.",
                "Metric.measure.id": [
                    "M1",
                    "M3"
                ],
                "Metric.equation": "Coverage = M1 / M3"
            },
            {
                "Metric.defination": "The volume of audit logs collected, indicating active log collection.",
                "Metric.measure.description": "M4: Volume of audit logs collected, measured in events or megabytes.",
                "Metric.measure.id": [
                    "M4"
                ],
                "Metric.equation": "Log_Volume = M4"
            }
        ]
    },
    {
        "Observable": "Storage capacity settings and current usage data of logging destinations, such as configuration files, monitoring dashboards, and log files indicating storage status.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because storage adequacy can be checked by reviewing configuration settings of logging systems to ensure they meet policy requirements. Measurable because storage usage data can be collected and analyzed over time to assess compliance with the audit log management process.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven evaluation involves generating statistics from storage usage logs and monitoring data to track adequacy. Model-based evaluation uses the configured storage limits and policy requirements to model and verify compliance without active probing.",
        "Metric": [
            {
                "defination": "Percentage of logging destinations that have configured storage capacity meeting or exceeding the enterprise's required minimum storage capacity.",
                "measure.description": "Configured storage capacity of each logging destination, required minimum storage capacity as per policy, and total number of logging destinations.",
                "measure.id": "M1, M3, M4",
                "equation": "(Count of destinations where configured capacity >= required capacity) / total number of destinations"
            },
            {
                "defination": "Percentage of logging destinations where current storage usage is within safe limits (e.g., below 80% of configured capacity) to prevent overflow and ensure log retention.",
                "measure.description": "Current storage usage of each logging destination, configured storage capacity of each logging destination, and total number of logging destinations.",
                "measure.id": "M2, M1, M4",
                "equation": "(Count of destinations where current usage < 0.8 * configured capacity) / total number of destinations"
            }
        ]
    },
    {
        "Observable": "Configuration settings for time synchronization sources on assets, logs from time synchronization services (e.g., NTP), status reports indicating time synchronization status and sources.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the presence and configuration of at least two time sources can be checked through system inspection of configuration files or settings. Measurable because the degree of synchronization (e.g., time drift) can be quantified using data from logs and monitoring tools.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves verifying configuration files or system settings to confirm the number of time sources configured. Data-driven evaluation uses statistics from time synchronization logs to measure synchronization accuracy and compliance over time.",
        "Metric": [
            {
                "defination": "The percentage of assets that support time synchronization and have at least two time sources configured.",
                "measure.description": "M_supported: Count of assets where time synchronization is supported. M_compliant: Count of supported assets with at least two time sources configured.",
                "measure.id": [
                    "M_supported",
                    "M_compliant"
                ],
                "equation": "M_compliant / M_supported"
            },
            {
                "defination": "The percentage of all enterprise assets that support time synchronization.",
                "measure.description": "M_supported: Count of assets where time synchronization is supported. M_total: Total number of enterprise assets.",
                "measure.id": [
                    "M_supported",
                    "M_total"
                ],
                "equation": "M_supported / M_total"
            },
            {
                "defination": "The percentage of supported assets with time synchronization within acceptable limits (e.g., time drift less than 100 milliseconds).",
                "measure.description": "M_supported: Count of assets where time synchronization is supported. M_in_sync: Count of supported assets with time drift within the acceptable threshold.",
                "measure.id": [
                    "M_supported",
                    "M_in_sync"
                ],
                "equation": "M_in_sync / M_supported"
            }
        ]
    },
    {
        "Observable": "Audit logs generated by enterprise assets containing sensitive data, including elements such as event source, date, username, timestamp, source addresses, destination addresses, and other forensic details, along with configuration settings for audit logging.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": [
            "Scripts can be used to automatically check if audit logging is configured with the required elements, such as by parsing configuration files or using command-line tools.",
            "System configuration can be verified manually or through automated checks to ensure audit logging is enabled and includes specified details like event source and timestamps.",
            "Log data must be analyzed using data-driven methods to assess the completeness and presence of required elements in the logs, which involves statistical analysis."
        ],
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": [
            "Evaluation relies on analyzing event log data to check for the presence and completeness of required elements, such as by aggregating and examining log entries.",
            "Evaluation uses the configuration model of the system, such as checking audit policy settings or configuration files, to verify if logging is properly set up.",
            "Evaluation involves probing the system by generating test events and checking if they are captured in the logs with all required details, ensuring functional logging."
        ],
        "Metric": [
            {
                "defination": "Measures the proportion of sensitive data assets that have audit logging configured.",
                "measure.description": "M1: Total number of enterprise assets containing sensitive data; M2: Number of assets with audit logging configured and enabled.",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "Measures the proportion of audit log entries that include all required elements (event source, date, username, timestamp, source addresses, destination addresses, and other forensic details).",
                "measure.description": "M3: Number of audit log entries that contain all required elements; M4: Number of audit log entries missing one or more required elements.",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "DNS query audit logs being generated, stored, and the logging configuration on enterprise assets where supported.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard is verifiable because it can be assessed by examining the configuration settings of assets to confirm that DNS query logging is enabled where appropriate and supported, without necessarily requiring complex data analysis.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Data-driven evaluation is appropriate because assessing the collection of DNS query logs involves analyzing the actual log data to verify that logs are being captured, which is essential for detection purposes in security monitoring.",
        "Metric": [
            {
                "defination": "The proportion of supported enterprise assets that have DNS query logging enabled, indicating coverage of log collection.",
                "measure.description": "M1: Count of enterprise assets that support DNS query logging (appropriate and supported), M2: Count of assets with DNS query logging enabled",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            }
        ]
    },
    {
        "Observable": "URL request audit logs being collected on enterprise assets, and configuration settings indicating that URL request logging is enabled where supported.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because system configurations can be inspected to check if URL request logging is enabled on assets. Measurable because the volume, frequency, and coverage of log collection can be quantified and analyzed using data from logs.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves examining system configurations and settings to verify if URL request logging is enabled. Data-driven evaluation involves analyzing collected log data, such as the number of log entries and timestamps, to assess the effectiveness and completeness of log collection.",
        "Metric": [
            {
                "defination": "The percentage of enterprise assets that support URL request logging and have it enabled, indicating the coverage of logging configuration.",
                "measure.description": [
                    "M1: Count of enterprise assets that support URL request logging and have it enabled.",
                    "M2: Count of enterprise assets that support URL request logging but do not have it enabled."
                ],
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "The percentage of assets with URL request logging enabled that are actively generating logs, indicating the effectiveness of log collection.",
                "measure.description": [
                    "M1: Count of enterprise assets that support URL request logging and have it enabled.",
                    "M3: Count of enterprise assets that have generated at least one URL request log in the last 24 hours."
                ],
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "M3 / M1"
            }
        ]
    },
    {
        "Observable": "Command-line audit logs being collected and stored in a centralized log management system or repository, visible through log entries from sources like PowerShell, BASH, and remote administrative terminals.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the configuration of logging can be checked on systems (e.g., registry settings in Windows, configuration files in Linux). Measurable because the actual log data needs to be analyzed for volume, coverage, and timeliness to assess enforcement quality.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven: Analyze collected logs to generate statistics on log volume and coverage. Model-based: Inspect system configurations to verify if logging is enabled and properly set up. Active testing: Execute command-line commands to probe systems and verify that logs are generated and collected.",
        "Metric": [
            {
                "defination": "Enablement Coverage - Proportion of systems with command-line logging enabled.",
                "measure.description": "Count of systems with command-line logging enabled and total number of systems that should have logging.",
                "measure.id": "enabled_systems_count, total_systems_count",
                "equation": "enabled_systems_count / total_systems_count"
            },
            {
                "defination": "Source Coverage - Proportion of command-line source types (e.g., PowerShell, BASH, remote administrative terminals) with logs collected.",
                "measure.description": "Number of source types with logs collected and total expected source types.",
                "measure.id": "sources_with_logs_count, total_expected_sources",
                "equation": "sources_with_logs_count / total_expected_sources"
            },
            {
                "defination": "Log Freshness - Inverse of time since the last log entry, indicating how up-to-date the logs are.",
                "measure.description": "Timestamp of the most recent log entry and current timestamp.",
                "measure.id": "last_log_timestamp, current_timestamp",
                "equation": "1 / (current_timestamp - last_log_timestamp)"
            }
        ]
    },
    {
        "Observable": "Centralized audit log collection system (e.g., SIEM), logs being ingested from enterprise assets, configured retention policies in the system, documentation of the audit log management process, and evidence of log retention compliance.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the enforcement can be checked by inspecting system configurations, such as SIEM settings and log source integrations, against the documented process. Measurable because quantitative metrics like coverage percentages and compliance ratios can be derived from data counts and periods.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation involves analyzing log data, event volumes, and retention logs to assess centralization and compliance. Model-based evaluation involves reviewing configuration files, SIEM settings, and process documentation to verify adherence to the safeguard.",
        "Metric": [
            {
                "defination": "The percentage of enterprise assets that are sending audit logs to the central collection system.",
                "measure.description": "M1: Count of enterprise assets sending audit logs to the central system, M6: Total number of enterprise assets that should send logs (from inventory)",
                "measure.id": [
                    "M1",
                    "M6"
                ],
                "equation": "M1 / M6"
            },
            {
                "defination": "The ratio of configured retention period to documented retention period, indicating compliance level (capped at 1 for over-compliance).",
                "measure.description": "M3: Configured retention period in the central system (in days), M4: Documented retention period from the audit log management process (in days)",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "min(M3 / M4, 1)"
            },
            {
                "defination": "The percentage of expected log source types that are integrated into the central system.",
                "measure.description": "M5: Count of log source types integrated into the central system, M7: Expected number of log source types from documentation",
                "measure.id": [
                    "M5",
                    "M7"
                ],
                "equation": "M5 / M7"
            }
        ]
    },
    {
        "Observable": "Audit log files and their retention configurations across all enterprise assets, including log timestamps and policy settings.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the retention policy can be checked against system configurations (e.g., via scripts or manual inspection). Measurable because the actual retention duration needs to be calculated from log timestamps using data analysis.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves inspecting configured log retention policies in systems (e.g., checking settings in log management tools). Data-driven evaluation requires analyzing timestamps of audit logs to determine the actual retention duration (e.g., from event logs or database entries).",
        "Metric": [
            {
                "defination": "Percentage of enterprise assets with audit log retention policy configured to retain logs for at least 90 days.",
                "measure": {
                    "id": "M1",
                    "description": "Count of assets where the log retention policy is set to a minimum of 90 days."
                },
                "equation": "M1 / M5 * 100"
            },
            {
                "defination": "Percentage of enterprise assets where audit logs are actually retained for at least 90 days, based on the oldest log entry.",
                "measure": {
                    "id": "M3",
                    "description": "Count of assets where the oldest audit log entry is dated more than 90 days ago."
                },
                "equation": "M3 / M5 * 100"
            }
        ]
    },
    {
        "Observable": "Audit logs being reviewed, records of review activities (including timestamps), and reports of detected anomalies or abnormal events.",
        "Class": [
            {
                "name": "Measurable",
                "explanation": "The safeguard involves measuring the frequency of reviews and the rate of anomaly detection from audit log data, requiring data-driven analytics rather than simple checklist or configuration verification."
            }
        ],
        "Evaluation_Method": [
            {
                "name": "Data-driven",
                "explanation": "Evaluation requires generating statistics from audit logs, review timestamps, and anomaly counts to assess compliance with the review frequency and detection effectiveness."
            }
        ],
        "Metric": [
            {
                "defination": "Weekly Review Compliance - indicates whether at least one audit log review was conducted within the past 7 days.",
                "measure": [
                    {
                        "description": "Count of audit log reviews conducted in the past 7 days",
                        "id": "M1"
                    }
                ],
                "equation": "1 if M1 > 0, else 0"
            },
            {
                "defination": "Anomaly Detection Rate - the average number of anomalies detected per review conducted in the past 7 days.",
                "measure": [
                    {
                        "description": "Count of anomalies detected in the past 7 days' reviews",
                        "id": "M2"
                    },
                    {
                        "description": "Count of audit log reviews conducted in the past 7 days",
                        "id": "M1"
                    }
                ],
                "equation": "M2 / M1 if M1 > 0, else undefined"
            },
            {
                "defination": "Review Interval Compliance - indicates whether the time between the last two consecutive reviews is within 7 days.",
                "measure": [
                    {
                        "description": "Timestamp of the most recent audit log review",
                        "id": "M3"
                    },
                    {
                        "description": "Timestamp of the audit log review immediately before the most recent one",
                        "id": "M4"
                    }
                ],
                "equation": "1 if (M3 - M4) <= 7 days, else 0"
            }
        ]
    },
    {
        "Observable": "Logs from service providers including authentication, authorization, data creation, data disposal, and user management events, as well as the configuration of log collection systems.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of log collection tools can be inspected to check if they are set up correctly. Measurable because the actual log data needs to be analyzed for volume, coverage, and timeliness to assess enforcement quality.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves examining the configuration models of log collection systems to verify settings. Data-driven evaluation involves generating statistics from log data, such as event counts and timestamps, to analyze compliance and effectiveness.",
        "Metric": [
            {
                "defination": "Percentage of service providers with log collection enabled for supported events",
                "measure.description": "Count of service providers with log collection enabled and count without it enabled or not supported",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Count of authentication events logged in the last 24 hours, indicating detection capability",
                "measure.description": "Number of authentication events logged",
                "measure.id": "M3",
                "equation": "M3"
            },
            {
                "defination": "Count of authorization events logged in the last 24 hours, indicating detection capability",
                "measure.description": "Number of authorization events logged",
                "measure.id": "M4",
                "equation": "M4"
            },
            {
                "defination": "Count of data creation events logged in the last 24 hours, indicating detection capability",
                "measure.description": "Number of data creation events logged",
                "measure.id": "M5",
                "equation": "M5"
            },
            {
                "defination": "Count of data disposal events logged in the last 24 hours, indicating detection capability",
                "measure.description": "Number of data disposal events logged",
                "measure.id": "M6",
                "equation": "M6"
            },
            {
                "defination": "Count of user management events logged in the last 24 hours, indicating detection capability",
                "measure.description": "Number of user management events logged",
                "measure.id": "M7",
                "equation": "M7"
            },
            {
                "defination": "Freshness score of logs, with higher values indicating newer logs and better timeliness",
                "measure.description": "Timestamp of the most recent log entry",
                "measure.id": "M8",
                "equation": "1 / (current_time - M8)"
            }
        ]
    },
    {
        "Observable": "The inventory of browsers and email clients executing on enterprise assets, including their versions and comparison to vendor-provided lists of supported and latest versions.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be assessed by verifying the configuration of systems to ensure only supported and latest versions of browsers and email clients are installed, which involves checking software inventories and comparing them against vendor support lists, making it verifiable through configuration inspection.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Enforcement is evaluated by modeling the system's software inventory data against the list of supported versions provided by the vendor, using configuration-based assessment without the need for active probing or extensive data analysis from logs.",
        "Metric": [
            {
                "definition": "Proportion of browsers and email clients that are fully supported and latest version",
                "measure.description": "Number of supported and latest instances (M1) and total number of instances (M3)",
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "M1 / M3"
            },
            {
                "definition": "Proportion of assets that have only supported and latest browsers and email clients",
                "measure.description": "Number of compliant assets (M4) and total number of assets (M6)",
                "measure.id": [
                    "M4",
                    "M6"
                ],
                "equation": "M4 / M6"
            }
        ]
    },
    {
        "Observable": "DNS filtering configuration settings on end-user devices, DNS query logs showing blocked or allowed domains, reports from DNS filtering services indicating blocked access attempts, and lists of known malicious domains used for filtering.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Checklist: Can be assessed through automated scripts that check DNS settings on devices. Verifiable: Can be verified by inspecting system configurations and service settings. Measurable: Requires data-driven analysis from logs to quantify blocking effectiveness and coverage.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven: Analyze DNS query logs and statistics to measure blocking rates and coverage. Model-based: Examine configuration models of devices to verify DNS filtering settings. Active testing: Probe the network by attempting to access known malicious domains and observe if they are blocked.",
        "Metric": [
            {
                "definition": "The proportion of end-user devices that have DNS filtering enabled, indicating coverage compliance.",
                "measure.description": "M1: Count of end-user devices with DNS filtering service configured and enabled. M5: Total number of end-user devices.",
                "measure.id": "M1, M5",
                "equation": "M1 / M5"
            },
            {
                "definition": "The percentage of DNS queries to known malicious domains that are successfully blocked, indicating filtering effectiveness.",
                "measure.description": "M3: Count of DNS queries made to domains classified as known malicious. M4: Count of DNS queries blocked by the filtering service for known malicious domains.",
                "measure.id": "M3, M4",
                "equation": "IF(M3 > 0, M4 / M3, 1) * 100%"
            }
        ]
    },
    {
        "Observable": "Network traffic logs indicating blocked and allowed URL connections, configuration settings of URL filtering systems (e.g., firewall rules, proxy settings), and records of filter list updates (e.g., update timestamps, change logs).",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable: The configuration of URL filters can be inspected on network devices or systems to ensure they are enabled and set up according to standards (e.g., category-based or block lists). Measurable: The effectiveness and enforcement can be quantified through data analysis of network traffic logs to compute rates of blocking, update frequencies, and coverage.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based: Evaluation involves examining the configuration models of the URL filtering system to verify compliance with enforcement policies (e.g., checking if filters are applied to all assets). Data-driven: Evaluation requires analyzing historical network traffic data and update logs to derive metrics such as blocking rates and update frequencies from event logs and traffic flows.",
        "Metric": [
            {
                "defination": "Proportion of enterprise assets that have URL filtering enforced, indicating coverage of the safeguard.",
                "measure.description": "Count of assets with URL filters enabled and total number of enterprise assets.",
                "measure.id": "M1, M3",
                "equation": "M1 / M3"
            },
            {
                "defination": "Number of URL filter updates per month, measuring how frequently filters are updated to address new threats.",
                "measure.description": "Count of filter updates in the last 30 days.",
                "measure.id": "M4",
                "equation": "M4"
            },
            {
                "defination": "Ratio of blocked malicious connection attempts to total malicious attempts, assessing the effectiveness of the filters in blocking threats.",
                "measure.description": "Count of blocked connections to malicious URLs and count of attempted connections to malicious URLs.",
                "measure.id": "M7, M6",
                "equation": "M7 / M6"
            },
            {
                "defination": "Proportion of malicious connection attempts that were not blocked, indicating potential security gaps.",
                "measure.description": "Count of allowed connections to malicious URLs and count of attempted connections to malicious URLs.",
                "measure.id": "M8, M6",
                "equation": "M8 / M6"
            },
            {
                "defination": "Proportion of approved connection attempts that were incorrectly blocked, measuring the accuracy of the filters to avoid disrupting legitimate traffic.",
                "measure.description": "Count of blocked connections to approved URLs and count of attempted connections to approved URLs.",
                "measure.id": "M10, M9",
                "equation": "M10 / M9"
            }
        ]
    },
    {
        "Observable": "The list of installed plugins, extensions, and add-ons in browsers and email clients, along with their status (enabled or disabled), and evidence of uninstallation or disablement actions.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be assessed by verifying the configuration settings of browsers and email clients to ensure that unauthorized or unnecessary plugins are either uninstalled or disabled, which involves checking system configurations rather than requiring data analytics or active probing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is performed by examining the configuration models and inventory of plugins in browsers and email clients, using system settings and logs to determine compliance, without the need for generating statistics from event logs or active testing.",
        "Metric": [
            {
                "defination": "System Compliance Rate - The percentage of browsers and email clients that have no unauthorized plugins enabled and all unnecessary plugins are restricted.",
                "measure.description": "Count of compliant systems and total systems assessed",
                "measure.id": "M2, M1",
                "equation": "M2 / M1"
            },
            {
                "defination": "Plugin Restriction Effectiveness - The percentage of unauthorized plugins that are either uninstalled or disabled.",
                "measure.description": "Count of restricted unauthorized plugins and total unauthorized plugins detected",
                "measure.id": "M4, M3",
                "equation": "M4 / M3"
            }
        ]
    },
    {
        "Observable": "DNS records for SPF, DKIM, and DMARC policies; email headers indicating SPF, DKIM, and DMARC validation results; logs of email transactions showing policy enforcement",
        "Class": [
            "Checklist",
            "Verifiable"
        ],
        "Class.explanation": "Checklist because the safeguard can be assessed through automated scripting to check DNS records for SPF, DKIM, and DMARC. Verifiable because it involves verifying the configuration settings of these DNS records to ensure they are correctly implemented.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based because evaluation relies on inspecting the configured DNS records for SPF, DKIM, and DMARC to assess compliance. Active testing because it involves sending test emails to probe the system and verify that email validation policies are enforced correctly.",
        "Metric": [
            {
                "defination": "Percentage of domains that have an SPF record implemented",
                "measure.description": "Count of domains with an SPF record (M1), Count of total domains (M10)",
                "measure.id": [
                    "M1",
                    "M10"
                ],
                "equation": "M1 / M10"
            },
            {
                "defination": "Percentage of domains that have a DKIM record implemented",
                "measure.description": "Count of domains with a DKIM record (M4), Count of total domains (M10)",
                "measure.id": [
                    "M4",
                    "M10"
                ],
                "equation": "M4 / M10"
            },
            {
                "defination": "Percentage of domains that have a DMARC record implemented",
                "measure.description": "Count of domains with a DMARC record (M7), Count of total domains (M10)",
                "measure.id": [
                    "M7",
                    "M10"
                ],
                "equation": "M7 / M10"
            },
            {
                "defination": "Percentage of domains with DMARC that have a strong policy (p=reject or p=quarantine)",
                "measure.description": "Count of domains with DMARC and policy set to p=reject or p=quarantine (M9), Count of domains with a DMARC record (M7)",
                "measure.id": [
                    "M9",
                    "M7"
                ],
                "equation": "M9 / M7"
            },
            {
                "defination": "Email spoofing prevention rate through active testing",
                "measure.description": "Number of test emails sent that should be blocked due to spoofing but are not blocked (M11), Total number of test emails sent for spoofing (M12)",
                "measure.id": [
                    "M11",
                    "M12"
                ],
                "equation": "1 - (M11 / M12)"
            }
        ]
    },
    {
        "Observable": "Configuration settings of email gateways defining blocked file types, and log entries of blocked email attempts.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration can be directly inspected to ensure unnecessary file types are blocked. Measurable because the number of blocked attempts can be quantified from logs.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves checking the configuration models of email gateways. Data-driven evaluation involves analyzing log data to measure the effectiveness of blocking.",
        "Metric": [
            {
                "defination": "The ratio of email gateways that are properly configured to block unnecessary file types.",
                "measure": [
                    {
                        "id": "M1",
                        "description": "Count of email gateways correctly configured to block unnecessary file types"
                    },
                    {
                        "id": "M2",
                        "description": "Count of email gateways not correctly configured to block unnecessary file types"
                    }
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "The proportion of attempts to send unnecessary file types that are blocked.",
                "measure": [
                    {
                        "id": "M3",
                        "description": "Count of email messages with unnecessary file types that were blocked"
                    },
                    {
                        "id": "M4",
                        "description": "Count of email messages with unnecessary file types that were allowed"
                    }
                ],
                "equation": "M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Configuration settings of email servers indicating anti-malware is enabled, logs of email scans including attachment scanning and sandboxing events, detection alerts, and timestamps of anti-malware definition updates.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the presence and configuration of anti-malware protections can be checked through system inspections or configuration files. Measurable because operational metrics like scan rates and detection effectiveness can be calculated from log data and event records.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves inspecting configuration settings of email servers to verify that anti-malware features are deployed and configured. Active testing involves probing the system by sending test emails with attachments to verify that scanning, sandboxing, and detection are functioning correctly.",
        "Metric": [
            {
                "defination": "Proportion of email servers with anti-malware protection enabled",
                "measure.description": "M1: number of email servers with anti-malware protection enabled, M2: number of email servers without anti-malware protection enabled",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Percentage of incoming emails that are scanned by anti-malware",
                "measure.description": "M3: total number of emails received in a specified time period, M4: number of emails scanned by anti-malware in that period",
                "measure.id": "M3, M4",
                "equation": "M4 / M3"
            },
            {
                "defination": "Rate of malware detection among scanned emails",
                "measure.description": "M4: number of emails scanned, M5: number of malware detections reported in logs",
                "measure.id": "M4, M5",
                "equation": "M5 / M4"
            },
            {
                "defination": "Rate of false positives among scanned emails",
                "measure.description": "M4: number of emails scanned, M6: number of false positives (incorrect detections) reported",
                "measure.id": "M4, M6",
                "equation": "M6 / M4"
            },
            {
                "defination": "Compliance with anti-malware definition update requirement (e.g., updated within 24 hours)",
                "measure.description": "M7: time since last anti-malware definition update (in seconds)",
                "measure.id": "M7",
                "equation": "1 if M7 <= 86400 else 0"
            }
        ]
    },
    {
        "Observable": "Anti-malware software installation status, configuration settings, scan logs, update logs, and detection events on enterprise assets.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable: The deployment of anti-malware can be verified by checking system configurations, installation records, and presence on devices. Measurable: The maintenance and effectiveness can be measured through data on update frequencies, scan rates, and detection counts, requiring quantitative analysis.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based: Installation and configuration compliance can be assessed using system models and configuration checks. Data-driven: Metrics like update status and scan frequencies are derived from logs and event data. Active testing: The system can be probed with test malware to verify detection capabilities and response.",
        "Metric": [
            {
                "defination": "Proportion of enterprise assets with anti-malware software installed.",
                "measure.description": "Count of assets with anti-malware installed (M1), total number of enterprise assets (M5)",
                "measure.id": "M1, M5",
                "equation": "Coverage = M1 / M5"
            },
            {
                "defination": "Proportion of assets with anti-malware that have recent definition updates.",
                "measure.description": "Count of assets with anti-malware updated within the last 7 days (M3), count of assets with anti-malware installed (M1)",
                "measure.id": "M3, M1",
                "equation": "Update Compliance = M3 / M1"
            },
            {
                "defination": "Percentage of assets that have undergone a malware scan within a recent period.",
                "measure.description": "Count of assets scanned within the last 24 hours (M4), count of assets with anti-malware installed (M1)",
                "measure.id": "M4, M1",
                "equation": "Scan Coverage = M4 / M1"
            },
            {
                "defination": "Rate of malware detections relative to the number of scans performed, indicating effectiveness.",
                "measure.description": "Number of malware detections in the last month (M6), number of scans performed in the last month (M7)",
                "measure.id": "M6, M7",
                "equation": "Detection Rate = M6 / M7"
            }
        ]
    },
    {
        "Observable": "Configuration settings of anti-malware software indicating automatic updates are enabled for signature files, and logs or records of update events showing regular and successful updates.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be verified by inspecting the configuration settings of anti-malware software on each asset to confirm that automatic updates are enabled, without requiring complex data analytics or active probing.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": [
            "Model-based: Evaluation involves examining the configuration models or settings of the anti-malware software to verify that automatic updates are configured correctly.",
            "Data-driven: Evaluation uses data from update logs, event histories, and success rates to analyze the frequency and effectiveness of automatic updates.",
            "Active testing: Evaluation involves probing the system, such as forcing an update check, to verify that automatic updates are functioning as intended."
        ],
        "Metric": [
            {
                "defination": "Percentage of enterprise assets that have automatic updates configured for anti-malware signature files.",
                "measure.description": "Count of assets with automatic updates configured (M1) and total number of enterprise assets (M3).",
                "measure.id": "M1, M3",
                "equation": "(M1 / M3) * 100%"
            },
            {
                "defination": "Percentage of assets with anti-malware signature files updated within the last 24 hours, indicating freshness of signatures.",
                "measure.description": "Count of assets with last signature update within the last 24 hours (M7) and total number of enterprise assets (M3).",
                "measure.id": "M7, M3",
                "equation": "(M7 / M3) * 100%"
            },
            {
                "defination": "Ratio of successful anti-malware signature update attempts to total attempts within a defined period, indicating update reliability.",
                "measure.description": "Count of successful update attempts (M5) and count of failed update attempts (M6) in the defined period.",
                "measure.id": "M5, M6",
                "equation": "M5 / (M5 + M6)"
            }
        ]
    },
    {
        "Observable": "Configuration settings on devices indicating that autorun and autoplay are disabled for removable media, and system logs showing no automatic execution events upon media insertion.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be assessed by verifying the configuration settings of the operating system or device management policies, such as checking registry keys or group policies, which does not require data-driven analytics or active testing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Enforcement is evaluated by comparing the actual configuration state to a predefined model where autorun and autoplay are disabled, using system configuration data without the need for active probing or extensive log analysis.",
        "Metric": [
            {
                "defination": "The compliance rate of devices where autorun and autoplay functionality is disabled for removable media.",
                "measure.description": "Count of devices with autorun and autoplay disabled (M1) and total number of devices that support removable media (M3)",
                "measure.id": "M1, M3",
                "equation": "Compliance Rate = M1 / M3"
            }
        ]
    },
    {
        "Observable": "Configuration settings of anti-malware software indicating automatic scanning for removable media is enabled, and logs of scans performed on removable media.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable: The configuration can be directly inspected in the anti-malware software settings to confirm if automatic scanning is enabled. Measurable: Data from scan logs can be analyzed over time to assess scan frequency and coverage, requiring data-driven analytics.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based: Evaluation involves checking the configuration model of the anti-malware software. Data-driven: Analysis of scan log statistics, such as count of scans and insertion events. Active testing: Probing the system by inserting removable media to observe if it is scanned automatically.",
        "Metric": [
            {
                "defination": "Percentage of anti-malware instances configured to automatically scan removable media.",
                "measure.description": "Count of anti-malware instances with automatic scan enabled for removable media and total number of anti-malware instances.",
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "Compliance Rate = M1 / M3"
            },
            {
                "defination": "Percentage of removable media insertion events that are scanned by anti-malware software.",
                "measure.description": "Number of removable media scans performed and number of removable media insertion events in a specified time period.",
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "Scan Coverage = M4 / M5"
            }
        ]
    },
    {
        "Observable": "Configuration settings and logs indicating the enablement status of anti-exploitation features such as Microsoft Data Execution Prevention (DEP), Windows Defender Exploit Guard (WDEG), Apple System Integrity Protection (SIP), and Gatekeeper on enterprise devices.",
        "Class": [
            {
                "class": "Checklist",
                "explanation": "Can be assessed through automated scripts or tools that check the configuration status of anti-exploitation features on each device, allowing for systematic verification."
            },
            {
                "class": "Verifiable",
                "explanation": "Can be verified by inspecting system settings, configuration files, or management consoles to confirm that anti-exploitation features are enabled as required."
            },
            {
                "class": "Measurable",
                "explanation": "The enforcement quality can be quantified by measuring metrics such as the percentage of devices with features enabled out of those that support them, providing a data-driven assessment."
            }
        ],
        "Evaluation_Method": [
            {
                "method": "Data-driven",
                "explanation": "Evaluation involves analyzing data from device inventories, configuration management databases, or security logs to determine the status and coverage of anti-exploitation features across the enterprise."
            },
            {
                "method": "Model-based",
                "explanation": "Assessment is based on comparing actual system configurations against a predefined model or policy that specifies the required anti-exploitation settings, using configuration management tools."
            }
        ],
        "Metric": [
            {
                "defination": "The percentage of devices that support anti-exploitation features and have them enabled, measuring the effectiveness of enforcement.",
                "measure.description": "Count of devices with anti-exploitation features enabled and count of devices that support such features.",
                "measure.id": "ME_enabled, ME_supported",
                "equation": "ME_enabled / ME_supported"
            }
        ]
    },
    {
        "Observable": "Central management console status, anti-malware configuration logs indicating central server settings, deployment records from the management system, and update logs showing centralized push of definitions and scans.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of anti-malware software on devices and central servers can be checked through system settings and logs to confirm central management. Measurable because metrics such as coverage and compliance rates can be quantified using data from logs and configurations.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based as it involves verifying system configurations against expected models for central management. Data-driven as it requires analyzing event logs, management activities, and update frequencies to derive statistical metrics.",
        "Metric": [
            {
                "defination": "Coverage of devices under central anti-malware management, indicating the proportion of devices that are managed centrally.",
                "measure.description": "Count of devices with anti-malware configured for central management, Count of devices without central management for anti-malware",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Configuration compliance of central management servers, showing the percentage of servers properly configured for central management.",
                "measure.description": "Count of central management servers that are properly configured, Count of total central management servers",
                "measure.id": "M3, M4",
                "equation": "M3 / M4"
            },
            {
                "defination": "Update freshness, representing the average time since the last definition update was pushed centrally, to ensure timely protection.",
                "measure.description": "Time of last definition update for each device (aggregated to average), but typically derived from logs; for simplicity, assume M5 as average time since last update in days",
                "measure.id": "M5",
                "equation": "Average(M5)  // Lower values indicate better freshness, e.g., if M5 is days since update, aim for M5 <= threshold like 7 days"
            }
        ]
    },
    {
        "Observable": "Presence of behavior-based anti-malware software on devices, configuration settings indicating it is enabled and updated, and logs of detection events or missed detections.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the installation, configuration, and enablement of anti-malware software can be checked through system audits and configuration reviews. Measurable because the detection performance, such as rates of detection and false negatives, can be quantified using data from logs and incident reports.",
        "Evaluation_Method": "Data-driven, Active testing",
        "Evaluation_Method.explanation": "Data-driven evaluation involves analyzing historical data from logs, such as detection events and incident reports, to assess performance. Active testing involves deploying controlled test malware samples to verify the software's behavior-based detection capabilities in real-time.",
        "Metric": [
            {
                "defination": "Coverage rate of behavior-based anti-malware installation on devices",
                "measure.description": "M1: Number of devices with behavior-based anti-malware installed and enabled, M2: Number of devices without behavior-based anti-malware or disabled",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Detection rate based on logged events, measuring the proportion of malware incidents detected",
                "measure.description": "M3: Number of behavior-based detection events (true positives), M4: Number of undetected malware incidents (false negatives)",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Configuration compliance rate for anti-malware software updates",
                "measure.description": "M5: Number of devices with anti-malware software updated to the latest version, M6: Number of devices with outdated anti-malware software",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 / (M5 + M6)"
            },
            {
                "defination": "Test detection rate from active testing, measuring the effectiveness against known malware samples",
                "measure.description": "M7: Number of test malware samples detected during active testing, M8: Number of test malware samples not detected during active testing",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "M7 / (M7 + M8)"
            }
        ]
    },
    {
        "Observable": "The documented data recovery process, including the document itself, records of review and update activities, audit logs, and evidence of addressing scope, recovery prioritization, and backup data security.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the documentation can be inspected for presence and content through manual or automated checks. Measurable because aspects like review frequency, update timeliness, and completeness can be quantified using data.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because statistics from audit logs, timestamps of reviews and updates, and change records can be analyzed. Model-based because the documentation serves as a configuration model for the recovery process, and its structure can be assessed.",
        "Metric": [
            {
                "defination": "Measures how completely the documentation addresses the required elements: scope, recovery prioritization, and backup data security.",
                "measure.description": "Score indicating the presence of required sections in the document, where each section (scope, prioritization, security) contributes 1 if present, 0 if not.",
                "measure.id": "M3",
                "equation": "M3 / 3"
            },
            {
                "defination": "Assesses whether the documentation was reviewed within the last 365 days, as required annually.",
                "measure.description": "Timestamp of the last review and the current timestamp for calculating the time difference.",
                "measure.id": "M1 and M4",
                "equation": "1 if (M4 - M1) <= 31536000000 milliseconds (equivalent to 365 days), else 0"
            },
            {
                "defination": "Evaluates the ratio of significant changes that triggered updates to the documentation, ensuring updates occur when changes happen.",
                "measure.description": "Count of significant enterprise changes detected and count of times the documentation was updated after such changes.",
                "measure.id": "M5 and M6",
                "equation": "M6 / M5 if M5 > 0, else 1"
            }
        ]
    },
    {
        "Observable": "Backup execution logs, configuration settings for automated backups, timestamps of backup events, success/failure statuses of backup attempts, and records of backup schedules based on data sensitivity.",
        "Class": [
            {
                "name": "Verifiable",
                "explanation": "The safeguard can be verified by inspecting the configuration of backup systems to ensure that automated backups are enabled and schedules are set according to data sensitivity."
            },
            {
                "name": "Measurable",
                "explanation": "The enforcement quality can be measured by analyzing quantitative data from backup logs, such as frequency, timeliness, and success rates of backups."
            }
        ],
        "Evaluation_Method": [
            {
                "name": "Data-driven",
                "explanation": "Evaluation requires analyzing data from backup logs, including event timestamps and success indicators, to compute metrics like frequency compliance and success rate."
            },
            {
                "name": "Model-based",
                "explanation": "Evaluation involves checking the system configuration models, such as backup software settings, to verify that automation is configured and schedules align with data sensitivity policies."
            }
        ],
        "Metric": [
            {
                "defination": "The proportion of in-scope assets that have automated backups configured.",
                "measure": {
                    "description": "Count of assets with automated backups enabled (M2) and total in-scope assets (M1)",
                    "id": "M1, M2"
                },
                "equation": "M2 / M1"
            },
            {
                "defination": "The percentage of assets where backups are run at least weekly (time since last backup <= 7 days) or more frequently as per sensitivity.",
                "measure": {
                    "description": "Count of assets with time since last backup <= 7 days (M4) and total in-scope assets (M1)",
                    "id": "M1, M4"
                },
                "equation": "M4 / M1"
            },
            {
                "defination": "The proportion of backup attempts that are successful over a specified period.",
                "measure": {
                    "description": "Number of successful backup attempts (M5) and total backup attempts (M6) in the last week",
                    "id": "M5, M6"
                },
                "equation": "M5 / M6"
            }
        ]
    },
    {
        "Observable": "Configuration settings for encryption and data separation on recovery data, access control logs, audit trails of data protection measures, and records of equivalent controls compared to original data.",
        "Class": "Verifiable, Measurable, Checklist",
        "Class.explanation": "Verifiable because the configurations for encryption and data separation can be directly inspected and verified against requirements; Measurable because compliance rates and effectiveness can be quantified over time using data; Checklist because automated scripts can assess the settings and configurations.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based: Evaluation relies on checking system configurations and models to ensure equivalent controls are in place; Data-driven: Analysis of access logs, encryption status, and incident reports provides statistical insights into protection effectiveness.",
        "Metric": [
            {
                "definition": "Encryption Compliance Rate - The proportion of recovery data sets that have encryption enabled equivalent to the original data controls.",
                "measure.description": "Count of recovery data sets with proper encryption (M1) and count without proper encryption (M2)",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "definition": "Data Separation Compliance Rate - The proportion of recovery data sets that have data separation implemented as required.",
                "measure.description": "Count of recovery data sets with data separation (M3) and count without data separation (M4)",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / (M3 + M4)"
            },
            {
                "definition": "Security Incident Rate - The rate of unauthorized access attempts to recovery data, indicating effectiveness of protection controls.",
                "measure.description": "Number of unauthorized access attempts to recovery data (M5) and total number of access attempts or time period, but M5 alone may suffice for rate calculation over time",
                "measure.id": [
                    "M5"
                ],
                "equation": "M5 / (time period or total access attempts if available, else use M5 as a count over time)"
            }
        ]
    },
    {
        "Observable": "Configuration settings of backup systems indicating isolation (e.g., offline, off-site, cloud with isolation), logs of backup operations showing version control, records of backup instances and their properties.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the isolation and version control settings can be checked against configuration standards and system models. Measurable because the proportion of compliant backup instances and the extent of version control can be quantified using data from logs and configurations.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based because system configurations and models can be used to verify isolation and version control settings. Data-driven because backup logs, version histories, and instance data provide statistical information for analysis and measurement.",
        "Metric": [
            {
                "defination": "Proportion of backup instances that are isolated from the primary network or system to ensure recovery data integrity.",
                "measure.description": "M1: Count of backup instances configured as isolated (e.g., offline, off-site), M2: Count of backup instances not isolated",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "Isolation_Coverage = M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of backup instances with version control enabled to manage and track changes in recovery data.",
                "measure.description": "M3: Count of backup instances with version control enabled, M4: Count of backup instances without version control enabled",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "Version_Control_Compliance = M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Records of backup recovery tests, including test logs, timestamps, lists of assets tested, and reports indicating the frequency and coverage of tests.",
        "Class": "Measurable",
        "Class.explanation": "Assessing this safeguard requires analyzing data from backup recovery test logs and reports to measure the frequency of tests and the coverage of assets, which involves data-driven analytics rather than simple checklist or configuration verification.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "We evaluate the enforcement by generating statistics from event logs of backup recovery tests, such as counting the number of tests conducted, the assets involved, and the timestamps, to assess compliance with quarterly testing and sampling requirements.",
        "Metric": [
            {
                "defination": "Indicates whether backup recovery tests are conducted at least quarterly.",
                "measure.description": "Time in days since the last backup recovery test was conducted.",
                "measure.id": "M4",
                "equation": "1 if M4 <= 90 else 0"
            },
            {
                "defination": "The percentage of in-scope assets that have been tested in backup recovery tests in the last quarter, ensuring a sampling is covered.",
                "measure.description": "Count of unique assets tested in the last 90 days and total number of in-scope assets.",
                "measure.id": "M3, M2",
                "equation": "M3 / M2"
            }
        ]
    },
    {
        "Observable": "Software versions of network infrastructure, support status of NaaS offerings, and timestamps of software version reviews.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because software versions and support status can be checked through configuration inspection and vendor documentation. Measurable because the compliance rates (e.g., proportion of up-to-date devices) and review frequencies can be quantified and tracked over time.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation involves collecting and analyzing data from version histories, support status logs, and review records. Model-based evaluation compares current software versions against a model of supported versions (e.g., vendor lists). Active testing may involve probing network devices with scanning tools to check for vulnerabilities or update status.",
        "Metric": [
            {
                "definition": "Proportion of network devices running the latest stable software version.",
                "measure.description": "Count of devices with latest software version and total number of devices.",
                "measure.id": [
                    "M1",
                    "M5"
                ],
                "equation": "M1 / M5"
            },
            {
                "definition": "Proportion of network devices using currently supported NaaS offerings.",
                "measure.description": "Count of devices using supported NaaS and total number of devices.",
                "measure.id": [
                    "M3",
                    "M5"
                ],
                "equation": "M3 / M5"
            },
            {
                "definition": "Time interval between the last two software version reviews, in days.",
                "measure.description": "Timestamps of the last and previous software version reviews.",
                "measure.id": [
                    "M6",
                    "M7"
                ],
                "equation": "M6 - M7"
            }
        ]
    },
    {
        "Observable": "Network architecture diagrams, configuration files for network devices (e.g., routers, firewalls), traffic flow logs, access control policies, design documentation, and records of maintenance activities.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because network configurations and documentation can be inspected against design standards and policies to check for compliance. Measurable because data from traffic logs, performance monitors, and access controls can be quantified to assess segmentation, least privilege, and availability enforcement.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves using network configuration models and design documents to verify architectural adherence to principles. Data-driven evaluation utilizes collected traffic statistics, log data, and performance metrics to measure actual enforcement and effectiveness over time.",
        "Metric": [
            {
                "defination": "Measures the effectiveness of network segmentation by calculating the proportion of authorized cross-segment traffic flows.",
                "measure.description": "Total count of cross-segment traffic flows and count of unauthorized cross-segment traffic flows based on policy violations",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "1 - (M4 / M3)"
            },
            {
                "defination": "Measures compliance with least privilege principles by determining the ratio of access control rules that do not violate least privilege.",
                "measure.description": "Total count of access control rules and count of rules that are overly permissive or violate least privilege",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "1 - (M6 / M5)"
            },
            {
                "defination": "Measures the availability of critical network services by calculating the uptime percentage over a monitoring period.",
                "measure.description": "Uptime duration of critical network services and total monitoring time period",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "M7 / M8"
            },
            {
                "defination": "Measures the completeness of network documentation by assessing the proportion of network components with up-to-date records.",
                "measure.description": "Count of network components with current documentation and total count of network components",
                "measure.id": [
                    "M9",
                    "M10"
                ],
                "equation": "M9 / M10"
            }
        ]
    },
    {
        "Observable": "Version-controlled Infrastructure-as-Code (IaC) repositories, network configuration files, traffic logs showing protocol usage (e.g., SSH, HTTPS, insecure protocols), and management activity logs.",
        "Class": "Checklist, Verifiable, Measurable",
        "Class.explanation": "Checklist: Automated scripts can verify if IaC is version-controlled. Verifiable: Network configurations can be inspected to ensure secure protocols like SSH and HTTPS are enabled. Measurable: Traffic data can be analyzed quantitatively to measure secure protocol usage.",
        "Evaluation_Method": "Data-driven, Model-based, Active testing",
        "Evaluation_Method.explanation": "Data-driven: Statistical analysis of traffic logs and event data to measure protocol usage. Model-based: Assessment of configuration models from IaC or device settings to verify compliance. Active testing: Probing network services to test if they respond only with secure protocols.",
        "Metric": [
            {
                "defination": "Proportion of network infrastructure managed with version-controlled Infrastructure-as-Code (IaC).",
                "measure.description": "M1: Count of network infrastructure components managed with version-controlled IaC. M2: Count of network infrastructure components not managed with version-controlled IaC.",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Percentage of network services configured to use secure protocols (SSH or HTTPS).",
                "measure.description": "M3: Count of network services configured to use SSH. M4: Count of network services configured to use HTTPS. M5: Count of network services using insecure protocols (e.g., Telnet, HTTP).",
                "measure.id": "M3, M4, M5",
                "equation": "(M3 + M4) / (M3 + M4 + M5)"
            },
            {
                "defination": "Ratio of network traffic volume using secure protocols to total traffic volume.",
                "measure.description": "M6: Volume of network traffic using SSH or HTTPS (in bytes or packets). M7: Volume of network traffic using insecure protocols (in bytes or packets).",
                "measure.id": "M6, M7",
                "equation": "M6 / (M6 + M7)"
            }
        ]
    },
    {
        "Observable": "Architecture diagrams, documentation files, review logs with timestamps, update records, and logs of significant enterprise changes.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard is verifiable because it can be assessed by directly inspecting the documentation files and their metadata, such as creation dates, modification dates, and review history, to ensure they exist and are maintained according to the policy of annual reviews or updates after significant changes.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is model-based as it relies on examining the current state and configuration of the documentation system, including file properties, audit logs, and metadata, without requiring active testing or extensive data-driven statistical analysis.",
        "Metric": {
            "defination": "The proportion of architecture diagrams that have been reviewed within the last 365 days or updated after the last significant enterprise change, if applicable.",
            "measure.description": "M1: Total number of architecture diagrams, M2: Number of diagrams with last review date within the past 365 days, M3: Date of the last significant enterprise change (as a Unix timestamp), M4: Number of diagrams updated after the date in M3",
            "measure.id": "M1, M2, M3, M4",
            "equation": "If (current_time - M3) <= 31536000 seconds (365 days), then (M2 + M4 - overlap) / M1, else M2 / M1. Note: Overlap accounts for diagrams counted in both M2 and M4, but ideally, M4 should only include diagrams not in M2 if M3 is recent."
        }
    },
    {
        "Observable": "Centralized AAA server configuration, network device AAA settings pointing to a central server, logs of authentication, authorization, and accounting events from the central system.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be assessed by verifying the configuration of network devices to ensure they are set to use a central AAA service, which is typically done through manual or automated checks of device settings and server configurations.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is performed by examining the configuration models of network devices and AAA servers to confirm that AAA services are centralized, using static configuration data rather than dynamic logs or active tests.",
        "Metric": [
            {
                "definition": "The proportion of network devices that are configured to use a central AAA service.",
                "measure.description": "Count of network devices configured with central AAA and total number of network devices.",
                "measure.id": "M1, M5",
                "equation": "M1 / M5"
            },
            {
                "definition": "The proportion of authentication requests that are handled by the central AAA service.",
                "measure.description": "Count of authentication requests processed by central AAA and total authentication requests (including those not handled centrally).",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Network device configurations showing enabled 802.1X and WPA2 Enterprise protocols, and network traffic encrypted using these protocols.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "This safeguard is verifiable because it involves checking configuration settings on network devices through scripts or manual inspection. It is measurable because usage statistics and compliance can be derived from network logs, traffic analysis, and authentication events.",
        "Evaluation_Method": "Model-based, Active testing",
        "Evaluation_Method.explanation": "Model-based evaluation is used to inspect device configurations and settings for protocol adoption. Active testing is used to probe the network by attempting connections with insecure protocols to verify enforcement and functionality.",
        "Metric": [
            {
                "defination": "Percentage of network devices using 802.1X for secure network access control",
                "measure": [
                    {
                        "description": "Count of network devices with 802.1X enabled",
                        "id": "M1"
                    },
                    {
                        "description": "Count of network devices without 802.1X enabled",
                        "id": "M2"
                    }
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Percentage of wireless access points using WPA2 Enterprise or more secure protocols",
                "measure": [
                    {
                        "description": "Count of wireless access points with WPA2 Enterprise or better enabled",
                        "id": "M3"
                    },
                    {
                        "description": "Count of wireless access points without secure protocols enabled",
                        "id": "M4"
                    }
                ],
                "equation": "M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Authentication logs from VPN and authentication services, access logs to enterprise resources with timestamps indicating authentication prior to access, and configuration settings of VPN and authentication services.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the configuration of VPN and authentication services can be inspected to ensure authentication is required before access. Measurable because authentication and access logs can be analyzed quantitatively to assess compliance rates.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves checking the configuration settings of VPN and authentication services to verify they enforce authentication. Data-driven evaluation involves analyzing authentication and access logs to measure compliance through statistical data.",
        "Metric": [
            {
                "definition": "Access Authentication Compliance Rate: The proportion of access attempts to enterprise resources that are preceded by successful authentication.",
                "measure.description": "M4: Count of access attempts with prior successful authentication, M3: Count of total access attempts to enterprise resources",
                "measure.id": [
                    "M4",
                    "M3"
                ],
                "equation": "M4 / M3"
            },
            {
                "definition": "Device Configuration Compliance Rate: The percentage of end-user devices that are properly configured to use VPN and authentication services.",
                "measure.description": "M5: Count of end-user devices configured to use VPN/auth services, M6: Count of end-user devices not configured or misconfigured",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 / (M5 + M6)"
            },
            {
                "definition": "Service Configuration Compliance Rate: The percentage of VPN and authentication service instances that are properly configured to require authentication.",
                "measure.description": "M7: Count of VPN/auth service instances properly configured to require authentication, M8: Count of VPN/auth service instances not properly configured",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "M7 / (M7 + M8)"
            }
        ]
    },
    {
        "Observable": "Dedicated computing resources for administrative tasks, network segmentation configurations indicating isolation from the enterprise's primary network, and settings showing no internet access on these resources. If not enforced, administrative tasks may be performed on non-dedicated resources, lack of network segmentation, or internet access on admin resources.",
        "Class": [
            {
                "name": "Checklist",
                "explanation": "Automated scripts can check for the existence and basic configuration of dedicated administrative resources, such as verifying dedicated servers or VMs and their network settings."
            },
            {
                "name": "Verifiable",
                "explanation": "System and network configurations, including firewall rules and access control lists, can be reviewed to confirm segmentation and the absence of internet access on dedicated resources."
            },
            {
                "name": "Measurable",
                "explanation": "Data from event logs and task execution records can be analyzed to measure the proportion of administrative tasks performed on dedicated resources and compliance with segmentation and access policies."
            }
        ],
        "Evaluation_Method": [
            {
                "name": "Data-driven",
                "explanation": "Analysis of event logs, network traffic flows, and administrative task records can show if tasks are confined to dedicated resources and if internet access is blocked, providing statistical insights into enforcement."
            },
            {
                "name": "Model-based",
                "explanation": "Configuration models and network diagrams can be used to verify that dedicated resources are segmented from the primary network and have no internet access, based on system settings and rules."
            },
            {
                "name": "Active testing",
                "explanation": "Probing the network through penetration tests or access attempts can validate isolation and internet blockages on dedicated administrative resources by simulating real-world scenarios."
            }
        ],
        "Metric": [
            {
                "defination": "The percentage of administrative tasks performed on dedicated computing resources, assessing task confinement.",
                "measure": {
                    "description": "M4: Number of administrative tasks performed on dedicated resources; M6: Total number of administrative tasks (sum of tasks on dedicated and non-dedicated resources).",
                    "id": "M4, M6"
                },
                "equation": "M4 / M6"
            },
            {
                "defination": "The percentage of dedicated administrative resources that are properly segmented from the primary network, evaluating network isolation.",
                "measure": {
                    "description": "M1: Number of dedicated administrative computing resources; M2: Number of dedicated administrative computing resources that are segmented from the primary network.",
                    "id": "M1, M2"
                },
                "equation": "M2 / M1"
            },
            {
                "defination": "The percentage of dedicated administrative resources with no internet access, measuring access control enforcement.",
                "measure": {
                    "description": "M1: Number of dedicated administrative computing resources; M3: Number of dedicated administrative computing resources with no internet access.",
                    "id": "M1, M3"
                },
                "equation": "M3 / M1"
            }
        ]
    },
    {
        "Observable": "Centralized security event alerting system (e.g., SIEM or log analytics platform), configuration settings for log ingestion and correlation alerts, logs of events and alerts generated, records of alert correlations and analyses.",
        "Class": "Checklist, Verifiable, Measurable",
        "Class.explanation": "Checklist: Automated scripts can check if assets are configured to send logs and if correlation alerts are set up. Verifiable: Manual inspection can confirm the configuration of the SIEM and alert rules. Measurable: Data analysis is required to assess performance metrics like coverage and alert effectiveness.",
        "Evaluation_Method": "Data-driven, Model-based, Active testing",
        "Evaluation_Method.explanation": "Data-driven: Analysis of log data, alert statistics, and event correlations to evaluate enforcement. Model-based: Inspection of configuration settings in the SIEM or log analytics platform. Active testing: Probing the system by generating test events to verify if alerts trigger appropriately.",
        "Metric": [
            {
                "defination": "Percentage of enterprise assets that are configured to send logs to the central alerting system.",
                "measure.description": "Count of assets configured to send logs (M1) and total number of enterprise assets (M3).",
                "measure.id": "M1, M3",
                "equation": "M1 / M3"
            },
            {
                "defination": "Number of vendor-defined or security-relevant correlation alerts configured in the system.",
                "measure.description": "Count of correlation alerts configured (M4).",
                "measure.id": "M4",
                "equation": "M4"
            },
            {
                "defination": "Ratio of true positive alerts to total alerts generated, indicating alert effectiveness.",
                "measure.description": "Count of true positive alerts (M5) and total alerts generated (M7, where M7 = M5 + M6, with M6 being count of false positive alerts).",
                "measure.id": "M5, M7",
                "equation": "M5 / M7"
            },
            {
                "defination": "Average time taken from the occurrence of a security event to the generation of an alert.",
                "measure.description": "Average detection time calculated from individual event-to-alert time measurements (M8).",
                "measure.id": "M8",
                "equation": "M8"
            }
        ]
    },
    {
        "Observable": "Presence of host-based intrusion detection software on enterprise assets, its configuration settings, and logs of detection events.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the deployment and configuration of HIDS can be checked through system audits and configuration reviews against policies. Measurable because the coverage and potential detection performance can be quantified using data from asset inventories and logs.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based for evaluating HIDS configuration against security models or policies. Data-driven for generating statistics on deployment coverage and detection events from system logs.",
        "Metric": [
            {
                "defination": "The percentage of appropriate enterprise assets that have a host-based intrusion detection solution deployed.",
                "measure": [
                    {
                        "description": "Count of enterprise assets that are appropriate and support HIDS deployment",
                        "id": "M1"
                    },
                    {
                        "description": "Count of assets with HIDS deployed",
                        "id": "M2"
                    }
                ],
                "equation": "M2 / M1"
            },
            {
                "defination": "The percentage of deployed HIDS that are configured according to security policy.",
                "measure": [
                    {
                        "description": "Count of assets with HIDS deployed",
                        "id": "M2"
                    },
                    {
                        "description": "Count of assets with HIDS properly configured",
                        "id": "M3"
                    }
                ],
                "equation": "M3 / M2"
            }
        ]
    },
    {
        "Observable": "Deployment of network intrusion detection solutions (e.g., NIDS or CSP services), their configuration settings, alert logs generated, and coverage of network segments.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Checklist: Deployment can be assessed through scripting to check if NIDS is installed on assets. Verifiable: Configuration settings can be verified against best practices or standards. Measurable: Detection performance, such as alert rates and accuracy, can be measured using data analytics from logs.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven: Evaluation involves analyzing logs and alert data to compute metrics like detection rates. Model-based: Configuration files or settings are checked against a security model or policy. Active testing: The system is probed with simulated intrusions to test if NIDS detects them.",
        "Metric": [
            {
                "defination": "Proportion of enterprise assets with NIDS deployed, regardless of configuration.",
                "measure.description": "M1: Count of assets with NIDS deployed and configured correctly, M2: Count of assets with NIDS deployed but misconfigured, M4: Total number of enterprise assets",
                "measure.id": [
                    "M1",
                    "M2",
                    "M4"
                ],
                "equation": "(M1 + M2) / M4"
            },
            {
                "defination": "Proportion of deployed NIDS that are properly configured.",
                "measure.description": "M1: Count of assets with NIDS deployed and configured correctly, M2: Count of assets with NIDS deployed but misconfigured",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of network segments monitored by NIDS.",
                "measure.description": "M5: Number of network segments covered by NIDS, M6: Total number of network segments",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 / M6"
            },
            {
                "defination": "Rate of intrusion alerts generated per unit time.",
                "measure.description": "M7: Number of intrusion alerts generated in a time period, M10: Time period for measurement (e.g., in days)",
                "measure.id": [
                    "M7",
                    "M10"
                ],
                "equation": "M7 / M10"
            },
            {
                "defination": "Proportion of alerts that are true positives.",
                "measure.description": "M7: Number of intrusion alerts generated, M8: Number of true positive alerts",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "M8 / M7"
            },
            {
                "defination": "Proportion of alerts that are false positives.",
                "measure.description": "M7: Number of intrusion alerts generated, M9: Number of false positive alerts",
                "measure.id": [
                    "M7",
                    "M9"
                ],
                "equation": "M9 / M7"
            }
        ]
    },
    {
        "Observable": "Configuration settings of network devices (e.g., firewalls, routers) showing traffic filtering rules, network traffic logs indicating filtered and allowed traffic, and network segmentation diagrams.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the presence and configuration of traffic filtering can be checked by inspecting network device settings against policies. Measurable because the coverage and effectiveness can be assessed through data analysis of traffic flows and configuration compliance.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based because network configuration models can be used to verify the setup of filtering rules. Data-driven because analysis of traffic data from logs and flows is required to evaluate filtering performance and compliance.",
        "Metric": [
            {
                "defination": "The proportion of network segments that have traffic filtering enabled.",
                "measure.description": "M1: Number of network segments with traffic filtering configured. M2: Number of network segments without traffic filtering configured.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "Coverage = M1 / (M1 + M2)"
            },
            {
                "defination": "The proportion of filtering rules that are appropriate based on security policy.",
                "measure.description": "M3: Number of filtering rules that are compliant with security policy (e.g., correctly configured for allowed/denied traffic). M4: Number of filtering rules that are non-compliant or inappropriate.",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "Appropriateness = M3 / (M3 + M4)"
            }
        ]
    },
    {
        "Observable": "Logs of access control decisions for remote connections, status reports indicating anti-malware version and update status, configuration compliance checks, and operating system and application update status for assets remotely connecting to enterprise resources.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the configuration of access control systems and the status of assets (e.g., anti-malware, OS updates) can be checked through audits or automated scripts. Measurable because compliance rates (e.g., percentage of assets meeting criteria) can be calculated from collected data, allowing for quantitative assessment.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven because evaluation involves analyzing data from logs, status reports, and event records to compute metrics. Model-based because it requires comparing actual system configurations and states against a predefined secure configuration model to assess compliance.",
        "Metric": [
            {
                "defination": "Percentage of remote assets with up-to-date anti-malware software installed",
                "measure.description": "Count of remote assets with anti-malware software that is current and up-to-date",
                "measure.id": "M1",
                "equation": "M1 / M7"
            },
            {
                "defination": "Percentage of remote assets compliant with the enterprise's secure configuration process",
                "measure.description": "Count of remote assets that meet the secure configuration standards",
                "measure.id": "M3",
                "equation": "M3 / M7"
            },
            {
                "defination": "Percentage of remote assets with operating system and applications up-to-date",
                "measure.description": "Count of remote assets where the OS and all applications are current with the latest updates",
                "measure.id": "M5",
                "equation": "M5 / M7"
            }
        ]
    },
    {
        "Observable": "Network traffic flow logs being collected and alerts generated based on reviews from network devices.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the configuration of network devices can be inspected to verify if flow logging is enabled. Measurable because quantifiable metrics such as log volume and alert rate can be derived from data analysis.",
        "Evaluation_Method": [
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Data-driven because evaluating this safeguard requires collecting and analyzing data from flow logs, traffic flows, and alert logs to assess enforcement, rather than relying solely on configuration checks or active probing.",
        "Metric": [
            {
                "definition": "Proportion of network devices configured to collect flow logs",
                "measure.description": "Count of network devices with flow logging enabled, Total number of network devices",
                "measure.id": "M1, M3",
                "equation": "M1 / M3"
            },
            {
                "definition": "Rate of flow log collection",
                "measure.description": "Volume of flow logs collected per time unit (e.g., logs per day)",
                "measure.id": "M4",
                "equation": "M4 / time_interval"
            },
            {
                "definition": "Rate of alert generation",
                "measure.description": "Number of alerts generated per time unit from log reviews",
                "measure.id": "M5",
                "equation": "M5 / time_interval"
            },
            {
                "definition": "Average alert latency",
                "measure.description": "Time delay between log collection and alert generation",
                "measure.id": "M6",
                "equation": "Average(M6)"
            }
        ]
    },
    {
        "Observable": "Presence of host-based IPS or EDR agents on enterprise assets, their configuration settings (e.g., enabled status, signature updates), and deployment or installation logs.",
        "Class": [
            "Verifiable",
            "Checklist"
        ],
        "Class.explanation": "Verifiable because the deployment and configuration of IPS can be confirmed by examining system settings and agent status. Checklist because a script can be used to systematically check for installation on each asset, creating a verifiable list.",
        "Evaluation_Method": [
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves using configuration data from systems, such as management interfaces or agent status reports, to assess compliance with deployment and configuration policies without active probing.",
        "Metric": [
            {
                "defination": "The percentage of enterprise assets that have a host-based IPS deployed.",
                "measure.description": "M1: Count of assets with IPS deployed, M2: Total count of enterprise assets",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / M2"
            },
            {
                "defination": "The percentage of deployed IPS that are properly configured (e.g., enabled, with updated signatures).",
                "measure.description": "M3: Count of assets with properly configured IPS, M1: Count of assets with IPS deployed",
                "measure.id": [
                    "M3",
                    "M1"
                ],
                "equation": "M3 / M1"
            }
        ]
    },
    {
        "Observable": "Network intrusion prevention system configuration, logs of intrusion alerts, deployment records, and traffic monitoring data",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the deployment and configuration of NIPS can be checked through system inspections and configuration reviews. Measurable because its effectiveness can be quantified through data analysis of intrusion detection, prevention rates, and alert logs.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation involves analyzing event logs, traffic flows, and alert data to assess intrusion patterns. Model-based evaluation checks the configuration settings and rules of the NIPS against best practices. Active testing involves probing the network with simulated attacks to test the NIPS's response and detection capabilities.",
        "Metric": [
            {
                "defination": "Proportion of network segments covered by NIPS deployment",
                "measure": {
                    "description": "Count of network segments with NIPS deployed and count without NIPS deployed",
                    "id": "M1, M2"
                },
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Proportion of NIPS instances that are properly configured",
                "measure": {
                    "description": "Count of NIPS instances with proper configuration (e.g., updated rules, enabled) and count with improper configuration",
                    "id": "M3, M4"
                },
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Detection rate from active testing, measuring the percentage of simulated attacks detected by NIPS",
                "measure": {
                    "description": "Number of simulated attacks detected by NIPS and total number of simulated attacks conducted",
                    "id": "M9, M10"
                },
                "equation": "M9 / M10"
            },
            {
                "defination": "False positive rate of NIPS alerts, indicating the proportion of alerts that are incorrect",
                "measure": {
                    "description": "Number of false positive alerts generated by NIPS and total number of alerts generated",
                    "id": "M7, M5"
                },
                "equation": "M7 / M5"
            }
        ]
    },
    {
        "Observable": "Network device configurations showing 802.1x or similar NAC settings, authentication logs indicating successful or failed access attempts, and access control lists or event logs.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Checklist: Can be assessed through automated scripts that check device configurations for 802.1x settings. Verifiable: Configuration settings on network devices can be manually or automatically verified against standards. Measurable: Data from authentication logs and port configurations can be analyzed to measure coverage and success rates.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based: Evaluation involves using configuration models or snapshots of network devices to check for 802.1x implementation. Data-driven: Analysis of authentication event logs, traffic flows, and user/device activities to derive statistics. Active testing: Probing the network by attempting unauthorized access to verify if port-level controls are enforced.",
        "Metric": [
            {
                "defination": "Percentage of network ports configured with 802.1x or similar network access control protocols.",
                "measure.description": "Count of configured ports and total ports available.",
                "measure.id": "M1, M3",
                "equation": "M1 / M3"
            },
            {
                "defination": "Rate of successful authentication attempts for network access.",
                "measure.description": "Count of successful authentication events and total authentication attempts in a specified period.",
                "measure.id": "M4, M6",
                "equation": "M4 / M6"
            },
            {
                "defination": "Compliance score based on the proportion of ports with access control enabled versus not enabled.",
                "measure.description": "Count of configured ports and not configured ports.",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Frequency of configuration audits or changes to ensure access control settings are up-to-date.",
                "measure.description": "Time of last configuration audit or change.",
                "measure.id": "M7",
                "equation": "1 / (current_time - M7)  // Inverse to indicate freshness, higher values mean more recent audits"
            }
        ]
    },
    {
        "Observable": "Logs from application layer filtering devices (e.g., proxy, firewall, gateway) showing filtered traffic, including allowed and blocked requests, and configuration settings of these devices.",
        "Class": "Checklist, Verifiable, Measurable",
        "Class.explanation": "Checklist: The presence and basic configuration of filtering devices can be checked via scripting or automated tools. Verifiable: The configuration settings, such as rule sets and enabled features, can be examined for correctness. Measurable: Data from traffic logs can be analyzed quantitatively to assess coverage and effectiveness.",
        "Evaluation_Method": "Data-driven, Model-based, Active testing",
        "Evaluation_Method.explanation": "Data-driven: Statistics from event logs and traffic flows can be used to compute metrics like coverage and block rates. Model-based: The configuration models of filtering devices can be inspected to verify enforcement. Active testing: Probing the system with test requests (e.g., malicious or benign) can validate filtering behavior.",
        "Metric": [
            {
                "definition": "The percentage of network traffic that is processed by the application layer filter, indicating how much traffic is subject to filtering.",
                "measure.description": "Volume of traffic processed by the filter (in bytes or packets) and total volume of network traffic.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / M2"
            },
            {
                "definition": "The percentage of filtering devices that are properly configured with appropriate rule sets and settings.",
                "measure.description": "Count of properly configured filtering devices and total count of filtering devices.",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / M4"
            },
            {
                "definition": "The accuracy of the filter in correctly handling requests, based on allowed benign and blocked malicious traffic.",
                "measure.description": "Number of correctly handled requests (benign allowed and malicious blocked) and total number of requests handled.",
                "measure.id": [
                    "M5",
                    "M6",
                    "M7",
                    "M8"
                ],
                "equation": "(M5 + M6) / (M5 + M6 + M7 + M8)"
            }
        ]
    },
    {
        "Observable": "Logs of tuning activities for alerting thresholds on network security devices (e.g., firewalls, IDS/IPS), including timestamps of when tuning was performed, configuration changes, and records of threshold adjustments.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the occurrence of tuning activities can be confirmed by inspecting system logs, configuration management systems, and audit trails. Measurable because the frequency of tuning events can be quantified using timestamps from logs to calculate intervals and compliance rates.",
        "Evaluation_Method": [
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Data-driven because it requires collecting and analyzing historical data from security event logs, configuration change logs, or audit trails to determine the timing and frequency of tuning activities, rather than relying solely on static configurations or active probing.",
        "Metric": {
            "defination": "Compliance Rate for Tuning Frequency, representing the proportion of network security devices that have had their alerting thresholds tuned within the last 30 days, ensuring adherence to the monthly or more frequent requirement.",
            "measure.description": "Count of network security devices where the time since the last tuning event is 30 days or less, and the total number of network security devices.",
            "measure.id": "M1, M3",
            "equation": "M1 / M3"
        }
    },
    {
        "Observable": "Records of security awareness training completion (e.g., logs from training systems), documentation of the security awareness program (e.g., policy documents, training materials), logs of content reviews and updates (e.g., revision dates, change logs), and records of significant enterprise changes that might trigger updates.",
        "Class": "Measurable",
        "Class.explanation": "This safeguard involves quantitative assessment of training completion rates and review frequencies, which requires data-driven analysis rather than simple checklist verification or configuration checks.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Evaluation requires collecting and analyzing data from training completion records, employee databases, and review logs to generate statistics on compliance with training schedules and content update policies.",
        "Metric": [
            {
                "defination": "Percentage of employees who have completed security awareness training at hire within a specified period (e.g., 30 days).",
                "measure.description": "Count of employees trained at hire, Total number of employees",
                "measure.id": "M1, M5",
                "equation": "M1 / M5"
            },
            {
                "defination": "Percentage of employees who have completed annual security awareness training in the past 365 days.",
                "measure.description": "Count of employees with annual training completed in the last year, Total number of employees",
                "measure.id": "M3, M5",
                "equation": "M3 / M5"
            },
            {
                "defination": "Indicates whether the content review was conducted within the past 365 days, as required for annual compliance.",
                "measure.description": "Days since last content review",
                "measure.id": "M8",
                "equation": "if M8 <= 365 then 1 else 0"
            }
        ]
    },
    {
        "Observable": "Training completion records, test scores from recognition assessments, incident reports related to social engineering, and results from simulated attacks (e.g., phishing simulations).",
        "Class": "Measurable",
        "Class.explanation": "This safeguard involves quantifiable aspects such as training participation rates, test performance scores, and incident reduction metrics, which can be measured using data from training systems, assessments, and security logs.",
        "Evaluation_Method": "Data-driven, Active testing",
        "Evaluation_Method.explanation": "Data-driven evaluation uses statistics from training completion logs, test scores, and incident reports to assess enforcement. Active testing involves conducting simulated social engineering attacks (e.g., phishing campaigns) to test and measure recognition skills directly.",
        "Metric": [
            {
                "definition": "Training Completion Rate",
                "measure": {
                    "description": "Count of workforce members who have completed the social engineering recognition training (M1) and total number of workforce members (M3)",
                    "id": "M1, M3"
                },
                "equation": "M1 / M3"
            },
            {
                "definition": "Average Test Score",
                "measure": {
                    "description": "Sum of scores from social engineering recognition tests (M4) and count of test takers (M5)",
                    "id": "M4, M5"
                },
                "equation": "M4 / M5"
            },
            {
                "definition": "Incident Reduction Rate",
                "measure": {
                    "description": "Number of reported social engineering incidents in a defined period before training implementation (M6) and number of reported incidents in a similar period after training (M7)",
                    "id": "M6, M7"
                },
                "equation": "(M6 - M7) / M6 * 100"
            },
            {
                "definition": "Simulated Attack Recognition Rate",
                "measure": {
                    "description": "Number of simulated social engineering attacks (e.g., phishing emails) that were correctly reported or identified by workforce members (M9) and total number of simulated attacks sent (M8)",
                    "id": "M8, M9"
                },
                "equation": "M9 / M8"
            }
        ]
    },
    {
        "Observable": "Records of training completion, assessment scores, and logs from learning management systems indicating that workforce members have been trained on authentication best practices, such as MFA, password composition, and credential management.",
        "Class": "Measurable",
        "Class.explanation": "Assessing this safeguard requires data-driven analysis of training completion rates and assessment scores, which involves measuring and analyzing data rather than simply checking configurations or using scripts, as it relies on quantitative metrics to evaluate enforcement.",
        "Evaluation_Method": [
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation involves collecting and analyzing statistics from training logs, completion records, and assessment scores to determine the extent and effectiveness of training, using metrics derived from this data.",
        "Metric": [
            {
                "definition": "Training coverage rate, which measures the proportion of workforce members who have completed the required authentication training.",
                "measure.description": "Count of workforce members who have completed authentication training (M1) and total number of workforce members (M3)",
                "measure.id": "M1, M3",
                "equation": "M1 / M3"
            },
            {
                "definition": "Training effectiveness score, which measures the average performance on post-training assessments for authentication best practices.",
                "measure.description": "Average score on post-training assessments for authentication topics (M4)",
                "measure.id": "M4",
                "equation": "M4"
            }
        ]
    },
    {
        "Observable": "Training completion records, screen lock event logs, data mishandling incident reports, audit observations of desk cleanliness and whiteboard erasure, and records of data storage and destruction practices.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because training completion and compliance with practices like screen locking can be checked through records and direct observations; Measurable because outcomes such as incident rates and compliance percentages can be quantified and tracked over time.",
        "Evaluation_Method": [
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation utilizes historical data from training logs, incident reports, and system logs to assess trends and compliance; Active testing involves conducting audits, simulations, or direct observations to test current employee behavior and knowledge.",
        "Metric": [
            {
                "defination": "Training Completion Rate",
                "measure.description": "Count of workforce members who have completed the data handling training and total number of workforce members",
                "measure.id": "M1, M6",
                "equation": "M1 / M6"
            },
            {
                "defination": "Data Mishandling Incident Rate",
                "measure.description": "Count of reported data mishandling incidents and total number of workforce members",
                "measure.id": "M3, M6",
                "equation": "M3 / M6"
            },
            {
                "defination": "Screen Lock Compliance Rate",
                "measure.description": "Count of screen lock violations observed during audits and total number of audit observations for screen locking",
                "measure.id": "M4, M7",
                "equation": "1 - (M4 / M7)"
            },
            {
                "defination": "Whiteboard Erasure Compliance Rate",
                "measure.description": "Count of whiteboard not erased incidents observed and total number of audit observations for whiteboard usage",
                "measure.id": "M5, M8",
                "equation": "1 - (M5 / M8)"
            }
        ]
    },
    {
        "Observable": "Training completion records, data exposure incident logs, awareness survey results, and training session details",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because training attendance and completion can be confirmed through records such as certificates or logs; Measurable because the effectiveness of training can be quantified through metrics like incident rates and survey scores.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Evaluation relies on analyzing data from training completion logs, incident reports, and survey responses to assess the enforcement and effectiveness of the training program.",
        "Metric": [
            {
                "defination": "Proportion of workforce members who have completed the training",
                "measure.description": "M1: Count of workforce members who have completed the training, M2: Total number of workforce members",
                "measure.id": "M1, M2",
                "equation": "M1 / M2"
            },
            {
                "defination": "Reduction in data exposure incidents after training compared to before",
                "measure.description": "M3: Count of data exposure incidents in a reference period before training, M4: Count of data exposure incidents in a similar period after training",
                "measure.id": "M3, M4",
                "equation": "(M3 - M4) / M3"
            },
            {
                "defination": "Improvement in average awareness scores from pre-training to post-training",
                "measure.description": "M5: Average awareness score from pre-training survey, M6: Average awareness score from post-training survey",
                "measure.id": "M5, M6",
                "equation": "M6 - M5"
            }
        ]
    },
    {
        "Observable": [
            "Training completion records",
            "Incident report logs",
            "Simulation test results"
        ],
        "Class": [
            {
                "name": "Measurable",
                "explanation": "The safeguard requires data-driven analysis to measure the effectiveness of training in enabling users to recognize and report incidents, based on metrics such as training completion rates and incident reporting frequencies."
            }
        ],
        "Evaluation_Method": [
            {
                "name": "Data-driven",
                "explanation": "Evaluate by collecting and analyzing statistics from training records and incident reports to assess training completion and reporting behavior."
            },
            {
                "name": "Active testing",
                "explanation": "Probe the system by simulating potential incidents to test if users recognize and report them, providing direct evidence of training effectiveness."
            }
        ],
        "Metric": [
            {
                "definition": "The proportion of workforce members who have completed the required training.",
                "measure": {
                    "description": "Count of users who completed training (M1) and total number of users (M6).",
                    "id": "M1, M6"
                },
                "equation": "M1 / M6"
            },
            {
                "definition": "The percentage of simulated incidents that are reported by users, indicating the effectiveness of training in incident recognition and reporting.",
                "measure": {
                    "description": "Count of simulated incidents conducted (M4) and count of simulated incidents reported by users (M5).",
                    "id": "M4, M5"
                },
                "equation": "M5 / M4 if M4 > 0, else undefined"
            }
        ]
    },
    {
        "Observable": "Training completion records, incident reports from employees about out-of-date software patches or tool failures, and notifications to IT personnel.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "It is verifiable because training records and incident reports can be inspected to confirm implementation. It is measurable because the proportion of trained employees and the rate of reporting can be quantified.",
        "Evaluation_Method": "Data-driven, Active testing",
        "Evaluation_Method.explanation": "Data-driven evaluation involves collecting and analyzing data on training completions and reported incidents. Active testing involves simulating failures to test if employees report them as trained.",
        "Metric": [
            {
                "defination": "Training Completion Rate",
                "measure.description": "Count of employees who have completed the training and total number of employees",
                "measure.id": "M1, M2",
                "equation": "M1 / M2"
            },
            {
                "defination": "Incident Report Rate per Employee",
                "measure.description": "Number of reports made about out-of-date patches or tool failures in a specified period and total number of employees",
                "measure.id": "M3, M2",
                "equation": "M3 / M2"
            }
        ]
    },
    {
        "Observable": "Training completion records, survey responses on network security knowledge, configuration reports for home networks of remote workers, and logs of training sessions",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because training completion and configuration reports can be checked against organizational records; Measurable because knowledge and behavior changes can be assessed through data from surveys and monitoring",
        "Evaluation_Method": [
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven as it involves analyzing training completion data, survey results, and configuration reports; Active testing as it may require simulating insecure network scenarios to test user awareness and responses",
        "Metric": [
            {
                "definition": "The percentage of all workforce members who have completed the required training",
                "measure.description": "Count of workforce members who completed training and total number of workforce members",
                "measure.id": [
                    "M2",
                    "M1"
                ],
                "equation": "M2 / M1"
            },
            {
                "definition": "The percentage of remote workers who have completed the training",
                "measure.description": "Count of remote workers who completed training and total number of remote workers",
                "measure.id": [
                    "M4",
                    "M3"
                ],
                "equation": "M4 / M3"
            },
            {
                "definition": "The percentage of remote workers who have securely configured their home networks",
                "measure.description": "Count of remote workers with secure home network configuration and total number of remote workers",
                "measure.id": [
                    "M5",
                    "M3"
                ],
                "equation": "M5 / M3"
            },
            {
                "definition": "The average score on post-training knowledge assessments about network security dangers",
                "measure.description": "Average score from knowledge assessment surveys or tests",
                "measure.id": [
                    "M6"
                ],
                "equation": "M6"
            }
        ]
    },
    {
        "Observable": "Records of training completion, assessment scores, and logs of training activities for role-specific security awareness training.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the completion of training can be checked through records and system configurations. Measurable because the effectiveness can be quantified using data such as test scores or incident rates.",
        "Evaluation_Method": [
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven because we analyze data from training platforms and security incident reports. Active testing because we can conduct knowledge assessments or simulations to probe employees' skills.",
        "Metric": [
            {
                "defination": "Training coverage rate, representing the proportion of employees who have completed the required role-specific security training.",
                "measure.description": "Count of employees who completed training and total employees in roles requiring training.",
                "measure.id": "M1, M5",
                "equation": "M1 / M5"
            },
            {
                "defination": "Training effectiveness score, based on the average test score from training assessments.",
                "measure.description": "Average test score from training assessments.",
                "measure.id": "M3",
                "equation": "M3"
            }
        ]
    },
    {
        "Observable": "The service provider inventory document or database, including listings of service providers, their classifications, enterprise contacts, and records of reviews and updates.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the inventory can be directly inspected to confirm the existence of required elements such as provider names, classifications, and contacts. Measurable because quantitative metrics for completeness and review frequency can be derived from the data.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based as the inventory's structure and configuration can be examined against requirements. Data-driven as statistics from review logs and update records are analyzed to assess compliance and timeliness.",
        "Metric": [
            {
                "defination": "Classification Completeness: The proportion of service providers in the inventory that have a classification specified.",
                "measure.description": "M1: Count of service providers listed in the inventory. M2: Count of service providers with classification specified.",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "Contact Completeness: The proportion of service providers in the inventory that have an enterprise contact specified.",
                "measure.description": "M1: Count of service providers listed in the inventory. M3: Count of service providers with enterprise contact specified.",
                "measure.id": "M1, M3",
                "equation": "M3 / M1"
            },
            {
                "defination": "Review Timeliness: An indicator of whether the inventory has been reviewed within the last 365 days, as required for annual review.",
                "measure.description": "M4: Date of the last review of the inventory.",
                "measure.id": "M4",
                "equation": "1 if (current_date - M4) <= 365 else 0"
            }
        ]
    },
    {
        "Observable": "The service provider management policy document, including sections on classification, inventory, assessment, monitoring, and decommissioning of service providers, as well as records of policy reviews (e.g., dates) and updates triggered by significant changes.",
        "Class": [
            "Checklist",
            "Verifiable"
        ],
        "Class.explanation": "This safeguard is checklist-based because it can be assessed by verifying the presence and completeness of the policy against a predefined checklist of required elements. It is verifiable as the policy content can be examined to confirm it addresses all specified aspects, such as classification and monitoring.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves inspecting the policy document's configuration and content to ensure it meets the requirements. Data-driven evaluation can be applied by analyzing timestamps from review and update logs to compute metrics like review frequency and compliance.",
        "Metric": [
            {
                "defination": "Indicates whether the service provider management policy exists.",
                "measure.description": "Binary indicator (1 or 0) representing the presence of the policy document.",
                "measure.id": "M1",
                "equation": "M1"
            },
            {
                "defination": "Measures how well the policy covers all required aspects: classification, inventory, assessment, monitoring, and decommissioning.",
                "measure.description": "Count of the number of aspects addressed in the policy, ranging from 0 to 5.",
                "measure.id": "M2",
                "equation": "M2 / 5"
            },
            {
                "defination": "Assesses whether the policy is reviewed at least annually as required.",
                "measure.description": "Date of the last review (M3) and the current date (M4) for comparison.",
                "measure.id": "M3, M4",
                "equation": "1 if (M4 - M3) <= 365 else 0"
            }
        ]
    },
    {
        "Observable": "A documented classification system for service providers, including records of classifications with attributes such as data sensitivity, data volume, etc., and logs of review activities and updates.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard can be verified by inspecting documentation and logs to ensure that service providers are classified and reviews are conducted as required, without needing complex data analytics or active probing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "The evaluation uses the configuration of the classification system, such as documented policies, classification records, and review schedules, which can be modeled and checked against requirements.",
        "Metric": [
            {
                "defination": "The percentage of service providers that have been classified.",
                "measure.description": "Count of service providers with classification (M1) and total service providers (M5).",
                "measure.id": "M1, M5",
                "equation": "M1 / M5"
            },
            {
                "defination": "The percentage of classifications that were reviewed within the required timeframe (annually or after significant change).",
                "measure.description": "Count of classifications reviewed compliantly (M3) and total classifications (M1).",
                "measure.id": "M3, M1",
                "equation": "M3 / M1"
            }
        ]
    },
    {
        "Observable": "Service provider contracts, review records, and policy documents",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the presence of security requirements in contracts can be checked against a policy checklist through inspection. Measurable because the compliance rate and review frequency can be quantified over time using data analysis.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Data-driven evaluation is used because it involves collecting and analyzing data from contract databases and review logs to generate statistics on compliance with security requirements and the timeliness of reviews.",
        "Metric": [
            {
                "defination": "Percentage of service provider contracts that include all required security requirements and are consistent with the enterprise's policy",
                "measure.description": "M1: Total number of service provider contracts, M2: Number of contracts with all security requirements included and consistent with policy",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "Percentage of service provider contracts reviewed within the last 365 days",
                "measure.description": "M1: Total number of service provider contracts, M3: Number of contracts with last review date within the past 365 days",
                "measure.id": "M1, M3",
                "equation": "M3 / M1"
            }
        ]
    },
    {
        "Observable": "Assessment reports (e.g., SOC 2, PCI AoC), customized questionnaires, contract documents with assessment clauses, logs of review activities, and records of reassessments.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard is verifiable because enforcement can be checked by inspecting documents and records, such as assessment reports and policy compliance evidence, to confirm that assessments are conducted and aligned with the enterprise's service provider management policy.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Data-driven evaluation is used because it requires collecting and analyzing statistical data from assessment logs, contract dates, and review records to compute metrics like coverage, timeliness, and method compliance, which involve historical data and frequencies.",
        "Metric": [
            {
                "defination": "Measures the proportion of service providers that have been assessed according to the enterprise's policy.",
                "measure": {
                    "description": "Count of service providers with assessments conducted and compliant with policy (M1), and total number of service providers (M3).",
                    "id": "M1, M3"
                },
                "equation": "M1 / M3"
            },
            {
                "defination": "Measures the proportion of service providers reassessed on time, either annually or with new/renewed contracts, as per policy.",
                "measure": {
                    "description": "Count of service providers reassessed within the required timeframe (e.g., within 365 days or upon contract renewal) (M4), and total number of service providers (M3).",
                    "id": "M4, M3"
                },
                "equation": "M4 / M3"
            },
            {
                "defination": "Measures the proportion of assessments that use appropriate methods, such as standardized reports or customized questionnaires, as specified in the policy.",
                "measure": {
                    "description": "Count of assessment reports using standardized methods (e.g., SOC 2, PCI AoC) (M6), count of assessment reports using customized questionnaires (M7), and count of service providers with assessments conducted (M1).",
                    "id": "M6, M7, M1"
                },
                "equation": "(M6 + M7) / M1"
            }
        ]
    },
    {
        "Observable": "Logs of monitoring activities, reassessment reports, records of release note checks, and dark web monitoring alerts.",
        "Class": "Measurable",
        "Class.explanation": "The safeguard requires quantitative analysis of monitoring data, compliance reports, and external scans to assess enforcement, as it involves measuring frequencies, coverage, and effectiveness rather than simple checklist verification or configuration checks.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Evaluation relies on collecting and analyzing data from various sources such as monitoring tool logs, reassessment outcomes, release note review records, and dark web scan results to generate statistics on compliance, coverage, and incident handling.",
        "Metric": [
            {
                "defination": "Proportion of service providers that are actively monitored according to the policy.",
                "measure.description": "Count of service providers with monitoring activities logged.",
                "measure.id": "M1",
                "equation": "Coverage = M1 / M2"
            },
            {
                "defination": "Percentage of service providers reassessed within the policy-defined period (e.g., quarterly).",
                "measure.description": "Count of service providers with a reassessment completed within the last policy period (e.g., 90 days).",
                "measure.id": "M3",
                "equation": "Reassessment Compliance Rate = M3 / M2"
            },
            {
                "defination": "Rate at which service provider release notes are monitored and reviewed.",
                "measure.description": "Count of release notes reviewed from service providers.",
                "measure.id": "M4",
                "equation": "Release Note Monitoring Rate = M4 / M5"
            },
            {
                "defination": "Proportion of dark web monitoring alerts that are addressed or resolved.",
                "measure.description": "Count of dark web monitoring alerts that have been addressed or resolved.",
                "measure.id": "M6",
                "equation": "Dark Web Monitoring Effectiveness = M6 / M7"
            }
        ]
    },
    {
        "Observable": "Logs of user and service account deactivation, records of data flow termination, certificates of secure data disposal, inventory of decommissioned service providers.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because system configurations and logs can be inspected to confirm deactivation and termination actions. Measurable because event data from logs and activities can be analyzed to compute rates and completeness of decommissioning.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven for analyzing event logs, such as deactivation and disposal events, to measure decommissioning activities. Model-based for verifying configurations, like account status and data flow settings, to ensure compliance.",
        "Metric": {
            "defination": "The proportion of service providers that have been securely decommissioned, indicating completeness of the decommissioning process.",
            "measure.description": [
                "Count of service providers securely decommissioned",
                "Total number of service providers to decommission"
            ],
            "measure.id": [
                "M1",
                "M11"
            ],
            "equation": "M1 / M11"
        }
    },
    {
        "Observable": "Secure application design standards document, secure coding practices guidelines, records of developer training, vulnerability management reports, security assessments of third-party code, application security testing procedures, and logs of documentation reviews and updates.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the existence and content of documentation can be directly inspected to confirm compliance. Measurable because quantitative aspects such as the number of documented items and the time between reviews can be calculated and assessed.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based because evaluation involves comparing the documentation against a predefined model of required items (e.g., the six specified process aspects). Data-driven because it requires collecting and analyzing data from documents, logs, and records to derive metrics.",
        "Metric": [
            {
                "defination": "The proportion of required secure application development process items that are documented.",
                "measure.description": [
                    "Number of documented items from the required list",
                    "Total number of required items (fixed at 6)"
                ],
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / M2"
            },
            {
                "defination": "Indicates whether the documentation is reviewed within the required frequency (annually or after significant changes).",
                "measure.description": [
                    "Date of the last documentation review",
                    "Current date",
                    "Number of significant changes recorded since the last review"
                ],
                "measure.id": [
                    "M3",
                    "M4",
                    "M5"
                ],
                "equation": "IF ( (M4 - M3) <= 365 OR M5 > 0, 1, 0 )"
            }
        ]
    },
    {
        "Observable": "Vulnerability handling policy document, vulnerability tracking system (e.g., software like JIRA) with configured metrics and severity ratings, logs of vulnerability reports and their handling status (intake, assignment, remediation, testing), records of policy reviews and updates, and means for external reporting (e.g., web form or email).",
        "Class": "Checklist, Verifiable, Measurable",
        "Class.explanation": "Checklist: The existence of key components like the policy and tracking system can be verified through automated scripts or checklists. Verifiable: The content of the policy can be inspected to ensure it includes required elements such as reporting process and responsible party. Measurable: Quantitative aspects like timing metrics and address rates can be measured from data collected by the tracking system.",
        "Evaluation_Method": "Model-based, Data-driven, Active testing",
        "Evaluation_Method.explanation": "Model-based: The policy documentation and system configurations can be evaluated against established standards and requirements. Data-driven: Metrics from the tracking system, such as time deltas and report counts, require statistical analysis of event logs and data. Active testing: The reporting process can be tested by submitting simulated vulnerability reports to verify intake and handling.",
        "Metric": [
            {
                "defination": "Percentage of vulnerability reports that are addressed or remediated.",
                "measure.description": "M_addressed: number of vulnerability reports marked as addressed or closed, M_total: total number of vulnerability reports received in a specified period.",
                "measure.id": "M_addressed, M_total",
                "equation": "(M_addressed / M_total) * 100"
            },
            {
                "defination": "Average time taken from report receipt to identification of the vulnerability.",
                "measure.description": "T_ident: time delta in days for each report from the time it was received to the time it was identified.",
                "measure.id": "T_ident",
                "equation": "mean(T_ident)"
            },
            {
                "defination": "Average time from identification to analysis of the vulnerability.",
                "measure.description": "T_analysis: time delta in days for each report from identification time to analysis time.",
                "measure.id": "T_analysis",
                "equation": "mean(T_analysis)"
            },
            {
                "defination": "Average time from analysis to remediation of the vulnerability.",
                "measure.description": "T_remediate: time delta in days for each report from analysis time to remediation time.",
                "measure.id": "T_remediate",
                "equation": "mean(T_remediate)"
            },
            {
                "defination": "Indicates whether the vulnerability handling policy was reviewed or updated within the last year.",
                "measure.description": "D_last_update: date of the last policy update or review.",
                "measure.id": "D_last_update",
                "equation": "1 if (current_date - D_last_update) <= 365 days, else 0"
            },
            {
                "defination": "Score based on the inclusion of required elements in the vulnerability handling policy.",
                "measure.description": "C_elements: count of required elements present in the policy (e.g., reporting process, responsible party, intake process, assignment process, remediation process, remediation testing process).",
                "measure.id": "C_elements",
                "equation": "(C_elements / 6) * 100"
            }
        ]
    },
    {
        "Observable": "Reports, logs, and documentation of root cause analyses performed on security vulnerabilities, including timestamps of analysis, findings, implementation status of recommendations, and records of vulnerability identifications.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the presence and configuration of root cause analysis processes can be checked through documentation reviews and system configurations; Measurable because the frequency, timeliness, and effectiveness of root cause analyses can be quantified using data from logs and reports.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves verifying the setup and configuration of root cause analysis tools and processes in systems like issue trackers or security management platforms; Data-driven evaluation involves analyzing statistical data from RCA logs, vulnerability databases, and implementation records to compute metrics such as coverage and timeliness.",
        "Metric": [
            {
                "definition": "The proportion of identified vulnerabilities that have undergone root cause analysis.",
                "measure.description": [
                    "Count of vulnerabilities that have undergone root cause analysis",
                    "Total number of vulnerabilities identified"
                ],
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "M1 / M3"
            },
            {
                "definition": "The average time taken to complete root cause analysis after vulnerability identification.",
                "measure.description": [
                    "Time duration from vulnerability identification to RCA completion for each vulnerability"
                ],
                "measure.id": [
                    "M4"
                ],
                "equation": "average(M4)"
            },
            {
                "definition": "The proportion of root cause analysis findings that are implemented.",
                "measure.description": [
                    "Number of RCA findings that are implemented",
                    "Number of RCA findings that are not implemented"
                ],
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 / (M5 + M6)"
            }
        ]
    },
    {
        "Observable": [
            "Bill of materials inventory document or database",
            "Logs of monthly evaluations",
            "Records of risk assessments for each third-party component",
            "Validation records for component support status"
        ],
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Checklist: The existence of the inventory and basic properties like presence of risk information can be checked via automated scripts or manual checks. Verifiable: The content and completeness, such as whether all components have risk assessments and support validations, can be inspected and verified against system configurations. Measurable: Quantitative aspects like the proportion of components with risk info, the time since last evaluation, and the rate of support validation can be measured using data from the inventory.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven: Evaluation involves analyzing data from the inventory, evaluation logs, and risk records to compute statistics like coverage rates and frequencies. Model-based: The setup and configuration of the inventory system, such as database schemas or file structures, can be assessed to ensure proper management. Active testing: The system can be probed by querying the inventory for specific components or externally validating support status through APIs or manual checks.",
        "Metric": [
            {
                "defination": "Percentage of third-party components in the inventory that have risk information documented.",
                "measure.description": "M1: Total number of third-party components in the inventory. M2: Number of components with risk information documented.",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "(M2 / M1) * 100"
            },
            {
                "defination": "Percentage of components that have been validated as still supported.",
                "measure.description": "M1: Total number of third-party components in the inventory. M3: Number of components validated as supported.",
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "(M3 / M1) * 100"
            },
            {
                "defination": "Number of days since the last evaluation was performed, to assess timeliness against the monthly requirement.",
                "measure.description": "M4: Date of the last evaluation (in days since a reference date, e.g., Unix timestamp). M5: Current date or assessment date (in same units as M4).",
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "M5 - M4"
            }
        ]
    },
    {
        "Observable": "Lists of third-party software components with versions, acquisition sources (e.g., trusted repositories), and records of vulnerability evaluations performed before use or periodically.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because we can inspect configuration files, dependency lists, and logs to confirm the use of trusted sources and the performance of vulnerability evaluations. Measurable because we can quantify the proportions and frequencies using data analysis on software inventories and vulnerability databases.",
        "Evaluation_Method": [
            "Data-driven",
            "Model-based"
        ],
        "Evaluation_Method.explanation": "Data-driven because it involves analyzing data from software inventories, vulnerability databases (e.g., NVD), and update logs to assess compliance. Model-based because we can model expected behaviors, such as sourcing from trusted repositories or following evaluation protocols, and compare against actual system configurations.",
        "Metric": [
            {
                "defination": "Percentage of software components that are either acquired from trusted sources or evaluated for vulnerabilities before use",
                "measure.description": "M1: Count of components acquired from trusted sources, M2: Count of components acquired from untrusted sources but evaluated before use, M3: Count of components acquired from untrusted sources and not evaluated before use",
                "measure.id": [
                    "M1",
                    "M2",
                    "M3"
                ],
                "equation": "(M1 + M2) / (M1 + M2 + M3)"
            },
            {
                "defination": "Percentage of software components that are up-to-date (latest version)",
                "measure.description": "M4: Count of up-to-date components, M5: Count of outdated components",
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "M4 / (M4 + M5)"
            }
        ]
    },
    {
        "Observable": "Documentation of the severity rating system, records of vulnerability assessments with severity ratings, annual review reports, and evidence of prioritization processes in action (e.g., from bug tracking systems).",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the existence and content of the severity rating documentation and process can be inspected through checks of policy documents and configuration settings. Measurable because aspects like the coverage of severity ratings, compliance with annual reviews, and effectiveness of prioritization can be quantified using data from vulnerability management systems.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven because evaluation involves analyzing statistical data from vulnerability logs, review records, and prioritization actions to assess enforcement. Model-based because it requires verifying against the documented process model of the severity rating system, such as checking if the system adheres to the defined criteria and update schedules.",
        "Metric": {
            "definition": "Severity Rating Coverage measures the proportion of discovered vulnerabilities that have a severity rating assigned, indicating how well the rating system is implemented.",
            "measure.description": "Count of vulnerabilities with a severity rating assigned and count of vulnerabilities without a severity rating assigned.",
            "measure.id": "M1, M2",
            "equation": "M1 / (M1 + M2)"
        }
    },
    {
        "Observable": "Configuration files, system settings, audit logs of configuration changes, presence of standard hardening templates, absence of configuration alterations by in-house software",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because system configurations can be checked against industry-standard templates; Measurable because compliance rates and weakening incidents can be quantified and analyzed over time",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based by comparing configurations to standard hardening models; Data-driven by collecting and analyzing data on compliance and weakening events from logs and scans",
        "Metric": [
            {
                "defination": "Percentage of systems with standard hardening configuration applied",
                "measure.description": "Count of systems compliant with standard templates and total number of systems",
                "measure.id": [
                    "M1",
                    "M4"
                ],
                "equation": "M1 / M4"
            },
            {
                "defination": "Percentage of systems where in-house software has weakened configuration hardening",
                "measure.description": "Count of systems weakened by in-house software and total number of systems",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M3 / M4"
            },
            {
                "defination": "Rate of preventing configuration weakening by in-house software",
                "measure.description": "Count of systems not weakened and total number of systems",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "1 - (M3 / M4)"
            }
        ]
    },
    {
        "Observable": "Network segmentation configurations, system environment assignments, and network traffic logs indicating separation between production and non-production systems.",
        "Class": [
            "Verifiable",
            "Checklist"
        ],
        "Class.explanation": "Verifiable because the separation can be confirmed by inspecting network and system configurations. Checklist because automated scripts can be written to verify the setup and check for compliance.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based because evaluation uses configuration data and models of the network to assess separation. Active testing because probing the network (e.g., sending test traffic) can verify isolation between environments.",
        "Metric": [
            {
                "definition": "Measures the percentage of systems correctly placed in their respective environments to ensure separation.",
                "measure.description": [
                    "Count of production systems located in the production network segment",
                    "Count of production systems incorrectly located in non-production network segment",
                    "Count of non-production systems located in the non-production network segment",
                    "Count of non-production systems incorrectly located in production network segment"
                ],
                "measure.id": [
                    "M1",
                    "M2",
                    "M3",
                    "M4"
                ],
                "equation": "Compliance Rate = (M1 + M3) / (M1 + M2 + M3 + M4)"
            },
            {
                "definition": "Measures the amount of unauthorized network traffic between production and non-production environments to assess isolation quality.",
                "measure.description": [
                    "Volume of network traffic from production to non-production (in bytes or packets)",
                    "Volume of network traffic from non-production to production (in bytes or packets)",
                    "Total volume of traffic within production environment (in bytes or packets)",
                    "Total volume of traffic within non-production environment (in bytes or packets)"
                ],
                "measure.id": [
                    "M5",
                    "M6",
                    "M7",
                    "M8"
                ],
                "equation": "Leakage Ratio = (M5 + M6) / (M7 + M8 + M5 + M6)"
            }
        ]
    },
    {
        "Observable": "Training completion records, attendance logs, training schedules, certificates of completion, and documentation of training design promoting security culture.",
        "Class": "Measurable, Verifiable",
        "Class.explanation": "Measurable because training completion rates and frequency can be quantified using data from records. Verifiable because training schedules and completion logs can be inspected for compliance.",
        "Evaluation_Method": "Data-driven",
        "Evaluation_Method.explanation": "Data-driven evaluation involves analyzing statistics from training attendance records, session logs, and potentially survey data to assess enforcement.",
        "Metric": [
            {
                "defination": "Percentage of software development personnel who have completed secure code training within the past 12 months.",
                "measure.description": [
                    "Count of personnel trained in the past year",
                    "Total number of software development personnel"
                ],
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "(M1 / M3) * 100"
            },
            {
                "defination": "Compliance with conducting training sessions at least annually, based on the number of sessions per year.",
                "measure.description": [
                    "Number of training sessions conducted in the past 12 months"
                ],
                "measure.id": [
                    "M6"
                ],
                "equation": "1 if M6 >= 1, else 0"
            },
            {
                "defination": "Average score from security culture surveys to measure the promotion of security within the development team.",
                "measure.description": [
                    "Average score from security culture surveys"
                ],
                "measure.id": [
                    "M7"
                ],
                "equation": "M7"
            }
        ]
    },
    {
        "Observable": "Application configuration files, design documents documenting secure principles, system logs showing input validation errors and error checking, settings for port and service management (e.g., disabled ports, removed programs), account configuration details (e.g., renamed default accounts), and evidence of least privilege implementation in access controls.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because secure design principles can be checked through inspection of application configurations, code reviews, and architecture documents; Measurable because the extent of implementation can be quantified using data from logs and metrics such as error rates, adoption percentages, and attack surface reduction scores.",
        "Evaluation_Method": [
            "Model-based",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves using system configurations and design models to assess adherence to secure principles; Active testing includes probing applications with inputs to validate error handling, input validation, and other security measures through techniques like penetration testing.",
        "Metric": [
            {
                "defination": "Secure Design Adoption Rate",
                "measure.description": "M1: Number of applications with documented secure design principles; M7: Total number of applications",
                "measure.id": "M1,M7",
                "equation": "(M1 / M7) * 100"
            },
            {
                "defination": "Input Validation Coverage",
                "measure.description": "M2: Total number of input validation mechanisms implemented across applications; M7: Total number of applications",
                "measure.id": "M2,M7",
                "equation": "M2 / M7"
            },
            {
                "defination": "Error Handling Implementation Rate",
                "measure.description": "M3: Number of error checking mechanisms in place; M7: Total number of applications",
                "measure.id": "M3,M7",
                "equation": "M3 / M7"
            },
            {
                "defination": "Attack Surface Reduction Score",
                "measure.description": "M4: Number of unprotected ports open; M5: Number of unnecessary programs installed; M6: Number of default accounts not modified; M10: Total number of ports; M11: Total number of programs; M12: Total number of default accounts",
                "measure.id": "M4,M5,M6,M10,M11,M12",
                "equation": "( ( (M10 - M4) / M10 ) + ( (M11 - M5) / M11 ) + ( (M12 - M6) / M12 ) ) / 3 * 100"
            },
            {
                "defination": "Input Validation Failure Rate",
                "measure.description": "M8: Number of input validation failures logged over a period; M13: Total number of input attempts over the same period",
                "measure.id": "M8,M13",
                "equation": "(M8 / M13) * 100"
            }
        ]
    },
    {
        "Observable": "Configuration settings indicating use of vetted security modules (e.g., for identity management, encryption, auditing, logging), audit logs showing secure components in use, and absence of custom or unvetted security implementations.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the use of vetted modules can be confirmed through inspection of system configurations, module settings, and documentation. Measurable because the proportion of applications complying with the use of standardized components can be quantified and tracked over time.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves examining configuration files, system settings, and module specifications to verify the implementation of vetted security components. Data-driven evaluation analyzes log data, usage patterns, and application behaviors to ensure that the security components are functioning as intended and to detect any deviations.",
        "Metric": [
            {
                "defination": "Percentage of applications using vetted identity management modules",
                "measure.description": "M1: Count of applications using vetted identity management modules, M2: Count of applications not using vetted identity management modules",
                "measure.id": "M1, M2",
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Percentage of applications using standardized encryption algorithms",
                "measure.description": "M3: Count of applications using standardized encryption algorithms, M4: Count of applications using non-standard encryption algorithms",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Percentage of applications using platform features for auditing",
                "measure.description": "M5: Count of applications using platform features for auditing, M6: Count of applications not using platform features for auditing",
                "measure.id": "M5, M6",
                "equation": "M5 / (M5 + M6)"
            },
            {
                "defination": "Percentage of applications using platform features for logging",
                "measure.description": "M7: Count of applications using platform features for logging, M8: Count of applications not using platform features for logging",
                "measure.id": "M7, M8",
                "equation": "M7 / (M7 + M8)"
            }
        ]
    },
    {
        "Observable": "Scan reports from static and dynamic analysis tools (e.g., SAST and DAST outputs), tool configuration settings in CI/CD pipelines, integration logs showing scan executions, and records of vulnerabilities detected and fixed.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the presence and configuration of analysis tools can be checked through system settings and logs. Measurable because the effectiveness of following secure coding practices can be assessed through data analytics on scan results and vulnerability counts.",
        "Evaluation_Method": "Model-based, Data-driven",
        "Evaluation_Method.explanation": "Model-based evaluation involves examining the configuration and integration of static and dynamic analysis tools in the application lifecycle. Data-driven evaluation requires analyzing data from scan reports, such as the number of scans and vulnerabilities, to assess compliance with secure coding practices.",
        "Metric": [
            {
                "defination": "Percentage of applications that have static analysis tools integrated.",
                "measure.description": "M1: Count of applications with static analysis tools integrated, M10: Total number of applications",
                "measure.id": "M1, M10",
                "equation": "M1 / M10"
            },
            {
                "defination": "Percentage of applications that have dynamic analysis tools integrated.",
                "measure.description": "M3: Count of applications with dynamic analysis tools integrated, M10: Total number of applications",
                "measure.id": "M3, M10",
                "equation": "M3 / M10"
            },
            {
                "defination": "Average number of static analysis scans performed per application per time period (e.g., per month).",
                "measure.description": "M5: Number of static analysis scans performed in a time period, M10: Total number of applications",
                "measure.id": "M5, M10",
                "equation": "M5 / M10"
            },
            {
                "defination": "Average number of dynamic analysis scans performed per application per time period (e.g., per month).",
                "measure.description": "M6: Number of dynamic analysis scans performed in a time period, M10: Total number of applications",
                "measure.id": "M6, M10",
                "equation": "M6 / M10"
            },
            {
                "defination": "Number of vulnerabilities detected per static analysis scan, indicating the effectiveness of tool usage.",
                "measure.description": "M7: Number of vulnerabilities detected by static analysis, M5: Number of static analysis scans performed",
                "measure.id": "M7, M5",
                "equation": "M7 / M5"
            },
            {
                "defination": "Number of vulnerabilities detected per dynamic analysis scan, indicating the effectiveness of tool usage.",
                "measure.description": "M8: Number of vulnerabilities detected by dynamic analysis, M6: Number of dynamic analysis scans performed",
                "measure.id": "M8, M6",
                "equation": "M8 / M6"
            },
            {
                "defination": "Percentage of detected vulnerabilities that have been fixed, assessing the responsiveness to findings.",
                "measure.description": "M9: Number of vulnerabilities fixed after detection, M7: Number of vulnerabilities detected by static analysis, M8: Number of vulnerabilities detected by dynamic analysis",
                "measure.id": "M9, M7, M8",
                "equation": "M9 / (M7 + M8)"
            }
        ]
    },
    {
        "Observable": "Penetration test reports, testing schedules, logs of testing activities, list of applications tested, findings including business logic vulnerabilities, and evidence of authenticated and unauthenticated testing.",
        "Class": "Measurable",
        "Class.explanation": "Penetration testing produces quantifiable results such as the number of tests conducted, coverage of applications, and vulnerabilities found, which can be measured to assess enforcement quality.",
        "Evaluation_Method": [
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven: Evaluation involves analyzing statistics from penetration test reports, vulnerability databases, and logs to measure outcomes like coverage and effectiveness. Active testing: Evaluation may require probing the system through audits or sample tests to verify that penetration testing is conducted as specified, especially for critical applications.",
        "Metric": [
            {
                "defination": "The proportion of critical applications that have undergone penetration testing.",
                "measure.description": "M1: Count of critical applications; M2: Count of critical applications that have been penetration tested",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "Coverage = M2 / M1"
            },
            {
                "defination": "The proportion of penetration tests on critical applications that are authenticated, as authenticated testing is emphasized for finding business logic vulnerabilities.",
                "measure.description": "M3: Count of authenticated penetration tests on critical applications; M4: Count of all penetration tests on critical applications",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "Authenticated Testing Rate = M3 / M4"
            },
            {
                "defination": "The average number of business logic vulnerabilities found per authenticated penetration test on critical applications, indicating effectiveness in detecting specific vulnerabilities.",
                "measure.description": "M5: Number of business logic vulnerabilities found; M3: Count of authenticated penetration tests on critical applications",
                "measure.id": [
                    "M5",
                    "M3"
                ],
                "equation": "Vulnerability Yield = M5 / M3"
            },
            {
                "defination": "The frequency of penetration testing, assessed by the time between consecutive tests for an application, to ensure regular testing.",
                "measure.description": "M6: Date of the most recent penetration test for an application; M7: Date of the previous penetration test for the same application",
                "measure.id": [
                    "M6",
                    "M7"
                ],
                "equation": "Testing Frequency = M6 - M7 (in days, should be compared to a benchmark like annual testing)"
            }
        ]
    },
    {
        "Observable": "Threat modeling reports, documentation of evaluation sessions, records of trained individuals performing threat modeling, updated design documents reflecting security fixes.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Threat modeling is verifiable because the existence and quality of threat models can be checked through documentation review. It is measurable because aspects like coverage and effectiveness can be quantified using data from the process.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Threat modeling involves examining application design models to identify security flaws, so evaluation is based on analyzing these models and their configurations rather than live testing or pure data analysis.",
        "Metric": {
            "defination": "Threat Modeling Coverage - The proportion of applications that have undergone threat modeling.",
            "measure": {
                "description": "Count of applications with threat models (M1) and count of applications without threat models (M2).",
                "id": "M1, M2"
            },
            "equation": "M1 / (M1 + M2)"
        }
    },
    {
        "Observable": "Documentation listing the designated key and backup incident handlers, records of incident response coordination and documentation, evidence of oversight for third-party providers if used, and logs of annual reviews or reviews triggered by significant changes.",
        "Class": [
            "Verifiable",
            "Checklist"
        ],
        "Class.explanation": "This safeguard can be verified by examining organizational documents, such as incident response plans and role assignments, to confirm the presence of required designations. It can also be assessed using a checklist to ensure that all elements (key person, backup, oversight, and reviews) are in place and documented.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Model-based evaluation involves checking the configuration of roles in identity management systems or organizational documentation. Data-driven evaluation involves analyzing the timeliness and frequency of reviews based on date logs and records, such as review dates and triggers from significant changes.",
        "Metric": [
            {
                "defination": "Ensures that there is at least one key person and one backup person designated for incident handling.",
                "measure.description": "Count of designated key incident handlers and count of designated backup incident handlers.",
                "measure.id": "M1, M2",
                "equation": "1 if M1 >= 1 and M2 >= 1, else 0"
            },
            {
                "defination": "If a service provider is used for incident handling, ensures that there is at least one internal person designated to oversee the third-party work.",
                "measure.description": "Indicator if service provider is used and count of internal overseers designated.",
                "measure.id": "M3, M4",
                "equation": "If M3 == 1, then 1 if M4 >= 1, else 0; if M3 == 0, then 1 (not applicable)"
            },
            {
                "defination": "Ensures that reviews of the incident handling designations are conducted at least annually or when significant changes occur, based on time since last review.",
                "measure.description": "Date of last review and current date for calculating the time elapsed.",
                "measure.id": "M5, M6",
                "equation": "1 if (M6 - M5) <= 365 days, else 0"
            }
        ]
    },
    {
        "Observable": "A documented list of contact information for security incidents, including verification dates and details of parties such as internal staff, service providers, law enforcement, etc.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be assessed by verifying the existence, completeness, and accuracy of the contact list through documentation review, which involves checking static records and configurations without the need for dynamic data analysis or active probing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is based on examining the documentation and verification records, which are part of the system's configuration or governance artifacts, allowing for assessment through inspection of setup and records rather than data analytics or active testing.",
        "Metric": [
            {
                "defination": "Contact List Completeness - The ratio of listed contacts to the expected number of contacts based on organizational policy.",
                "measure": [
                    {
                        "id": "M1",
                        "description": "Count of contacts listed in the documentation"
                    },
                    {
                        "id": "M2",
                        "description": "Count of expected contacts that should be listed based on policy or requirements"
                    }
                ],
                "equation": "M1 / M2"
            },
            {
                "defination": "Verification Freshness - The proportion of listed contacts that have been verified within the last 365 days to ensure information is up-to-date.",
                "measure": [
                    {
                        "id": "M1",
                        "description": "Count of contacts listed in the documentation"
                    },
                    {
                        "id": "M3",
                        "description": "Count of contacts with a verification date within the last 365 days"
                    }
                ],
                "equation": "M3 / M1"
            }
        ]
    },
    {
        "Observable": "The documented incident reporting process, its accessibility to the workforce (e.g., on intranet or in handbooks), records of annual reviews or reviews triggered by changes, and logs of security incidents reported following the process.",
        "Class": [
            "Checklist",
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Checklist: Automated scripts can check for the existence and accessibility of the document. Verifiable: Manual inspection can verify the content includes required elements like timeframe, personnel, mechanism, and minimum information. Measurable: Data analysis can assess reporting compliance rates and review frequency using incident logs and review records.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based: Evaluate the configuration and content of the documented process. Data-driven: Analyze statistics from incident reports, access logs, and review records. Active testing: Conduct surveys or simulations to test workforce awareness and usage of the reporting process.",
        "Metric": [
            {
                "defination": "Proportion of the workforce with access to the incident reporting document.",
                "measure.description": "Count of workforce members with access to the document and count without access.",
                "measure.id": "M3, M4",
                "equation": "M3 / (M3 + M4)"
            },
            {
                "defination": "Proportion of required elements present in the documented process.",
                "measure.description": "Number of elements present (timeframe, personnel, mechanism, minimum information) and total required elements (4).",
                "measure.id": "M2, total_elements=4",
                "equation": "M2 / 4"
            },
            {
                "defination": "Time since the last review of the process, indicating review frequency.",
                "measure.description": "Date of the last review and current date.",
                "measure.id": "M5, current_date",
                "equation": "current_date - M5"
            },
            {
                "defination": "Proportion of security incidents reported correctly according to the process.",
                "measure.description": "Number of incidents reported correctly and total number of incidents detected.",
                "measure.id": "M8, M7",
                "equation": "M8 / M7"
            }
        ]
    },
    {
        "Observable": "Documented incident response process file, review logs or records, timestamps of reviews, and records of significant enterprise changes.",
        "Class": [
            {
                "name": "Checklist",
                "explanation": "The safeguard can be assessed by scripting to check for the existence of the documented process and its key elements, such as roles and responsibilities, compliance requirements, and communication plan."
            },
            {
                "name": "Verifiable",
                "explanation": "The content of the document can be verified by inspecting it to ensure it addresses all required components, and review activities can be confirmed through logs or records."
            }
        ],
        "Evaluation_Method": [
            {
                "name": "Model-based",
                "explanation": "Evaluation relies on the state and configuration of the documentation, such as metadata, content completeness, and review timestamps, to assess enforcement."
            },
            {
                "name": "Active testing",
                "explanation": "Probing is required to access the document, check review logs, or simulate scenarios to verify that reviews are conducted as specified, especially after significant changes."
            }
        ],
        "Metric": [
            {
                "defination": "Presence of the documented incident response process.",
                "measure.description": "Indicator if the document exists (1 if yes, 0 if no).",
                "measure.id": "M1",
                "equation": "M1"
            },
            {
                "defination": "Average completeness of the document in addressing required elements: roles and responsibilities, compliance requirements, and communication plan.",
                "measure.description": [
                    "Indicator if roles and responsibilities are addressed (1 if yes, 0 if no).",
                    "Indicator if compliance requirements are addressed (1 if yes, 0 if no).",
                    "Indicator if communication plan is addressed (1 if yes, 0 if no)."
                ],
                "measure.id": [
                    "M2",
                    "M3",
                    "M4"
                ],
                "equation": "(M2 + M3 + M4) / 3"
            },
            {
                "defination": "Compliance with review requirements: whether reviews are conducted annually or within a specified period after significant changes.",
                "measure.description": [
                    "Last review date (timestamp).",
                    "Current date (timestamp for calculation).",
                    "Date of last significant change (timestamp, if available).",
                    "Indicator if review was done after last significant change (1 if last review date > last change date, else 0).",
                    "Indicator if review is within the last 365 days (1 if time since last review <= 365 days, else 0)."
                ],
                "measure.id": [
                    "M5",
                    "M6",
                    "M7",
                    "M8",
                    "M9"
                ],
                "equation": "1 if (M9 == 1) or (M8 == 1), else 0"
            }
        ]
    },
    {
        "Observable": "Incident response plan document, role assignment records, review meeting minutes, audit logs of review activities, evidence of updates after significant enterprise changes",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the existence and details of role assignments and reviews can be confirmed through inspection of documents and records. Measurable because the percentage of roles assigned and the adherence to review schedules can be quantified numerically.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation relies on the documented model of the incident response framework, including role assignments and review policies, without requiring data-driven analysis from event logs or active testing.",
        "Metric": [
            {
                "defination": "The proportion of key roles that have been assigned responsibilities.",
                "measure.description": "M1: Count of key roles with assigned responsibilities, M2: Total number of key roles expected (e.g., from policy or inventory).",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / M2"
            },
            {
                "defination": "Whether the review of roles and responsibilities is conducted as per policy (annually or after significant changes).",
                "measure.description": "M3: Date of the last review conducted, M4: Date of the last significant enterprise change (if any), M5: Current date.",
                "measure.id": [
                    "M3",
                    "M4",
                    "M5"
                ],
                "equation": "1 if ( (M5 - M3) <= 365 days ) or ( M4 is not null and M3 > M4 ), else 0"
            }
        ]
    },
    {
        "Observable": "The documented incident response communication plan specifying primary and secondary mechanisms, records of annual reviews or reviews after significant enterprise changes, and evidence of communication mechanisms such as logs or configurations for phone calls, emails, secure chat, or notification letters.",
        "Class": "Verifiable",
        "Class.explanation": "This safeguard is verifiable because it can be assessed by checking the existence, content, and completeness of documentation, such as the communication plan and review records, through manual inspection or automated verification of files and logs, without requiring data-driven analytics or active probing.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "The evaluation method is model-based because it involves using the configuration and documentation of the incident response plan (e.g., the specified mechanisms and review schedules) to assess enforcement, by comparing against required standards without generating statistics or actively testing the system.",
        "Metric": [
            {
                "defination": "Indicates whether a documented communication plan exists for incident response.",
                "measure.description": "Binary indicator (0 or 1) representing the existence of the communication plan document.",
                "measure.id": "M1",
                "equation": "M1"
            },
            {
                "defination": "Measures the percentage of expected communication mechanisms documented in the plan, with at least primary and secondary mechanisms specified.",
                "measure.description": "Count of mechanisms documented (M2) and expected minimum mechanisms (M3, constant value of 2 for primary and secondary).",
                "measure.id": "M2, M3",
                "equation": "min(M2 / M3, 1) * 100"
            },
            {
                "defination": "Indicates whether the communication plan has been reviewed within the required time frame (annually or after significant changes).",
                "measure.description": "Date of last review (M4) and current date (M5), used to calculate the time difference.",
                "measure.id": "M4, M5",
                "equation": "1 if (M5 - M4) <= 365 days, else 0"
            }
        ]
    },
    {
        "Observable": "Records of incident response exercises, including exercise plans, participation logs, after-action reports, documentation of tested aspects (communication channels, decision making, workflows), and dates of exercises.",
        "Class": "Measurable, Verifiable",
        "Class.explanation": "Measurable because quantitative data such as exercise frequency, aspect coverage, and participation rates can be analyzed; Verifiable because exercise documentation and logs can be inspected to confirm that exercises were planned and conducted as required.",
        "Evaluation_Method": "Data-driven, Active testing",
        "Evaluation_Method.explanation": "Data-driven evaluation involves generating statistics from exercise records, logs, and reports to assess frequency and coverage; Active testing is used because conducting the exercises themselves probes and tests the incident response capabilities, simulating real-world scenarios.",
        "Metric": [
            {
                "defination": "Annual Exercise Frequency Compliance - Indicates whether at least one incident response exercise is conducted within a 365-day period, as required by the safeguard.",
                "measure": {
                    "description": "Count of incident response exercises conducted in the past 365 days",
                    "id": "M1"
                },
                "equation": "IF(M1 >= 1, 1, 0)  // Returns 1 if compliant (at least one exercise), 0 otherwise"
            },
            {
                "defination": "Aspect Coverage Ratio - Measures the percentage of required aspects (communication channels, decision making, workflows) that were tested in the exercises.",
                "measure": {
                    "description": "Count of the number of aspects tested in the most recent exercise or exercises within a defined period, where each aspect is counted once if tested",
                    "id": "M2"
                },
                "equation": "M2 / 3  // Assuming there are 3 required aspects; result is a ratio from 0 to 1"
            },
            {
                "defination": "Participation Rate - Calculates the percentage of key incident response personnel who participated in the exercises, to ensure involvement as implied by the safeguard.",
                "measure": {
                    "description": "Number of unique key personnel involved in incident response exercises within the past year",
                    "id": "M3"
                },
                "equation": "M3 / M4  // Where M4 is the total number of key personnel involved in the incident response process; result is a ratio from 0 to 1"
            }
        ]
    },
    {
        "Observable": "Post-incident review reports, documentation of lessons learned, records of follow-up actions, and audit logs indicating the conduct of review meetings.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the existence and content of post-incident reviews can be confirmed by examining documentation, records, and logs. Measurable because quantitative metrics such as review completion rates, implementation of lessons, and recurrence rates can be calculated from data collected over time.",
        "Evaluation_Method": [
            "Data-driven"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation is appropriate because it involves analyzing historical data from incident management systems, review logs, and action trackers to compute statistics on the frequency, timeliness, and effectiveness of post-incident reviews in preventing recurrence.",
        "Metric": [
            {
                "defination": "Percentage of incidents that had a post-incident review conducted.",
                "measure.description": "Count of incidents with post-incident reviews conducted (M1), Total number of incidents (M3)",
                "measure.id": [
                    "M1",
                    "M3"
                ],
                "equation": "M1 / M3"
            },
            {
                "defination": "Percentage of identified lessons learned that have corresponding follow-up actions completed.",
                "measure.description": "Count of lessons learned identified in post-incident reviews (M4), Count of follow-up actions completed from reviews (M5)",
                "measure.id": [
                    "M4",
                    "M5"
                ],
                "equation": "M5 / M4 if M4 > 0, else 0"
            },
            {
                "defination": "Average time taken in days to complete a post-incident review after incident closure.",
                "measure.description": "Time delay in days from incident closure to review completion for each review (M7)",
                "measure.id": [
                    "M7"
                ],
                "equation": "Average(M7)"
            },
            {
                "defination": "Rate at which similar incidents recur, indicating the effectiveness of reviews in prevention.",
                "measure.description": "Count of recurring incidents of similar type within a defined period (M8), Total number of incidents (M3)",
                "measure.id": [
                    "M8",
                    "M3"
                ],
                "equation": "M8 / M3"
            }
        ]
    },
    {
        "Observable": "Documents such as incident response policy, threshold definitions (including differentiation between incidents and events), review records, and change management logs indicating significant enterprise changes.",
        "Class": "Verifiable",
        "Class.explanation": "The safeguard can be verified by inspecting the documentation to ensure that security incident thresholds are defined, differentiate between incidents and events, and are reviewed annually or after significant changes, as it involves checking the existence and content of records without extensive data analysis.",
        "Evaluation_Method": "Model-based",
        "Evaluation_Method.explanation": "Evaluation is performed by examining the configuration and records, such as policy documents and review logs, to assess compliance with the safeguard requirements, using the documented model of thresholds and reviews without active testing or data-driven analysis.",
        "Metric": [
            {
                "defination": "Percentage of security incident categories with thresholds defined",
                "measure": {
                    "description": "M1: Count of incident categories with thresholds defined (e.g., abnormal activity, security vulnerability, etc.), M2: Count of incident categories without thresholds defined",
                    "id": [
                        "M1",
                        "M2"
                    ]
                },
                "equation": "M1 / (M1 + M2)"
            },
            {
                "defination": "Whether the thresholds differentiate between incidents and events",
                "measure": {
                    "description": "M8: Boolean indicating if differentiation is implemented (1 for yes, 0 for no)",
                    "id": [
                        "M8"
                    ]
                },
                "equation": "M8"
            },
            {
                "defination": "Compliance with annual review requirement",
                "measure": {
                    "description": "M3: Date of the last review of thresholds, current date is used for calculation",
                    "id": [
                        "M3"
                    ]
                },
                "equation": "1 if (current_date - M3) <= 365 else 0"
            },
            {
                "defination": "Whether reviews are conducted after significant changes",
                "measure": {
                    "description": "M5: Number of significant enterprise changes that occurred, M7: Boolean indicating if a review was conducted after the last significant change (1 for yes, 0 for no)",
                    "id": [
                        "M5",
                        "M7"
                    ]
                },
                "equation": "if M5 > 0 then M7 else 1"
            }
        ]
    },
    {
        "Observable": "Penetration testing program documentation, test reports, schedules, contact lists, remediation records, audit logs, and program characteristics definitions such as scope, frequency, limitations, and retrospective requirements.",
        "Class": "Verifiable, Measurable",
        "Class.explanation": "Verifiable because the program's documentation and characteristics can be inspected for compliance with defined standards; Measurable because quantitative aspects like test frequency, coverage, and remediation rates can be assessed and quantified.",
        "Evaluation_Method": "Data-driven, Model-based",
        "Evaluation_Method.explanation": "Data-driven evaluation involves analyzing data from penetration tests, such as frequency logs and findings reports, to generate statistics; Model-based evaluation uses the program's defined model, such as scope and frequency settings, to verify adherence through configuration checks.",
        "Metric": [
            {
                "defination": "Ratio of assets covered by penetration tests to the total assets in the program's scope, indicating how well the testing covers intended targets.",
                "measure.description": [
                    "Count of assets covered by penetration tests in a given period",
                    "Total number of assets defined in the program's scope"
                ],
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M1 / M2"
            },
            {
                "defination": "Compliance with scheduled test frequency, measuring if tests are conducted as per the program's defined interval.",
                "measure.description": [
                    "Actual time interval between consecutive penetration tests",
                    "Scheduled time interval for tests from program definition"
                ],
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "1 if M3 <= M4 else 0 (for binary compliance per test cycle), or average deviation calculated as |M3 - M4| / M4 for continuous assessment"
            },
            {
                "defination": "Percentage of penetration test findings that have been remediated, assessing the effectiveness of addressing identified issues.",
                "measure.description": [
                    "Number of penetration test findings remediated",
                    "Total number of penetration test findings identified"
                ],
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M5 / M6"
            },
            {
                "defination": "Average time taken to remediate penetration test findings, indicating the speed of response to security issues.",
                "measure.description": [
                    "Time taken to remediate each finding (in days)"
                ],
                "measure.id": [
                    "M7"
                ],
                "equation": "average(M7)"
            },
            {
                "defination": "Score based on adherence to program characteristics such as scope definition, frequency, limitations, and contact information, derived from a checklist.",
                "measure.description": [
                    "Binary indicator for each program characteristic compliance (e.g., 1 if scope is defined, 0 otherwise)"
                ],
                "measure.id": [
                    "M8"
                ],
                "equation": "average(M8) over all characteristics"
            }
        ]
    },
    {
        "Observable": "Penetration test reports, test schedules, logs of test activities, evidence of qualified testers, and inclusion of enterprise and environmental reconnaissance in tests.",
        "Class": "Measurable",
        "Class.explanation": "The safeguard requires measuring the frequency, coverage, and quality of penetration tests through data analysis of test results, logs, and reports, which involves quantitative assessment rather than simple checklist or configuration verification.",
        "Evaluation_Method": "Active testing",
        "Evaluation_Method.explanation": "Penetration testing involves actively probing the network to detect vulnerabilities and exploitable information, which requires direct interaction and testing of the system, aligning with the active testing evaluation method.",
        "Metric": [
            {
                "defination": "Assessment of how often penetration tests are performed relative to the annual requirement.",
                "measure.description": "Time elapsed since the last penetration test in days (M1).",
                "measure.id": "M1",
                "equation": "Compliance_Score = 1 if M1 <= 365, else 0"
            },
            {
                "defination": "Percentage of the network segments that have been tested in penetration tests.",
                "measure.description": "Count of network segments tested in the last test (M2) and total count of network segments (M3).",
                "measure.id": "M2, M3",
                "equation": "Coverage_Ratio = M2 / M3"
            },
            {
                "defination": "Proportion of penetration tests that include enterprise and environmental reconnaissance.",
                "measure.description": "Count of penetration tests that included reconnaissance (M4) and total count of penetration tests conducted (M5).",
                "measure.id": "M4, M5",
                "equation": "Reconnaissance_Rate = M4 / M5"
            },
            {
                "defination": "Proportion of penetration tests conducted by qualified parties.",
                "measure.description": "Count of tests conducted by qualified parties (M6) and total count of penetration tests conducted (M5).",
                "measure.id": "M6, M5",
                "equation": "Qualified_Tester_Ratio = M6 / M5"
            }
        ]
    },
    {
        "Observable": "Penetration test reports, remediation logs, documentation of remediation timelines and efforts based on impact, system configuration changes, records of prioritization and effort estimation",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable because the remediation of penetration test findings can be confirmed by inspecting system configurations, logs, and documentation to ensure fixes are applied. Measurable because quantitative metrics such as remediation rates, timeliness, and process adherence can be calculated from data on findings and remediations.",
        "Evaluation_Method": [
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Data-driven evaluation involves analyzing historical data from penetration tests, remediation logs, and documentation to assess compliance and effectiveness. Active testing may be used to verify that vulnerabilities are indeed fixed by conducting follow-up penetration tests or probes.",
        "Metric": [
            {
                "defination": "The proportion of penetration test findings that have been successfully remediated.",
                "measure.description": "Total number of penetration test findings and number of remediated findings.",
                "measure.id": "M1, M2",
                "equation": "M2 / M1"
            },
            {
                "defination": "The proportion of findings that were remediated within their documented timeline.",
                "measure.description": "Number of findings with documented remediation timeline and effort based on impact, and number of findings remediated within that timeline.",
                "measure.id": "M3, M4",
                "equation": "M4 / M3"
            },
            {
                "defination": "The proportion of findings that have a documented remediation timeline and effort based on impact.",
                "measure.description": "Total number of penetration test findings and number of findings with documented timeline and effort.",
                "measure.id": "M1, M3",
                "equation": "M3 / M1"
            }
        ]
    },
    {
        "Observable": "Penetration test reports, logs of validation actions, configuration changes in security systems (e.g., updated rulesets in firewalls or IDS), detection event logs showing if techniques are detected.",
        "Class": [
            "Verifiable",
            "Measurable"
        ],
        "Class.explanation": "Verifiable: The safeguard can be verified by inspecting system configurations, logs, and documentation to confirm that validation and modifications occurred after penetration tests. Measurable: The effectiveness can be quantified using metrics such as validation rates, modification compliance, and detection improvements, which require data analysis.",
        "Evaluation_Method": [
            "Model-based",
            "Data-driven",
            "Active testing"
        ],
        "Evaluation_Method.explanation": "Model-based: Configuration files and logs can be examined to verify if rulesets and capabilities were modified as needed. Data-driven: Statistical analysis of event logs and detection rates over time can assess improvements. Active testing: Probing the system with techniques from penetration tests can verify if they are now detected.",
        "Metric": [
            {
                "defination": "Validation Coverage: The proportion of penetration tests where security measures were validated.",
                "measure.description": "M1: Count of penetration tests conducted, M2: Count of penetration tests where validation was performed",
                "measure.id": [
                    "M1",
                    "M2"
                ],
                "equation": "M2 / M1"
            },
            {
                "defination": "Modification Compliance Rate: The proportion of penetration tests where modifications were deemed necessary and actually implemented.",
                "measure.description": "M3: Count of penetration tests where modifications were deemed necessary, M4: Count of penetration tests where modifications were implemented",
                "measure.id": [
                    "M3",
                    "M4"
                ],
                "equation": "M4 / M3"
            },
            {
                "defination": "Technique Detection Coverage: The percentage of unique techniques used in penetration tests that are currently detectable by security measures.",
                "measure.description": "M5: Total number of unique techniques used in all penetration tests, M6: Number of those techniques detected by current security measures",
                "measure.id": [
                    "M5",
                    "M6"
                ],
                "equation": "M6 / M5"
            },
            {
                "defination": "Average Response Time: The mean time taken to implement modifications after a penetration test ends.",
                "measure.description": "M7: List of end times for penetration tests, M8: List of implementation times for modifications (for each test, compute time delta)",
                "measure.id": [
                    "M7",
                    "M8"
                ],
                "equation": "average(M8 - M7)"
            }
        ]
    },
    {
        "Observable": "Penetration test reports, schedules, logs of test execution, and records of network segments tested, including dates and coverage details.",
        "Class": "Measurable",
        "Class.explanation": "This safeguard involves measuring the frequency and coverage of penetration tests based on data from test reports and schedules, which requires data-driven analytics to assess compliance with program requirements.",
        "Evaluation_Method": "Active testing, Data-driven",
        "Evaluation_Method.explanation": "Active testing is used to conduct the penetration tests by actively probing the network to simulate attacks, and data-driven evaluation is applied to analyze the test data, such as frequency and coverage, to assess enforcement quality.",
        "Metric": [
            {
                "defination": "Frequency compliance indicator for penetration tests, checking if tests are performed at least annually as required.",
                "measure.description": "Days elapsed since the last penetration test was conducted.",
                "measure.id": "M1",
                "equation": "1 if M1 <= 365 else 0"
            },
            {
                "defination": "Coverage rate of network segments tested in the most recent penetration test, indicating the proportion of the network assessed.",
                "measure.description": "Number of network segments tested in the last penetration test and the total number of network segments.",
                "measure.id": "M2, M3",
                "equation": "M2 / M3"
            }
        ]
    }
]